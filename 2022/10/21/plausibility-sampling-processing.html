<!DOCTYPE html>
<html>

  <head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-WR9S0121FQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());gtag('config', 'G-WR9S0121FQ');
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Plausibility of Sampling for Processing</title>
  <meta name="description" content="I just posted a preprint:">


  <link rel="stylesheet" type="text/css" href="https://storage.googleapis.com/app.klipse.tech/css/codemirror.css">
    <script>
      window.klipse_settings = {
        selector: '.language-eval-clojure', // css selector for the html elements you want to klipsify
        selector_reagent: '.language-reagent', // selector for reagent snippets
        editor_type: 'codemirror',
        codemirror_options_in: {
           indentUnit: 2,
           lineWrapping: true,
           lineNumbers: true,
           autoCloseBrackets: true
        },
        codemirror_options_out: {
           lineWrapping: true,
           lineNumbers: false,
           readOnly: "true" 
        },
        // clojure_cached_macro_ns_regexp: /reagent.*/, // the regexp for clojure macro namespaces that are cached
        // clojure_cached_ns_regexp: /reagent.*/,       // the regexp for clojure namespaces that are cached
      };
    </script>

  <!-- Import Vega 5 & Vega-Lite 5 (does not have to be from CDN) -->
  <script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
  <script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
  <!-- Import vega-embed -->
  <script src="https://cdn.jsdelivr.net/npm/vega-embed"></script>
  <!-- FOR KATEX (https://stackoverflow.com/a/57370526/1676393) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/katex.min.css" integrity="sha384-Wsr4Nh3yrvMf2KCebJchRJoVo1gTU6kcP05uRSh5NV3sj9+a8IomuJoQzf3sMq4T" crossorigin="anonymous">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/katex.min.js" integrity="sha384-+W9OcrYK2/bD7BmUAk+xeFAyKp0QjyRQUCxeU31dfyTt/FrPsUgaBTLLkVf33qWt" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.28/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>


  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="https://jahoo.github.io/2022/10/21/plausibility-sampling-processing.html">
  <link rel="alternate" type="application/rss+xml" title="jacob hoover vigly" href="https://jahoo.github.io/feed.xml">

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">jacob hoover vigly</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
          <a class="page-link" href="/">about</a>
          
        
          
        
          
          <a class="page-link" href="/posts.html">posts</a>
          
        
          
          <a class="page-link" href="/pubs.html">research</a>
          
        
        <!-- Add CV manually -->
        <a class="page-link" href="/assets/CV/jlhv-academic_cv.pdf" target="_blank">cv</a>
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <div class="post-title" itemprop="name headline">
    	Plausibility of Sampling for Processing
    </div>
    <span class="post-meta">
    	<time datetime="2022-10-21T00:00:00+00:00" itemprop="datePublished">
    		21 Oct 2022
    	</time>
    	
	</span>
    <span class="post-tags">
      
        - note
      
        - paper
      
    </span>
  </header>

  <article class="post-content" itemscope itemtype="http://schema.org/BlogPosting">
    <!-- <div itemprop="articleBody"> -->
      <p>I just posted a preprint:</p>

<p><a href="https://osf.io/qjnpv">ðŸ”— <em>The Plausibility of Sampling as an Algorithmic Theory of Sentence Processing</em></a>.</p>

<p>This work is a collaboration with <a href="https://people.linguistics.mcgill.ca/~morgan/">Morgan Sonderegger</a>, <a href="http://colala.berkeley.edu/people/piantadosi/">Steve Piantadosi</a>, and <a href="https://todonnell.github.io/">Tim Oâ€™Donnell</a>. It is based on the well-documented observation that for humans, the difficulty to process a given item of linguistic input depends on how predictable it is in contextâ€”more surprising words take longer to process. However, most existing theories of processing cannot simply and directly predict this behavior. What algorithm might be capable of explaining this phenomenon?</p>

<p>In this work, we focus on a class of algorithms whose runtime does naturally scale in surprisalâ€”those that involve repeatedly sampling from the prior. Our first contribution is to show that <strong>simple examples of such algorithms predict runtime to increase superlinearly with surprisal, and also predict variance in runtime to increase.</strong> These two predictions stand in contrast with literature on surprisal theory (<a href="https://www.aclweb.org/anthology/N01-1021">Hale, 2001</a>; <a href="https://doi.org/10.1016/j.cognition.2007.05.006">Levy, 2008</a>) which argues that the expected processing cost should increase linearly with surprisal, and makes no prediction about variance.</p>

<p>In the second part of this paper, we conduct an empirical study of the relationship between surprisal and reading time, using a collection of modern language models to estimate surprisal, and fitting Generalized Additive Models of the relationship. We find that with better language models, reading time increases superlinearly in surprisal, and also that variance increases. These results are consistent with the predictions of sampling-based algorithms.</p>

<hr />
<p><br /></p>

<dl>
  <dt><em>update 2023-07</em></dt>
  <dd>Published in the journal <em>Open Mind</em> (2023) 7: 350â€“391. [<a href="https://doi.org/10.1162/opmi_a_00086">ðŸ”— open access</a>]</dd>
</dl>

    <!-- </div> -->
  </article>


</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">
    <span class="last-modified-note">
      Last modified: 2026-02-21 00:30:04 +0000
    </span>

  </div>

</footer>

    <script src="https://storage.googleapis.com/app.klipse.tech/plugin/js/klipse_plugin.js"></script>
  </body>

  <!-- Load Common JS (for custom things like abstract hidden / open) -->
  <script src="https://jahoo.github.io/assets/js/common.js"></script>
</html>

