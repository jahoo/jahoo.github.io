[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sampling and sentence processing",
    "section": "",
    "text": "\\global\\def\\I#1{\\operatorname{I}(#1)}\n\\global\\def\\H#1{\\operatorname{H}(#1)}\n\\global\\def\\surp#1{\\operatorname{surp}(#1)}\n\\global\\def\\DIV#1#2#3#4{\\operatorname{D_{#1}}(#3#2#4)}\n\\global\\def\\KL#1#2{\\DIV{KL}\\|{#1}{#2}}\n\\global\\def\\EE{\\operatorname*{\\mathbb{E}}}\n\\global\\def\\EEE#1#2{\\operatorname*{\\mathbb{E}}_{#1}\\left[#2\\right]}\n\\global\\def\\dee{\\mathop{\\mathrm{d}\\!}}\n\\global\\def\\var#1{\\operatorname{\\mathbb{V}}(#1)}\n\\global\\def\\Var#1#2{\\operatorname{\\mathbb{V}}\\!\\!{}_{#1}(#2)}\n\\global\\def\\indep{\\bot\\!\\!\\!\\bot}\n\\global\\def\\uu{\\breve u}"
  },
  {
    "objectID": "index.html#notebooks-on-various-topics",
    "href": "index.html#notebooks-on-various-topics",
    "title": "sampling and sentence processing",
    "section": "./notebooks on various topics",
    "text": "./notebooks on various topics\n\n\n\n\n\nDifficulty and surprisal\n\n\n\n\n\n\n\n\n1/16/25, 2:19:12 PM\n\n\n\n\n\n\n\nDensity of transformed random variable\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\n12/16/24, 4:05:49 PM\n\n\n\n\n\n\n\nLiterature review\n\n\nparsing, sampling, and linguistic phenomena\n\n\n\n\n\n11/20/24, 9:38:33 PM\n\n\n\n\n\n\n\nBayesian incremental parsing\n\n\nMaking a formal framework\n\n\n\n\n\n10/29/24, 6:56:34 PM\n\n\n\n\n\n\n\nRAVI notes\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n7/5/24, 2:20:22 PM\n\n\n\n\n\n\n\nbiased coin\n\n\n\n\n\n\n\n\n12/20/23, 3:31:55 PM\n\n\n\n\n\n\n\nWhy surprising doesnâ€™t always mean difficult to process\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n11/17/23, 10:48:10 AM\n\n\n\n\n\n\n\nKL(P||Q) idea\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n10/20/23, 12:16:08 PM\n\n\n\n\n\n\n\ninteractive KL and surprisal decomposition\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\n10/19/23, 3:16:09 PM\n\n\n\n\n\n\n\nRejection sampling\n\n\nThe rejection sampling algorithm, and how guess-and-check is a special case.\n\n\n\n\n\nAug 29, 2022\n\n\n2/14/23, 1:57:33 PM\n\n\n\n\n\n\n\na noisy channel LM\n\n\nstarting a new project\n\n\n\n\n\nInvalid Date\n\n\n11/8/22, 10:43:52 PM\n\n\n\n\n\n\n\nBeams of particles\n\n\nbuilding an SMC parsing model\n\n\n\n\n\n7/14/22, 10:27:20 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#notes-on-particular-papers",
    "href": "index.html#notes-on-particular-papers",
    "title": "sampling and sentence processing",
    "section": "./notes on particular papers",
    "text": "./notes on particular papers\n\n\n\n\n\nNotes on SMC for LLMs\n\n\nSequential Monte Carlo Steering of Large Language Models using Probabilistic Programs\n\n\n\n\n\nAug 1, 2023\n\n\n10/24/24, 6:04:48 PM\n\n\n\n\n\n\n\nHarmonic mean estimator\n\n\n\n\n\n\n\n\n9/30/24, 7:29:06 PM\n\n\n\n\n\n\n\nNotes on Resource-rational surprisal\n\n\nA resource-rational model of human processing of recursive linguistic structure\n\n\n\n\n\nNov 1, 2022\n\n\n1/15/23, 9:44:58 PM\n\n\n\n\n\n\n\nNotes on GCN parsing\n\n\nStrongly Incremental Constituency Parsing with Graph Neural Networks \n\n\n\n\n\nJan 1, 2021\n\n\n5/18/22, 11:55:44 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#logs",
    "href": "index.html#logs",
    "title": "sampling and sentence processing",
    "section": "logs",
    "text": "logs\n\n\n\n\n\nKL theory and noisy surprisal\n\n\nBranch off of processing-surprisal.md\n\n\n\n\n\n6/27/24, 11:19:36 AM\n\n\n\n\n\n\n\nPlausibility of sampling nonlinear surprisal project\n\n\nContinuation of eval2/log.md\n\n\n\n\n\nJan 21, 2025\n\n\n5/23/23, 4:02:45 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "sampling and sentence processing",
    "section": "slides",
    "text": "slides\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1/21/25, 7:28:03 PM\n\n\n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n9/30/24, 4:04:55 PM\n\n\n\n\n\n\n\nWhen unpredictable doesnâ€™t mean difficult\n\n\nTesting a belief-update theory of processing cost\n\n\n\n\n\nJan 21, 2025\n\n\n7/10/24, 12:53:00 PM\n\n\n\n\n\n\n\nâ–³\n\n\n\n\n\n\n\n\n7/8/24, 11:11:05 AM\n\n\n\n\n\n\n\nwhen does unpredictable not mean difficult?\n\n\ntesting a belief-update theory of processing cost\n\n\n\n\n\nDec 1, 2023\n\n\n12/1/23, 2:01:10 PM\n\n\n\n\n\n\n\nwhen unpredictable \\ne difficult\n\n\ntesting a belief-divergence theory of processing cost\n\n\n\n\n\nNov 21, 2023\n\n\n11/21/23, 4:27:48 PM\n\n\n\n\n\n\n\nIllusions etc\n\n\nconstructions that are unexpectedly easy to process\n\n\n\n\n\n11/10/23, 10:49:25 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#other",
    "href": "index.html#other",
    "title": "sampling and sentence processing",
    "section": "other",
    "text": "other\n\n\n\n\n\nSampling-based processing\n\n\nDissertation proposal\n\n\n\n\n\nJun 27, 2022\n\n\n10/19/23, 8:08:11 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/KL-demo-example.html#section",
    "href": "slides/KL-demo-example.html#section",
    "title": "ðŸ““",
    "section": "",
    "text": "\\begin{aligned}\n&\\operatorname{surprisal} {\\color{888888}= -\\log p(u) =  -\\log \\textstyle\\sum_z p(z)p(u|z) \\phantom{= -\\log \\mathop{\\mathbb{E}}_{\\operatorname{prior}} \\operatorname{likelihood}}}\\\\\n{\\colorKL\\operatorname{KL}(\\operatorname{posterior}\\|\\operatorname{prior})}\n= &\\overbrace{-\\log \\mathop{\\mathbb{E}}_{\\operatorname{prior}} \\operatorname{likelihood} }%^{\\operatorname{surprisal}}\n- {\\colorR\\underbrace{\\mathop{\\mathbb{E}}_{\\operatorname{posterior}}[-\\log \\operatorname{likelihood}]}_{\\operatorname{R}}}\n\\end{aligned}\n\n\n\nplot_bayes({\n//   title: 'Prior, likelihood and resulting posterior', \n//   subtitle:ex_selected.name,\n//   caption:ex_selected.desc,\n    // fontSize: 20,\n    width: 800,  \n    // height:200, \n    marginLeft:100, marginRight:30,\n    showValues: showValues,\n    eventNames: event_names\n})\n\n\n\n\n\n\n\nplot_infometrics_partition({\n    // fontSize: 20,\n    width: 800, height:90, \n    marginLeft:100, marginRight:80})\n\n\n\n\n\n\n\nviewof ex_selected = Inputs.select(exx, {\n  label: \"select example\", \n  format: (x,i)=&gt;`${x.name}`,width: 350,\n  value: exx.find(d=&gt;d.name.includes(\"embarrassed\"))\n})\n\n\n\n\n\n\n\n\n\n\ncontrols\n\n\ninput1 = html`&lt;table style=\"margin-left: 0px\"&gt;\n&lt;tr&gt;\n&lt;td&gt;event ${tex`z`}&lt;/td&gt;\n&lt;td&gt;&lt;b&gt;prior&lt;/b&gt;: ${tex`p(Z = \\cdot)`} (unnormalized)&lt;/td&gt;\n&lt;td&gt;&lt;b&gt;likelihood&lt;/b&gt;: ${tex`p(\\text{observation} \\mid Z = \\cdot)`} &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;${viewof event_names}&lt;/td&gt;\n&lt;td&gt;${viewof input_prior} &lt;/td&gt;\n&lt;td&gt;${viewof input_likelihood} &lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;/td&gt;\n&lt;td&gt;${viewof PriorButtons} &lt;/td&gt;\n&lt;td&gt;${viewof LikelihoodButtons} &lt;/td&gt;\n&lt;/tr&gt;\n&lt;/table&gt;`\n\nviewof scale_likelihood = Inputs.range(\n  [Math.min(options.probStep,options.probStep/Math.min(...input_likelihood)), 1/Math.max(...input_likelihood)], \n  {label: \"scale likelihood\", value:1, transform: probInputTransform})\n\nviewof applyScaleLikelihood = Inputs.button([[\n  \"apply likelihood scaling\",\n  () =&gt; {\n    set_(viewof input_likelihood, input_likelihood_scaled)\n  }\n]])\n\n\nhtml`&lt;hr&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhtml`&lt;table&gt;\n&lt;tr&gt;\n&lt;td&gt;${viewof dim}&lt;/td&gt;\n&lt;td&gt;${viewof maxUnits}&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;${viewof useLogInput}&lt;/td&gt;\n&lt;td&gt;${viewof allowZeroes}&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;${viewof showValues}&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/table&gt;`\n\n\n\n\n\n\n\nexx = [\n  {name:\"'embarrsased' typo example\",\n   desc:md`slide likelihood between _embarrsased_ versus _embarrassed_`,\n   in_pr: normalize([0.5,0.355,0.125,0.02]), in_lik: [0.03125,0,0,0],\n   event_names: ['embarrassed', '...', '...', 'innovative']},\n  {name:\"(fig 1.1 a): binary likelihood (KL=2, R=0)\",\n   desc:md`binary likelihood ${tex`\\implies`} KL=surprisal`,\n   in_pr: normalize([3,1,0]), in_lik: [0,1,1]},\n  {name:\"(fig 1.1 b): nonbinary likelihood (KL=2, R=3)\",\n   desc:md`non-binary likelihood ${tex`\\implies`} KL &lt; surprisal`,\n   in_pr: normalize([3,1,0]), in_lik: [0,1/8,1/64]},\n  {name:\"constant likelihood where prior nonzero (=&gt; KL=0)\",\n   desc:md`constant likelihood on prior support ${tex`\\implies`} prior = posterior`,\n   in_pr: normalize([1,2,3,0,0,1]), in_lik: [.2,.2,.2,.02,.35,.2]},\n  {name:\"binary likelihood where prior nonzero (=&gt; KL=surprisal)\",\n   desc:md`binary likelihood ${tex`\\implies`} KL=surprisal`,\n   in_pr: normalize([1,2,3,0,0,1]), in_lik: [1,0,1,1,.5,0]},\n  {name:\"deterministic prior (=&gt; KL=0)\",\n   desc:md`_(trivial sub-case of constant likelihood)_ deterministic prior ${tex`\\implies`} prior = posterior`,\n   in_pr: [0,1,0,0,0,0], in_lik: [.1,.3,.15,.02,.35,.2]},\n  {name:\"informative likelihood (=&gt; R small)\",\n   desc:md`an informative likelihood (=&gt; R small)`,\n   in_pr: normalize([1,7/8,1/2,1/4,1/8,1/16]), in_lik: [0.001,0.003,0.95,0.1,0.98,0.15]},\n  // {name:\"vary KL with constant surprisal\",\n  //  desc:md`Note: to see KL change while surprisal stays constant, vary prior(1).`,\n  //  in_pr: [0.01, 0.01, 0.09], in_lik: [0.1, 0.99, 0.001]},\n  // {name:\"vary surprisal with constant KL\",\n  //  desc:md`Note: to see surprisal change while KL stays constant, scale likelihood.`,\n  //  in_pr: [0.1, 0.2, 0.7], in_lik: [1, 0.01, 0.0001]}\n]\n\n\n\n\n\n\n\n\n\n\nimport {\n  plot_bayes, plot_infometrics_partition, plot_infometrics_bars, set_,\n  options, generate_unif, generate_logunif, base,\n  probInputTransform, get_infometrics, normalize\n} with {\n  dim as dim, useLogInput as useLogInput, allowZeroes as allowZeroes, \n  input_prior as input_prior, \n  input_likelihood as input_likelihood,\n  input_likelihood_scaled as input_likelihood_scaled, maxUnits as maxUnits\n} from \"@postylem/kl-and-surprisal\"\n\ninput_likelihood_scaled = input_likelihood.map(p =&gt; p * scale_likelihood)\n\nviewof dim = Inputs.range([1, 20], {step: 1, label: \"dimension\", value: ex_selected.in_lik.length})\nviewof useLogInput = Inputs.toggle({label: \"logarithmic sliders\", value: true})\nviewof allowZeroes = Inputs.toggle({label: \"allow zeroes\", value: useLogInput ? false: true, disabled: useLogInput ? true: false})\nviewof showValues = Inputs.toggle({label: \"show values\", value: true})\nviewof maxUnits = Inputs.range(base.slider.range, {\n  step: 0.1, label: base.unit + \" axis limit\", \n  value: base.slider.initialValue, width: 200, transform: Math.log\n})\n\nviewof event_names = Inputs.form(\n  Array.from({length: dim}, (_,i) =&gt; Inputs.text({\n    // label: i.toString(), \n    width: -30, value: ex_selected.event_names ? ex_selected.event_names[i] : i,\n    placeholder: `${i}`\n  }))\n)\n\nviewof input_prior = {\n  const inputs = Inputs.form(\n    Array.from({length: dim}, (_,i)=&gt;Inputs.range(\n      [options.probMin, 1], {\n        step: options.probStep, value: ex_selected.in_pr[i],//Math.random(),//1/dim, \n        transform: probInputTransform\n      }\n    ))\n  );\n  inputs.classList.add(\"input-prior\");\n  return inputs\n}\n\nviewof input_likelihood = {\n  const inputs = Inputs.form(\n    Array.from({length: dim}, (_,i)=&gt;Inputs.range(\n      [options.probMin, 1], {\n        step: options.probStep, value: ex_selected.in_lik[i],//Math.random(),//0.5,\n        transform: probInputTransform}\n    ))\n  );\n  inputs.classList.add(\"input-likelihood\");\n  return inputs\n}\n\nviewof PriorButtons = Inputs.button([\n  [\"constant\", () =&gt; set_(viewof input_prior, Array(dim).fill(1/dim))],\n  [\"rand. unif.\", () =&gt; {return set_(viewof input_prior, generate_unif(dim))}],\n  [\"rand. logunif.\", () =&gt; {\n    return set_(viewof input_prior, generate_logunif(dim, options.probMin == 0 ? 1e-5 : options.probMin, 1))\n  }],\n  [\"rand. determ.\", () =&gt; {\n    let j = Math.floor(Math.random() * dim);\n    return set_(viewof input_prior, Array.from({length: dim}, (_,i) =&gt; (i===j?1:0)))\n  }]\n])\n\n\nviewof LikelihoodButtons = Inputs.button([\n  [\"constant\", () =&gt; set_(viewof input_likelihood, Array(dim).fill(0.1))],\n  [\"rand. unif\", () =&gt; {\n    return set_(viewof input_likelihood, generate_unif(dim))\n  }],\n  // [\"asin\", () =&gt; {\n  //   return set_(viewof input_likelihood, generate_arcsin(dim))\n  // }],\n  [\"rand. logunif\", () =&gt; {\n    return set_(viewof input_likelihood, generate_logunif(dim, options.probMin == 0 ? 1e-5 : options.probMin, 1))\n  }],\n  [\"rand. binary\", () =&gt; {\n    return set_(viewof input_likelihood, Array.from({length: dim}, () =&gt; (Math.random()&lt;0.5?0:1)))\n  }],\n])\n\nhtml`&lt;style&gt;\n.input-prior input[type=\"range\"] { accent-color: #CE6860; }\n.input-likelihood input[type=\"range\"] { accent-color: #6482B2; }\n&lt;/style&gt;`"
  }
]