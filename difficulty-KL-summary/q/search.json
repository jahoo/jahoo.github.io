[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sampling and sentence processing",
    "section": "",
    "text": "\\global\\def\\I#1{\\operatorname{I}(#1)}\n\\global\\def\\H#1{\\operatorname{H}(#1)}\n\\global\\def\\surp#1{\\operatorname{surp}(#1)}\n\\global\\def\\DIV#1#2#3#4{\\operatorname{D_{#1}}(#3#2#4)}\n\\global\\def\\KL#1#2{\\DIV{KL}\\|{#1}{#2}}\n\\global\\def\\E{\\operatorname*{\\mathbb{E}}}\n\\global\\def\\dee{\\mathop{\\mathrm{d}\\!}}\n\\global\\def\\var#1{\\operatorname{\\mathbb{V}}(#1)}\n\\global\\def\\Var#1#2{\\operatorname{\\mathbb{V}}\\!\\!{}_{#1}(#2)}\n\\global\\def\\indep{\\bot\\!\\!\\!\\bot}\n\\global\\def\\uu{\\breve u}"
  },
  {
    "objectID": "index.html#notebooks-on-various-topics",
    "href": "index.html#notebooks-on-various-topics",
    "title": "sampling and sentence processing",
    "section": "./notebooks on various topics",
    "text": "./notebooks on various topics\n\n\n\n\n\nBayesian incremental parsing\n\n\nMaking a formal framework\n\n\n\n\n\n3/12/24, 12:31:35 PM\n\n\n\n\n\n\n\nBayesian incremental parsing\n\n\nMaking a formal framework\n\n\n\n\n\n3/12/24, 12:29:35 PM\n\n\n\n\n\n\n\nDifficulty and surprisal\n\n\n\n\n\n\n\n\n3/12/24, 12:06:51 PM\n\n\n\n\n\n\n\nLiterature review\n\n\nparsing, sampling, and linguistic phenomena\n\n\n\n\n\n3/8/24, 9:07:08 PM\n\n\n\n\n\n\n\nDensity of transformed random variable\n\n\n\n\n\n\n\n\nNov 2, 2022\n\n\n3/8/24, 12:19:57 PM\n\n\n\n\n\n\n\nbiased coin\n\n\n\n\n\n\n\n\n12/20/23, 3:31:55 PM\n\n\n\n\n\n\n\nWhy surprising doesn’t always mean difficult to process\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\n11/17/23, 10:48:10 AM\n\n\n\n\n\n\n\nKL(P||Q) idea\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\n10/20/23, 12:16:08 PM\n\n\n\n\n\n\n\ninteractive KL and surprisal decomposition\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\n10/19/23, 3:16:09 PM\n\n\n\n\n\n\n\nRejection sampling\n\n\nThe rejection sampling algorithm, and how guess-and-check is a special case.\n\n\n\n\n\nAug 29, 2022\n\n\n2/14/23, 1:57:33 PM\n\n\n\n\n\n\n\na noisy channel LM\n\n\nstarting a new project\n\n\n\n\n\nInvalid Date\n\n\n11/8/22, 10:43:52 PM\n\n\n\n\n\n\n\nBeams of particles\n\n\nbuilding an SMC parsing model\n\n\n\n\n\n7/14/22, 10:27:20 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#notes-on-particular-papers",
    "href": "index.html#notes-on-particular-papers",
    "title": "sampling and sentence processing",
    "section": "./notes on particular papers",
    "text": "./notes on particular papers\n\n\n\n\n\nNotes on SMC for LLMs\n\n\nSequential Monte Carlo Steering of Large Language Models using Probabilistic Programs\n\n\n\n\n\nAug 1, 2023\n\n\n11/9/23, 1:43:36 PM\n\n\n\n\n\n\n\nNotes on Resource-rational surprisal\n\n\nA resource-rational model of human processing of recursive linguistic structure\n\n\n\n\n\nNov 1, 2022\n\n\n1/15/23, 9:44:58 PM\n\n\n\n\n\n\n\nNotes on GCN parsing\n\n\nStrongly Incremental Constituency Parsing with Graph Neural Networks \n\n\n\n\n\nJan 1, 2021\n\n\n5/18/22, 11:55:44 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#logs",
    "href": "index.html#logs",
    "title": "sampling and sentence processing",
    "section": "logs",
    "text": "logs\n\n\n\n\n\nKL theory and noisy surprisal\n\n\nBranch off of processing-surprisal.md\n\n\n\n\n\n3/6/24, 12:44:22 PM\n\n\n\n\n\n\n\nPlausibility of sampling nonlinear surprisal project\n\n\nContinuation of eval2/log.md\n\n\n\n\n\nMar 12, 2024\n\n\n5/23/23, 4:02:45 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#slides",
    "href": "index.html#slides",
    "title": "sampling and sentence processing",
    "section": "slides",
    "text": "slides\n\n\n\n\n\nWhen unpredictable doesn’t mean difficult\n\n\nTesting a belief-update theory of processing cost\n\n\n\n\n\nMar 12, 2024\n\n\n3/5/24, 4:24:43 PM\n\n\n\n\n\n\n\n△\n\n\n\n\n\n\n\n\n12/13/23, 6:48:56 PM\n\n\n\n\n\n\n\nwhen does unpredictable not mean difficult?\n\n\ntesting a belief-update theory of processing cost\n\n\n\n\n\nDec 1, 2023\n\n\n12/1/23, 2:01:10 PM\n\n\n\n\n\n\n\nwhen unpredictable \\ne difficult\n\n\ntesting a belief-divergence theory of processing cost\n\n\n\n\n\nNov 21, 2023\n\n\n11/21/23, 4:27:48 PM\n\n\n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n11/17/23, 10:08:00 AM\n\n\n\n\n\n\n\nIllusions etc\n\n\nconstructions that are unexpectedly easy to process\n\n\n\n\n\n11/10/23, 10:49:25 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#other",
    "href": "index.html#other",
    "title": "sampling and sentence processing",
    "section": "other",
    "text": "other\n\n\n\n\n\nSampling-based processing\n\n\nDissertation proposal\n\n\n\n\n\nJun 27, 2022\n\n\n10/19/23, 8:08:11 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/difficulty-surprisal-summary.html",
    "href": "notebooks/difficulty-surprisal-summary.html",
    "title": "Difficulty and surprisal",
    "section": "",
    "text": "\\global\\def\\I#1{\\operatorname{I}(#1)}\n\\global\\def\\H#1{\\operatorname{H}(#1)}\n\\global\\def\\surp#1{\\operatorname{surp}(#1)}\n\\global\\def\\DIV#1#2#3#4{\\operatorname{D_{#1}}(#3#2#4)}\n\\global\\def\\KL#1#2{\\DIV{KL}\\|{#1}{#2}}\n\\global\\def\\E{\\operatorname*{\\mathbb{E}}}\n\\global\\def\\dee{\\mathop{\\mathrm{d}\\!}}\n\\global\\def\\var#1{\\operatorname{\\mathbb{V}}(#1)}\n\\global\\def\\Var#1#2{\\operatorname{\\mathbb{V}}\\!\\!{}_{#1}(#2)}\n\\global\\def\\indep{\\bot\\!\\!\\!\\bot}\n\\global\\def\\uu{\\breve u}\n% included just for quarto VSCode to display nicely\n\\global\\def\\uu{\\breve u}\n\\global\\def\\R{\\operatorname{R}_{\\pZu}(\\uu)}\n\\global\\def\\Rs#1{\\operatorname{R}_{#1}(\\uu)}\n\\global\\def\\cPOST{\\color{0A00A0}}\n\\global\\def\\cPRIO{\\color{008008}}\n\\global\\def\\cPROP{\\color{A00A00}}\n\\global\\def\\pZu{{\\cPOST p_{Z\\mid\\uu}}}\n\\global\\def\\pzu{{\\cPOST p(z\\mid \\uu)}}\n\\global\\def\\qZu{{\\cPROP q_{Z; \\uu}}}\n\\global\\def\\qzu{{\\cPROP q(z; \\uu)}}\n\\global\\def\\Dq{D_{\\pZu}^{\\qZu \\leftarrow  p_Z}(\\uu)}\n\\global\\def\\surp#1{\\operatorname{surp}(#1)}\n\\global\\def\\priorZ{{\\cPRIO p_{Z\\mid \\mathbf\\uu_{&lt; i}}}}\n\\global\\def\\posteriorZ{{\\cPOST p_{Z\\mid \\mathbf\\uu_{\\le i}}}}\n\\global\\def\\proposalZ{{\\cPROP q_{Z; \\mathbf\\uu_{\\le i}}}}\n\\global\\def\\priorz{{\\cPRIO p(z\\mid \\mathbf\\uu_{&lt; i})}}\n\\global\\def\\posteriorz{{\\cPOST p(z\\mid \\mathbf\\uu_{\\le i})}}\n\\global\\def\\proposalz{{\\cPROP q(z; \\mathbf\\uu_{\\le i})}}\n\\global\\def\\Dqi{D_{ {\\cPOST p} }^{{\\cPROP q} \\leftarrow  {\\cPRIO p} }(\\uu)}\n\\global\\def\\Ri{\\Rs{{\\cPOST p}}}\n%\n\\global\\def\\posteriorU{{{p_\\mathrm{LM}}(\\mathbf U_{&gt;i}\\mid \\mathbf\\uu_{\\le i})}}\n\\global\\def\\proposalU{{{q_\\mathrm{LM}}(\\mathbf U_{&gt;i}\\mid \\mathbf\\uu_{\\le i})}}\n\\global\\def\\posterioru{{{p_\\mathrm{LM}}(\\mathbf u_{&gt;i}\\mid \\mathbf\\uu_{\\le i})}}\n\\global\\def\\proposalu{{{q_\\mathrm{LM}}(\\mathbf u_{&gt;i}\\mid \\mathbf\\uu_{\\le i})}}\n\\global\\def\\prioru{p_\\mathrm{LM}(\\mathbf u_{&gt;i}\\mid \\mathbf\\uu_{&lt; i})}\n%\n\\global\\def\\posteriorN{{{p_\\mathrm{LM}}(\\mathbf U_{i+1}\\mid \\mathbf\\uu_{\\le i})}}\n\\global\\def\\posteriorn{{{p_\\mathrm{LM}}(u_{i+1}\\mid \\mathbf\\uu_{\\le i})}}\n\\global\\def\\dpdq{\\frac{\\dee p}{\\dee q}}"
  },
  {
    "objectID": "notebooks/difficulty-surprisal-summary.html#difficulty-as-kl",
    "href": "notebooks/difficulty-surprisal-summary.html#difficulty-as-kl",
    "title": "Difficulty and surprisal",
    "section": "Difficulty as KL",
    "text": "Difficulty as KL\nSay we want to approximate some target distribution p using samples from some other distribution q.\nIf we use samples from q to approximate p, using importance sampling, the number of samples necessary and sufficient for for an accurate approximation is exponential in the relative entropy:\n\n\\#\\text{samples}_{\\mathrm{IS}(p\\leftarrow q)} \\approx e^{\\KL p q}\n\nFor proof, see Chatterjee & Diaconis (2018), who show that taking e^{\\KL p q} samples from q is a necessary and sufficient condition for the absolute value of the error to be close to zero with high probability.\n\n\n\n\n\n\nConditions for Chatterjee result\n\n\n\nTechnically, this result only obtains when the log density of p wrt q is likely to be concentrated around its expected value.\nThat is, that \\log \\dpdq(Z) (where Z\\sim p) is concentrated around \\E_p{\\log \\dpdq(Z)} = \\KL p q. Or equivalently, that \\dpdq(Z')\\log \\dpdq(Z') (where Z'\\sim q) is concentrated around \\E_q{\\dpdq(Z')\\log \\dpdq(Z')} = \\KL p q. Roughly, this requirement is that the expected variance in importance weights is small.\nMore precisely, their result says that in order to bound the L^1-error of the estimate close to zero with high probability, a sample size of\n\ne^{\\KL p q + \\mathcal{O}(s)} is sufficient\ne^{\\KL p q - \\mathcal{O}(s)} is necessary\n\nwhere s is the typical order of fluctuations in \\log \\dpdq(Z) around its expected value, \\KL p q.\n\n\nSo, for a sampling-based mechanism we can define the update cost as this exponentiatied relative entropy:\n\n\\mathrm{cost} \\coloneqq e^{\\KL p q}\n\nalso note, many empirical studies of human reading time as a function of surprisal log transform the response variable, which in fact implies an exponential relationship like this. this is acknowledged, if only rarely (for example, in Oh et al., 2024)\n\n\n\n\n\n\nOr… choose your fave divergence:\n\n\n\n\n\nThe number of samples necessary for IS can also be related to other divergences (other than KL). See Agapiou et al. (2017) discuss the \\chi^2-divergence, and also Sanz-Alonso (2018) (who also dicsuss Hellinger and TV).\n\n\\DIV{\\chi^2}\\| p q = \\E_q{((\\dpdq)^2)}-1 = \\E_p{(\\dpdq)}-1\n\\KL p q = \\E_q{(\\dpdq\\log\\dpdq)} =\\E_p{(\\log\\dpdq)}\n\nNote it’s clear by Jensen’s that e^{\\mathrm{KL}}\\le \\mathrm{D}_{\\chi^2} + 1.\nThe result discussed in Agapiou et al. (2017) is:\n\nThey define\n\nunnormalized density g as \\frac1{\\E_q{g}}g(\\cdot)\\coloneqq\\dpdq(\\cdot), and\ndenote with \\rho the second moment of this RN-derivative: \\rho\\coloneqq \\E_q(\\dpdq^2) = \\frac{\\E_q{g^2}}{(\\E_q{g})^2}\n\n\\rho \\ge 1 since (\\E_q{g})^2\\le\\E_q{\\mathbf1^2}\\E_q{g^2}= \\E_q{g^2} by Cauchy-Schwarz.\n\n\nTheir main result is that both bias and MSE of IS are \\approx\\rho/N\nThis gives, for some fixed accuracy, the sufficient sample size in terms of KL and chi-squared divergences as:\n\ngrowing linearly in chi-squared, since \\DIV{\\chi^2}\\| p q = \\E_q{((\\dpdq)^2)}-1 = \\rho-1\nexponentially in KL, since \\KL p q= \\E_q{(\\dpdq\\log\\dpdq)} =\\E_p{(\\log\\dpdq)}\\le\\log\\E_p{(\\dpdq)} = \\log \\E_q{((\\dpdq)^2)}=\\log \\rho, by Jensen’s ineq. so e^{\\KL p q} \\le \\rho\n\n\nThere are many references for the relationships between KL and \\chi^2 and other probability metrics/divergences (see Gibbs & Su, 2002; Sanz-Alonso, 2018).\n\\implies For us, the point is: we could alternatively say \\mathrm{cost} \\coloneqq \\DIV{\\chi^2}\\| p q, and it would amount to something roughly similar as \\mathrm{cost} \\coloneqq e^{\\KL p q}.\n\n\n\n\nSetup\n\nLet p_{Z,U} be a hypothetical joint distribution for Z a latent random variable, and U an observable random variable. We don’t assume we have any access to this distribution, but we’ll make use of the following derived distributions:\n\nthe marginal p_Z (the prior distribution on Z)\nthe conditional \\pZu, for any fixed outcome \\uu of U (the posterior on Z).\nAssume \\pZu\\ll p_Z, that is \\pZu=0 anywhere p_Z=0.1\n\nLet the proposal \\qZu be some other distribution over Z, which may depend on the outcome \\uu. Again assume \\pZu\\ll\\qZu.\n\n1 A sufficient but not quite necessary condition for IS weights to be well defined. Also a natural property in a Bayesian setting where the posterior is a reweighted version of the prior, so can’t put mass outside the support of the prior.I’m writing a breve on the outcome variable just to denote that it is fixed.\n\n\nDecomposing KL between prior and posterior\nThe relative entropy of prior p_Z with respect to posterior \\pZu can be written as:\n\n\\begin{aligned}\n\\KL{\\pZu}{p_Z}\n    &= \\E_{\\pZu}{ \\log\\frac{\\pzu}{p(z)} }\n     = \\E_{\\pZu}{ \\log\\frac{p(z,\\uu)}{p(z)p(\\uu)} }\\\\\n    &= \\E_{\\pZu}{ \\log\\frac{p(\\uu\\mid z)}{p(\\uu)} }\\\\\n    &= \\log \\frac1{p(\\uu)}\n     + \\E_{\\pZu}{\\log p(\\uu\\mid z)} \\\\\n    &= \\underbrace{-\\log p(\\uu)}_{\\surp{\\uu}}\n     - \\underbrace{\\E_{\\pZu}{-\\log p(\\uu\\mid z)}}_{\\coloneqq\\ \\R}\n\\end{aligned}\n\nSo the relative entropy between prior and posterior consists of\n\nthe surprisal, \\surp{\\uu}\\ge0,\nminus a term which we denote \\R, which I’ll call ‘reconstruction information’. It is the posterior-expected conditional surprisal… the number of bits by which surprisal of the observation exceeds the size of the belief update it causes. We can think of \\R measuring how many bits of surprisal are irrelevant to belief-updating.\n\nNote that 0\\le\\R\\le \\surp{\\uu}\n\n\n\n\nWe could define a more general \\Rs{q} generally as\n\nDefinition 1 \n\\Rs{q} \\coloneqq \\E_{z\\sim q}{\\log \\frac1{p(\\uu|z,\\breve c)}}\n\\tag{1}\nwhere q is any distribution over the latent variable Z (such as a proposal, or variational approximation to the posterior), and \\uu is a fixed observed outcome of U. Both the probability and the proposal may optionally depend on context \\breve c.\n\n\n\\mathrm{R}_{\\qZu}(\\uu) (for \\qZu an approximation to the posterior \\pZu) is sometimes called the (negative) reconstruction error in variational inference literature (Dehaene et al., 2019; Liang et al., 2018) or original autoencoders literature (such as Vincent et al., 2010).\nin a bounded-rational decision-making framework (Genewein et al., 2015; as in Ortega & Braun, 2013), the negative of this quantity is the expected utility.\n\nNote \\R=0 is a necessary and sufficient condition for the relative entropy between prior and posterior equalling surprisal:\n\n\\KL{\\pZu}{p_Z}=-\\log p(\\uu) \\quad\\iff\\quad \\R=0\n\nU being a deterministic function of Z is a sufficient condition for this to hold (this is the assumption made in the proofs of the equivalence of KL and surprisal, such as in Levy, 2008).\n\n\nDecomposing KL between proposal and posterior\nIf instead of sampling from the prior, we were sampling from some proposal distribution proposal \\qZu, then the we can break down that divergence with respect to posterior \\pZu, to get an additional term:\n\n\\begin{aligned}\n\\KL{\\pZu}{\\qZu}\n    &= \\E_{\\pZu}\\log\\frac{\\pzu}{\\qzu}\\\\\n    &= \\E_{\\pZu}\\log\\frac{\\pzu}{p(z)}\\frac{p(z)}{\\qzu}\\\\\n    &= \\KL{\\pZu}{p_Z}\n        + \\E_{\\pZu}\\log\\frac{p(z)}{\\qzu}\\\\\n    &= {\\surp{\\uu}} - {\\R}\n       - \\underbrace{\\E_{\\pZu}\\log\\frac{\\qzu}{p(z)}}_{\\coloneqq\\ \\Dq}\\\\\n\\end{aligned}\n\nwhere the term \\Dq, quantifies how much better \\qZu is than p_Z for estimating \\pZu. More precisely, as a difference in KLs, it represents the reduction in excess surprise resulting from using \\qZu instead of p_Z, when the actual distribution is \\pZu:\n\n\\Dq = \\KL{\\pZu}{p_Z} - \\KL{\\pZu}{\\qZu}\n\nEquivalently it can be viewed as measuring the reduction in cross-entropy: \n\\Dq = \\E_{\\pZu}\\log\\frac{1}{p(z)}- \\E_{\\pZu}\\log\\frac{1}{\\qzu}=\\H{\\pZu,p_Z}-\\H{\\pZu,\\qZu}\n\nInterpretation:\n\n\\Dq &gt; 0 if \\qZu is better than p_Z for estimating \\pZu and\n\\Dq &lt; 0 if \\qZu is worse than p_Z for estimating \\pZu, and\n\\Dq = 0 if \\qZu = p_Z.\n\nBounds: -\\infty\\le\\Dq\\le\\KL{\\pZu}{p_Z}={\\surp{\\uu}} - {\\R}.\n\n\n\nJust to get a visual picture of the boundaries, here are level surfaces of D_\\mathrm{KL} = S - (R + D)\n\n\n\n\n\n\n\n\nGeneral identity for change in KLs\n\n\n\n\n\nThis could be seen as a result of the general identity\n\n\\KL P Q = \\KL P R - \\E_P\\log\\frac{\\dee Q}{\\dee R}\n\nfor any measures P, Q, R, on the same space with P\\ll R and P\\ll Q.\n\n\n\nThis could alternatively be written as\n\n\\begin{aligned}\n\\KL{\\pZu}{\\qZu}\n    &= \\surp{\\uu}\n    - \\left(\n        \\E_{\\pZu}{\\log\\frac1{p(\\uu\\mid z)}}\n        + \\E_{\\pZu}{\\log\\frac{\\qzu}{p(z)}}\n    \\right)\\\\\n    &= \\surp{\\uu}\n    + \\underbrace{\\E_{\\pZu}{\\log\\frac{p(z,\\uu)}{\\qzu}}}_{-\\R-\\Dq}\n\\end{aligned}\n\n\n\n\n\n\n\nComparison with VAE and ‘Bayesian Surprise’\n\n\n\n\n\nThe expectation here looks a lot like \\operatorname{ELBO}(\\uu)\\coloneqq\\E_{\\qZu}\\left[\\log\\frac{p(z, \\uu)}{\\qzu}\\right] in variational inference (aka negative variational free energy), but they differ in what the expectation is taken with respect to.\nFor comparison, using the same notation, the usual derivation in VAE looks like this. See, e.g., (Kingma, 2017; though this math goes back before VAEs at least to Dayan et al., 1995, eq 2.5):\n\n\\begin{aligned}\n\\log p(\\uu)\n    &= \\KL{\\qZu}{\\pZu}\n    + \\overbrace{\\E_{\\qZu}{\\log\\frac{p(z, \\uu)}{\\qzu}}}^{\\operatorname{ELBO}(\\uu)}\\\\\n\\end{aligned}\n\nSo, we get a very similar equation to the above for a KL in the opposite direction:\n\n\\begin{aligned}\n\\KL{\\qZu}{\\pZu}\n    &= -\\surp{\\uu}\n    - \\E_{\\qZu}{\\log\\frac{p(z, \\uu)}{\\qzu}}\n\\end{aligned}\n\nor\n\n\\begin{aligned}\n\\KL{\\qZu}{\\pZu}\n    &= - \\surp{\\uu}  \n    - \\E_{\\qZu}{\\log\\frac{p(\\uu\\mid z)p(z)}{\\qzu}}\\\\\n    &= -\\surp{\\uu}\n    - \\E_{\\qZu}{\\log p(\\uu\\mid z)}\n    + \\KL{\\qZu}{p_Z}\\\\\n    &= - \\surp{\\uu}\n    + \\underbrace{\\E_{\\qZu}{-\\log p(\\uu\\mid z)}}_{\\text{neg. reconstr. error }\\operatorname{R}_{\\qZu}(\\uu)}\n    + \\underbrace{\\KL{\\qZu}{p_Z}}_{\\text{regularizer}}\\\\\n\\end{aligned}\n\nIn this setup, \\qZu is chosen in order to maximize the ELBO. The ELBO consists of two components, the reconstruction error (which is a negative-log-likelihood term, to be maximized) minus the KL between \\qZu and the prior (which can be seen as a regularization term, to be minimized).\nThese equations look very similar, but it is very different from the case we are interested in, where expectations are taken with respect to the true unknown posterior.\n\nWhile this direction of KL (with expectation over \\qZu) may be the “backward” direction from the point of view of the connection with sampling, it might be important to understand whether/how it relates to processing effort in some way, since when \\qZu=p_Z, this is precisely the divergence used as “Bayesian Surprise” (Baldi, 2002; Baldi & Itti, 2010) (see lit review). Might be that this was chosen purely for computational convenience, but even so, worth understanding what it implies.\nDepending on which direction of KL we choose to use we have two ways of expressing the surprisal:\n\n\\begin{aligned}\n\\surp{\\uu}\n    &= \\overbrace{\\E_{\\qZu}{\\log\\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{-\\operatorname{ELBO}(\\uu)}\n    - \\KL{\\qZu}{\\pZu}\\\\\n\\surp{\\uu}\n    &= \\underbrace{\\E_{\\pZu}{\\log \\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}_{\\R+\\Dq}\n    + \\KL{\\pZu}{\\qZu}\n\\end{aligned}\n\nor, put another way, with R, D, and the ELBO we can express the sum of the KL and reverse-KL \\DIV{J},pq\\coloneqq \\KL pq  + \\KL qp = \\E_q{(\\dpdq-1)\\log\\dpdq} = \\DIV{\\lambda t.(t-1)\\log t}\\|pq (this symmetric f-divergence is actually the one originally proposed by K&L, and earlier defined by Jeffreys):\n\n\\overbrace{\\E_{\\qZu}{\\log\\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{-\\operatorname{ELBO}(\\uu)}\n- \\overbrace{\\E_{\\pZu}{\\log \\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{\\R+\\Dq}\\ge0\n= {\\DIV{J},{\\pZu}{\\qZu}}\n\nThis is a bit pointless in the abstract, since,\\Dq can be positive or negative, hence no bounds are implied. Yet, when we just consider the case where \\qZu=p_Z, then D = 0, and we have\n\n\\overbrace{\\E_{p_Z}{\\log\\frac{1}{p(\\uu\\mid z)}}}^{-\\operatorname{ELBO}(\\uu)=\\operatorname{R}_{p_Z}(\\uu)}\n- \\overbrace{\\E_{\\pZu}{\\log \\frac{1}{p(\\uu\\mid z)}}}^{\\R}\n= \\DIV{J},{\\pZu}{p_Z}\\ge0\n\nwhere all the terms are nonnegative, so then we can say the magnitude of the ELBO (or, free energy, I guess) is in fact a upper bound on the magnitude of surprisal, which is an upper bound on R.\n\n0\\le\\R\\le\\surp{\\uu}\\le-\\operatorname{ELBO}(\\uu)=\\operatorname{R}_{p_Z}(\\uu)\n\nIs it useful to say that surprisal is bounded between R below and prior-reconstruction error/free energy above?\n\n\n\n\n\nIncremental version\nAbove we’re assuming all the probabilities depend on a (notationally suppressed) ‘context’ random var. Now let’s write out the same derivation but with the observation being explicitly the ith item in a sequence \\uu_1, \\uu_2, \\ldots. So the ‘context’ is \\mathbf\\uu_{&lt;i}, and \\mathbf\\uu_{\\le i} the context with the current observation.\n\nprior p_Z above becomes \\priorZ\nposterior \\pZu above becomes \\posteriorZ\nproposal \\qZu above becomes \\proposalZ\n\nDecomposing the KL into two pieces (Leaving R+D as a single term), we can write the KL as:2\n2 where the first step is since\n\n\\begin{aligned}\n&\\log \\frac{\\posteriorZ(z)}{\\priorZ(z)}\\\\\n&= \\log \\frac{p({z\\mid \\mathbf\\uu_{&lt; i},\\uu_i})}{p({z\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\log \\frac{p({z,\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}{p({z\\mid \\mathbf\\uu_{&lt; i}})p({\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\log \\frac{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}{p({\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\surp{\\uu_i}+\\log{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\n\\end{aligned}\n\n\nand the second is since\n\n\\begin{aligned}\n&=  \\log\\frac1{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\\frac{\\proposalZ}{\\priorZ}\\\\\n&= \\log\\frac{q({z; \\mathbf\\uu_{\\le i}})}{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})p({z\\mid \\mathbf\\uu_{&lt; i}})}\n\\end{aligned}\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n&= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\proposalZ}}\n= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\priorZ}\\frac{\\priorZ}{\\proposalZ}}\\\\\n&= \\surp{\\uu_i}-\\E_{\\posteriorZ}{\\log\\frac1{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\\frac{\\priorZ}{\\proposalZ}}\\\\\n&= \\surp{\\uu_i}-\\E_{\\posteriorZ}{\\log\\frac{q({z; \\mathbf\\uu_{\\le i}})}{p({z,\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}}\n\\end{aligned}\n\nOr, breaking the KL into three pieces\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n    &= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\proposalZ}}\n    = \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\priorZ}\\frac{\\priorZ}{\\proposalZ}}\\\\\n    &= \\KL{\\posteriorZ}{\\priorZ}\n        - \\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}\\\\\n    &= \\underbrace{\\log \\frac1{p(\\uu_i\\mid\\mathbf\\uu_{&lt; i})}}_{\\surp{\\uu}}\n        - \\underbrace{\\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}}_{\\Ri}\n        - \\underbrace{\\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}}_{\\Dqi}\n\\end{aligned}\n\nWe can also write out the surprisal term as the joint marginalized over the prior meanings:\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n    &= \\underbrace{\n        \\log \\frac1{\\E_{\\priorZ} {p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}}\n    }_{\\surp{\\uu}}\n        - \\underbrace{\n            \\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}\n        }_{\\Ri}\n        - \\underbrace{\n            \\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}\n        }_{\\Dqi}\n\\end{aligned}\n\nLet’s look at the first two terms above (the q=prior situation), and let’s break down the posterior into prior and likelihood (Bayes), recalling that the negative log marginal likelihood is the surprisal:\n\n\\posteriorz =\n\\frac{\\priorz p(\\uu_i\\mid z,\\mathbf\\uu_{&lt; i})}\n{p(\\uu_i\\mid\\mathbf\\uu_{&lt;i})}\n= e^{\\surp{\\uu_i}}\n      \\priorz p(\\uu_i\\mid z,\\mathbf\\uu_{&lt; i})\n\nSo,\n\n\\begin{align}\n\\KL{\\posteriorZ}{\\priorZ}\n&= \\overbrace{\n\\log \\frac1{p(\\uu_i\\mid \\mathbf\\uu_{&lt;i})}\n}^{\\surp{\\uu_i}}\n- \\overbrace{\n\\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}\n}^{\\Ri}\n\\\\\n&=  \\log \\frac1{\\E_{\\priorZ} {p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}}\\\\\n&\\quad - e^{\\surp{\\uu_i}}\\E_{\\priorZ}p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}\n\\end{align}\n\nThis is complicated looking, but one thing it means is if the prior is a one-hot/Dirac delta entirely concentrated on some value {\\cPRIO z'}, then the KL is zero.3\n3 Note in this degenerate case surprisal = negative log likelihood of \\cPRIO z'.\n\\KL{\\posteriorZ}{{\\cPRIO \\delta_{z'}}} =\n\\log \\frac1{ p(\\uu_i\\mid z',\\mathbf\\uu_{&lt; i})}\n- \\log\\frac1{p(\\uu_i\\mid z',\\mathbf\\uu_{&lt; i})} = 0\n\n\nWhat does it mean?\n\\KL{\\posteriorZ}{\\proposalZ} is the magnitude of the belief update from the proposal \\proposalZ to the posterior \\posteriorZ. Importance sampling is ideally exponential in this quantity (if such an algorithm exists). This quantity can be broken down into three pieces as KL = S – R – D.\n\nS (nonnegative) is the surprisal of \\uu_i, how unexpected it is (under the true prior)\nR (nonnegative) measures the nondeterminism in converting from meanings in the posterior to \\uu_i. If, under the posterior, \\uu_i is a certainty, then R is zero.\nD (positive or negative) measures how helpful it is to use the proposal \\proposalZ rather than the prior \\priorZ for this particular \\uu_i and context. Positive if helpful, zero if not at all helpful, negative if misleading.\n\n\n\nRelationship to lossy-context surprisal\nBy Jensen’s inequality (note -\\log(\\cdot) is concave up):\n\n\\surp{\\uu}\n%= -\\log p(\\uu_i\\mid\\mathbf\\uu_{&lt; i})\n= -\\log \\E_{\\priorZ}{p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}\n\\le \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}\\\\\n%= \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z})}\\quad\\text{if }U_i\\indep U_{\\le i} \\mid {\\cPRIO Z}\\\\\n\nIf we interpret the latent variable \\cPRIO z \\sim \\priorZ as a lossy/noised version of the context, and make an independence assumption, U_i\\indep U_{\\le i} \\mid {\\cPRIO Z}, then surprisal is upperbounded by lossy-context surprisal \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z})}, as defined in Futrell et al. (2020). \n\n\n\n\n\n\nPossible independence assumptions\n\n\n\n\n\n\nFor the S term: we might want to add an assumption that U_i\\indep U_{\\le i} \\mid Z_{i-1} (motivated by a model wherein \\cPOST z \\sim \\posteriorZ contains all the useful information from \\uu_{\\le i})\nThis assumption would yield that p(\\uu_i\\mid\\mathbf\\uu_{&lt; i}) = \\E_{\\priorZ} p(\\uu_i\\mid {\\cPRIO z}), so:\n\n\\surp{\\uu_i} = \\log \\frac1{\\E_{\\priorZ}{p(\\uu_i\\mid {\\cPRIO z})}}\n\nFor the R term: If we assume that U_i\\indep U_{\\le i} \\mid Z_i (this might be somewhat less obviously reasonable to assume), then we get something similar for R:\n\nR_{\\posteriorZ}(\\uu) = \\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z})}}\n\n\n\n\n\n\n\n\n\nKL theory vs (LC-)surprisal theory\nWe have the decomposition\n\n\\KL\\posteriorZ\\proposalZ\n= \\surp{\\uu} - \\left(\\Ri + \\Dqi\\right)\n\n\nSurprisal theory models difficulty as \\approx\\mathrm{S}. Generalizing, we can describe surprisal theory as difficulty \\approx f(\\mathrm{S}), for monotonic increasing f (not necessarily linear).\nWe would like to propose KL-theory which models difficulty as \\approx f(\\mathrm{D_{KL}}) instead (motivated by algorithmic complexity of sampling being \\approx e^{\\mathrm{D_{KL}}}).\n\nQ: When do these make different predictions?\nA: When \\surp{\\uu} is high, but [\\Ri+\\Dqi] is similarly high. Then surprisal theory predicts \\uu is difficult, and KL theory predicts it is not.\nThe possible cases leading to high surprisal but low KL:\n\nAssume D is negligible. We get low KL when \\uu remains unpredictable on average even when given the latent z_i (which encodes information about y_i). Intuitively: when the latent variable forgets/misrepresents the identity of y_i. For example, a perceived production error/typo?\nAssume R is negligible. We get low KL when \\proposalZ is much better than \\priorZ. That is, when your smart proposal gives a large reduction in excess surprise.\nWhen both of the above happen simultaneously."
  },
  {
    "objectID": "notebooks/difficulty-surprisal-summary.html#kl-theory-vs-lc-surprisal-theory",
    "href": "notebooks/difficulty-surprisal-summary.html#kl-theory-vs-lc-surprisal-theory",
    "title": "Difficulty and surprisal",
    "section": "KL theory vs (LC-)surprisal theory",
    "text": "KL theory vs (LC-)surprisal theory\nWe have the decomposition\n\n\\KL\\posteriorZ\\proposalZ\n= \\surp{\\uu} - \\left(\\Ri + \\Dqi\\right)\n\n\nSurprisal theory models difficulty as \\approx\\mathrm{S}. Generalizing, we can describe surprisal theory as difficulty \\approx f(\\mathrm{S}), for monotonic increasing f (not necessarily linear).\nWe would like to propose KL-theory which models difficulty as \\approx f(\\mathrm{D_{KL}}) instead (motivated by algorithmic complexity of sampling being \\approx e^{\\mathrm{D_{KL}}}).\n\nQ: When do these make different predictions?\nA: When \\surp{\\uu} is high, but [\\Ri+\\Dqi] is similarly high. Then surprisal theory predicts \\uu is difficult, and KL theory predicts it is not.\nThe possible cases leading to high surprisal but low KL:\n\nAssume D is negligible. We get low KL when \\uu remains unpredictable on average even when given the latent z_i (which encodes information about y_i). Intuitively: when the latent variable forgets/misrepresents the identity of y_i. For example, a perceived production error/typo?\nAssume R is negligible. We get low KL when \\proposalZ is much better than \\priorZ. That is, when your smart proposal gives a large reduction in excess surprise.\nWhen both of the above happen simultaneously."
  },
  {
    "objectID": "notebooks/difficulty-surprisal-summary.html#discussion",
    "href": "notebooks/difficulty-surprisal-summary.html#discussion",
    "title": "Difficulty and surprisal",
    "section": "Discussion",
    "text": "Discussion\n\nThe number of samples from p_Z for an IS estimate of \\pZu is e^{\\KL{\\pZu}{p_Z}}.\nThe number of samples from \\qZu for an IS estimate of \\pZu is e^{\\KL{\\pZu}{\\qZu}}.\n\nIf the observable U is assumed to be a deterministic function of the latent Z (as in Levy (2008), where latent state consists partially of the observable string), then \\R=0, and thus \\KL{\\pZu}{p_Z}=-\\log p(\\uu). Thus, IS (which, with a binary likelihood function becomes simply rejection sampling, see Chen, 2005), will require e^{-\\log p(\\uu)}=\\frac1{p(\\uu)} samples.\nThis gives us a clear prediction for runtime being an exponential function of surprisal.\nHowever, there are two issues with this:\n\nIf we want to model the latent states as not containing the observations within them, then U will not be a deterministic function of Z in general (and so \\R may be nonzero?), and thus the runtime will depend on \\R as well as suprisal.\nIndependent of the previous point, if we want to sample from something smarter than simply the prior, then the number of samples needed will be e^{\\KL{\\pZu}{\\qZu}}, and so the runtime will depend on \\Dq (and possibly also \\R)\n\n\nRequirements of a parametric relationship\nWhat properties of \\qZu or \\R would have to hold to have runtime be a particular parametric relationship with surprisal (such as a linear one)?\nIf we don’t assume anything particular about \\qZu, then, the sampling-based update cost is\n\n\\begin{aligned}\n\\operatorname{cost}(\\uu)\n    &= e^{\\KL\\pZu\\qZu}\\\\\n    &= e^{\\surp{\\uu} - \\R - \\Dq}\n\\end{aligned}\n\nIf \\operatorname{cost}(\\uu) = f(\\surp{\\uu}) for some linking function f, then\n\n\\begin{aligned}\nf(\\surp{\\uu})\n    &= e^{\\surp{\\uu} - \\R - \\Dq}\\\\\n\\log f(\\surp{\\uu})\n    &= \\surp{\\uu} - \\R - \\Dq\\\\\n\\end{aligned}\n\nso\n\n\\begin{aligned}\n\\R + \\Dq\n    &= \\surp{\\uu} - \\log f(\\surp{\\uu}) \\\\\n    % &= - \\log\\left(p(\\uu) f( \\surp{\\uu})\\right) \\\\\ne^{\\R + \\Dq}\n    &= \\frac1{p(\\uu) f(\\surp{\\uu})}\n\\end{aligned}\n\nIf we assume we’re sampling from the prior (so \\qZu=p_Z), then \\Dq=0, so\n\n\\R = \\surp{\\uu} - \\log f(\\surp{\\uu})\n\n\n\nParametric relationships. \nThus, if we measure cost as reading time (RT), and fit RT as a function of surprisal, \\operatorname{cost}(\\uu) = \\operatorname{RT}(\\uu) = f_\\mathrm{GAM}(\\surp{\\uu}), we can inspect the implies about the term G\\coloneqq \\R + \\Dq, by simply subtracting the log fit RT value from the surprisal.\n\nG(s) = s - \\log f_\\mathrm{GAM}(s)"
  },
  {
    "objectID": "notebooks/difficulty-surprisal-summary.html#notes",
    "href": "notebooks/difficulty-surprisal-summary.html#notes",
    "title": "Difficulty and surprisal",
    "section": "Notes",
    "text": "Notes\n\nThe number of samples from p_Z for an IS estimate of \\pZu is e^{\\KL{\\pZu}{p_Z}}.\nThe number of samples from \\qZu for an IS estimate of \\pZu is e^{\\KL{\\pZu}{\\qZu}}.\n\nIf the observable U is assumed to be a deterministic function of the latent Z (as in Levy (2008), where latent state consists partially of the observable string), then \\R=0, and thus \\KL{\\pZu}{p_Z}=-\\log p(\\uu). Thus, IS (which, with a binary likelihood function becomes simply rejection sampling, see Chen, 2005), will require e^{-\\log p(\\uu)}=\\frac1{p(\\uu)} samples.\nThis gives us a clear prediction for runtime being an exponential function of surprisal.\nHowever, there are two issues with this:\n\nIf we want to model the latent states as not containing the observations within them, then U will not be a deterministic function of Z in general (and so \\R may be nonzero?), and thus the runtime will depend on \\R as well as suprisal.\nIndependent of the previous point, if we want to sample from something smarter than simply the prior, then the number of samples needed will be e^{\\KL{\\pZu}{\\qZu}}, and so the runtime will depend on \\Dq (and possibly also \\R)\n\n\nRequirements of a parametric relationship\nWhat properties of \\qZu or \\R would have to hold to have runtime be a particular parametric relationship with surprisal (such as a linear one)?\nIf we don’t assume anything particular about \\qZu, then, the sampling-based update cost is\n\n\\begin{aligned}\n\\operatorname{cost}(\\uu)\n    &= e^{\\KL\\pZu\\qZu}\\\\\n    &= e^{\\surp{\\uu} - \\R - \\Dq}\n\\end{aligned}\n\nIf \\operatorname{cost}(\\uu) = f(\\surp{\\uu}) for some linking function f, then\n\n\\begin{aligned}\nf(\\surp{\\uu})\n    &= e^{\\surp{\\uu} - \\R - \\Dq}\\\\\n\\log f(\\surp{\\uu})\n    &= \\surp{\\uu} - \\R - \\Dq\\\\\n\\end{aligned}\n\nso\n\n\\begin{aligned}\n\\R + \\Dq\n    &= \\surp{\\uu} - \\log f(\\surp{\\uu}) \\\\\n    % &= - \\log\\left(p(\\uu) f( \\surp{\\uu})\\right) \\\\\ne^{\\R + \\Dq}\n    &= \\frac1{p(\\uu) f(\\surp{\\uu})}\n\\end{aligned}\n\nIf we assume we’re sampling from the prior (so \\qZu=p_Z), then \\Dq=0, so\n\n\\R = \\surp{\\uu} - \\log f(\\surp{\\uu})\n\n\n\nParametric relationships. \nThus, if we measure cost as reading time (RT), and fit RT as a function of surprisal, \\operatorname{cost}(\\uu) = \\operatorname{RT}(\\uu) = f_\\mathrm{GAM}(\\surp{\\uu}), we can inspect the implies about the term G\\coloneqq \\R + \\Dq, by simply subtracting the log fit RT value from the surprisal.\n\nG(s) = s - \\log f_\\mathrm{GAM}(s)"
  },
  {
    "objectID": "notebooks/difficulty-KL-summary.html",
    "href": "notebooks/difficulty-KL-summary.html",
    "title": "Bayesian incremental parsing",
    "section": "",
    "text": "\\global\\def\\I#1{\\operatorname{I}(#1)}\n\\global\\def\\H#1{\\operatorname{H}(#1)}\n\\global\\def\\surp#1{\\operatorname{surp}(#1)}\n\\global\\def\\DIV#1#2#3#4{\\operatorname{D_{#1}}(#3#2#4)}\n\\global\\def\\KL#1#2{\\DIV{KL}\\|{#1}{#2}}\n\\global\\def\\E{\\operatorname*{\\mathbb{E}}}\n\\global\\def\\dee{\\mathop{\\mathrm{d}\\!}}\n\\global\\def\\var#1{\\operatorname{\\mathbb{V}}(#1)}\n\\global\\def\\Var#1#2{\\operatorname{\\mathbb{V}}\\!\\!{}_{#1}(#2)}\n\\global\\def\\indep{\\bot\\!\\!\\!\\bot}\n\\global\\def\\uu{\\breve u}"
  },
  {
    "objectID": "notebooks/difficulty-KL-summary.html#difficulty-as-kl",
    "href": "notebooks/difficulty-KL-summary.html#difficulty-as-kl",
    "title": "Bayesian incremental parsing",
    "section": "Difficulty as KL",
    "text": "Difficulty as KL\nSay we want to approximate some target distribution p using samples from some other distribution q.\nIf we use samples from q to approximate p, using importance sampling, the number of samples necessary and sufficient for for an accurate approximation is exponential in the relative entropy:\n\n\\#\\text{samples}_{\\mathrm{IS}(p\\leftarrow q)} \\approx e^{\\KL p q}\n\nFor proof, see Chatterjee and Diaconis (2018), who show that taking e^{\\KL p q} samples from q is a necessary and sufficient condition for the absolute value of the error to be close to zero with high probability.\n\n\n\n\n\n\nConditions for Chatterjee result\n\n\n\nTechnically, this result only obtains when the log density of p wrt q is likely to be concentrated around its expected value.\nThat is, that \\log \\dpdq(Z) (where Z\\sim p) is concentrated around \\E_p{\\log \\dpdq(Z)} = \\KL p q. Or equivalently, that \\dpdq(Z')\\log \\dpdq(Z') (where Z'\\sim q) is concentrated around \\E_q{\\dpdq(Z')\\log \\dpdq(Z')} = \\KL p q. Roughly, this requirement is that the expected variance in importance weights is small.\nMore precisely, their result says that in order to bound the L^1-error of the estimate close to zero with high probability, a sample size of\n\ne^{\\KL p q + \\mathcal{O}(s)} is sufficient\ne^{\\KL p q - \\mathcal{O}(s)} is necessary\n\nwhere s is the typical order of fluctuations in \\log \\dpdq(Z) around its expected value, \\KL p q.\n\n\nSo, for a sampling-based mechanism we can define the update cost as this exponentiatied relative entropy:\n\n\\mathrm{cost} \\coloneqq e^{\\KL p q}\n\nalso note, many empirical studies of human reading time as a function of surprisal log transform the response variable, which in fact implies an exponential relationship like this. this is acknowledged, if only rarely (for example, in Oh, Yue, and Schuler 2024)\n\n\n\n\n\n\nOr… choose your fave divergence:\n\n\n\n\n\nThe number of samples necessary for IS can also be related to other divergences (other than KL). See Agapiou et al. (2017) discuss the \\chi^2-divergence, and also Sanz-Alonso (2018) (who also dicsuss Hellinger and TV).\n\n\\DIV{\\chi^2}\\| p q = \\E_q{((\\dpdq)^2)}-1 = \\E_p{(\\dpdq)}-1\n\\KL p q = \\E_q{(\\dpdq\\log\\dpdq)} =\\E_p{(\\log\\dpdq)}\n\nNote it’s clear by Jensen’s that e^{\\mathrm{KL}}\\le \\mathrm{D}_{\\chi^2} + 1.\nThe result discussed in Agapiou et al. (2017) is:\n\nThey define\n\nunnormalized density g as \\frac1{\\E_q{g}}g(\\cdot)\\coloneqq\\dpdq(\\cdot), and\ndenote with \\rho the second moment of this RN-derivative: \\rho\\coloneqq \\E_q(\\dpdq^2) = \\frac{\\E_q{g^2}}{(\\E_q{g})^2}\n\n\\rho \\ge 1 since (\\E_q{g})^2\\le\\E_q{\\mathbf1^2}\\E_q{g^2}= \\E_q{g^2} by Cauchy-Schwarz.\n\n\nTheir main result is that both bias and MSE of IS are \\approx\\rho/N\nThis gives, for some fixed accuracy, the sufficient sample size in terms of KL and chi-squared divergences as:\n\ngrowing linearly in chi-squared, since \\DIV{\\chi^2}\\| p q = \\E_q{((\\dpdq)^2)}-1 = \\rho-1\nexponentially in KL, since \\KL p q= \\E_q{(\\dpdq\\log\\dpdq)} =\\E_p{(\\log\\dpdq)}\\le\\log\\E_p{(\\dpdq)} = \\log \\E_q{((\\dpdq)^2)}=\\log \\rho, by Jensen’s ineq. so e^{\\KL p q} \\le \\rho\n\n\nThere are many references for the relationships between KL and \\chi^2 and other probability metrics/divergences (see Gibbs and Su 2002; Sanz-Alonso 2018).\n\\implies For us, the point is: we could alternatively say \\mathrm{cost} \\coloneqq \\DIV{\\chi^2}\\| p q, and it would amount to something roughly similar as \\mathrm{cost} \\coloneqq e^{\\KL p q}.\n\n\n\n\nSetup\n\nLet p_{Z,U} be a hypothetical joint distribution for Z a latent random variable, and U an observable random variable. We don’t assume we have any access to this distribution, but we’ll make use of the following derived distributions:\n\nthe marginal p_Z (the prior distribution on Z)\nthe conditional \\pZu, for any fixed outcome \\uu of U (the posterior on Z).\nAssume \\pZu\\ll p_Z, that is \\pZu=0 anywhere p_Z=0.1\n\nLet the proposal \\qZu be some other distribution over Z, which may depend on the outcome \\uu. Again assume \\pZu\\ll\\qZu.\n\n1 A sufficient but not quite necessary condition for IS weights to be well defined. Also a natural property in a Bayesian setting where the posterior is a reweighted version of the prior, so can’t put mass outside the support of the prior.I’m writing a breve on the outcome variable just to denote that it is fixed.\n\n\nDecomposing KL between prior and posterior\nThe relative entropy of prior p_Z with respect to posterior \\pZu can be written as:\n\n\\begin{aligned}\n\\KL{\\pZu}{p_Z}\n    &= \\E_{\\pZu}{ \\log\\frac{\\pzu}{p(z)} }\n     = \\E_{\\pZu}{ \\log\\frac{p(z,\\uu)}{p(z)p(\\uu)} }\\\\\n    &= \\E_{\\pZu}{ \\log\\frac{p(\\uu\\mid z)}{p(\\uu)} }\\\\\n    &= \\log \\frac1{p(\\uu)}\n     + \\E_{\\pZu}{\\log p(\\uu\\mid z)} \\\\\n    &= \\underbrace{-\\log p(\\uu)}_{\\surp{\\uu}}\n     - \\underbrace{\\E_{\\pZu}{-\\log p(\\uu\\mid z)}}_{\\coloneqq\\ \\R}\n\\end{aligned}\n\nSo the relative entropy between prior and posterior consists of\n\nthe surprisal, \\surp{\\uu}\\ge0,\nminus a term which we denote \\R, which I’ll call ‘reconstruction information’. It is the posterior-expected conditional surprisal… the number of bits by which surprisal of the observation exceeds the size of the belief update it causes. We can think of \\R measuring how many bits of surprisal are irrelevant to belief-updating.\n\nNote that 0\\le\\R\\le \\surp{\\uu}\n\n\n\n\nWe could define a more general \\Rs{q} generally as\n\nDefinition 1 \n\\Rs{q} \\coloneqq \\E_{z\\sim q}{\\log \\frac1{p(\\uu|z,\\breve c)}}\n\\tag{1}\nwhere q is any distribution over the latent variable Z (such as a proposal, or variational approximation to the posterior), and \\uu is a fixed observed outcome of U. Both the probability and the proposal may optionally depend on context \\breve c.\n\n\n\\mathrm{R}_{\\qZu}(\\uu) (for \\qZu an approximation to the posterior \\pZu) is sometimes called the (negative) reconstruction error in variational inference literature (Liang et al. 2018; Dehaene et al. 2019) or original autoencoders literature (such as Vincent et al. 2010).\nin a bounded-rational decision-making framework (as in Ortega and Braun 2013; Genewein et al. 2015), the negative of this quantity is the expected utility.\n\nNote \\R=0 is a necessary and sufficient condition for the relative entropy between prior and posterior equalling surprisal:\n\n\\KL{\\pZu}{p_Z}=-\\log p(\\uu) \\quad\\iff\\quad \\R=0\n\nU being a deterministic function of Z is a sufficient condition for this to hold (this is the assumption made in the proofs of the equivalence of KL and surprisal, such as in Levy 2008).\n\n\nDecomposing KL between proposal and posterior\nIf instead of sampling from the prior, we were sampling from some proposal distribution proposal \\qZu, then the we can break down that divergence with respect to posterior \\pZu, to get an additional term:\n\n\\begin{aligned}\n\\KL{\\pZu}{\\qZu}\n    &= \\E_{\\pZu}\\log\\frac{\\pzu}{\\qzu}\\\\\n    &= \\E_{\\pZu}\\log\\frac{\\pzu}{p(z)}\\frac{p(z)}{\\qzu}\\\\\n    &= \\KL{\\pZu}{p_Z}\n        + \\E_{\\pZu}\\log\\frac{p(z)}{\\qzu}\\\\\n    &= {\\surp{\\uu}} - {\\R}\n       - \\underbrace{\\E_{\\pZu}\\log\\frac{\\qzu}{p(z)}}_{\\coloneqq\\ \\Dq}\\\\\n\\end{aligned}\n\nwhere the term \\Dq, quantifies how much better \\qZu is than p_Z for estimating \\pZu. More precisely, as a difference in KLs, it represents the reduction in excess surprise resulting from using \\qZu instead of p_Z, when the actual distribution is \\pZu:\n\n\\Dq = \\KL{\\pZu}{p_Z} - \\KL{\\pZu}{\\qZu}\n\nEquivalently it can be viewed as measuring the reduction in cross-entropy: \n\\Dq = \\E_{\\pZu}\\log\\frac{1}{p(z)}- \\E_{\\pZu}\\log\\frac{1}{\\qzu}=\\H{\\pZu,p_Z}-\\H{\\pZu,\\qZu}\n\nInterpretation:\n\n\\Dq &gt; 0 if \\qZu is better than p_Z for estimating \\pZu and\n\\Dq &lt; 0 if \\qZu is worse than p_Z for estimating \\pZu, and\n\\Dq = 0 if \\qZu = p_Z.\n\nBounds: -\\infty\\le\\Dq\\le\\KL{\\pZu}{p_Z}={\\surp{\\uu}} - {\\R}.\n\n\n\nJust to get a visual picture of the boundaries, here are level surfaces of D_\\mathrm{KL} = S - (R + D)\n\n\n\n\n\n\n\n\nGeneral identity for change in KLs\n\n\n\n\n\nThis could be seen as a result of the general identity\n\n\\KL P Q = \\KL P R - \\E_P\\log\\frac{\\dee Q}{\\dee R}\n\nfor any measures P, Q, R, on the same space with P\\ll R and P\\ll Q.\n\n\n\nThis could alternatively be written as\n\n\\begin{aligned}\n\\KL{\\pZu}{\\qZu}\n    &= \\surp{\\uu}\n    - \\left(\n        \\E_{\\pZu}{\\log\\frac1{p(\\uu\\mid z)}}\n        + \\E_{\\pZu}{\\log\\frac{\\qzu}{p(z)}}\n    \\right)\\\\\n    &= \\surp{\\uu}\n    + \\underbrace{\\E_{\\pZu}{\\log\\frac{p(z,\\uu)}{\\qzu}}}_{-\\R-\\Dq}\n\\end{aligned}\n\n\n\n\n\n\n\nComparison with VAE and ‘Bayesian Surprise’\n\n\n\n\n\nThe expectation here looks a lot like \\operatorname{ELBO}_\\qZu(\\uu)\\coloneqq\\E_{\\qZu}\\left[\\log\\frac{p(z, \\uu)}{\\qzu}\\right] in variational inference (aka negative variational free energy), but they differ in what the expectation is taken with respect to.\nFor comparison, using the same notation, the usual derivation in VAE looks like this. See, e.g., (Kingma 2017; though this math goes back before VAEs at least to Dayan et al. 1995, eq 2.5):\n\n\\begin{aligned}\n\\log p(\\uu)\n    &= \\KL{\\qZu}{\\pZu}\n    + \\overbrace{\\E_{\\qZu}{\\log\\frac{p(z, \\uu)}{\\qzu}}}^{\\operatorname{ELBO}_\\qZu(\\uu)}\\\\\n\\end{aligned}\n\nSo, we get a very similar equation to the above for a KL in the opposite direction:\n\n\\begin{aligned}\n\\KL{\\qZu}{\\pZu}\n    &= -\\surp{\\uu}\n    - \\E_{\\qZu}{\\log\\frac{p(z, \\uu)}{\\qzu}}\n\\end{aligned}\n\nor\n\n\\begin{aligned}\n\\KL{\\qZu}{\\pZu}\n    &= - \\surp{\\uu}  \n    - \\E_{\\qZu}{\\log\\frac{p(\\uu\\mid z)p(z)}{\\qzu}}\\\\\n    &= -\\surp{\\uu}\n    - \\E_{\\qZu}{\\log p(\\uu\\mid z)}\n    + \\KL{\\qZu}{p_Z}\\\\\n    &= - \\surp{\\uu}\n    + \\underbrace{\\E_{\\qZu}{-\\log p(\\uu\\mid z)}}_{\\text{neg. reconstr. error }\\operatorname{R}_{\\qZu}(\\uu)}\n    + \\underbrace{\\KL{\\qZu}{p_Z}}_{\\text{regularizer}}\\\\\n\\end{aligned}\n\nIn this setup, \\qZu is chosen in order to maximize the ELBO. The ELBO consists of two components, the reconstruction error (which is a negative-log-likelihood term, to be maximized) minus the KL between \\qZu and the prior (which can be seen as a regularization term, to be minimized).\nThese equations look very similar, but it is very different from the case we are interested in, where expectations are taken with respect to the true unknown posterior.\n\nWhile this direction of KL (with expectation over \\qZu) may be the “backward” direction from the point of view of the connection with sampling, it might be important to understand whether/how it relates to processing effort in some way, since when \\qZu=p_Z, this is precisely the divergence used as “Bayesian Surprise” (Baldi 2002; Baldi and Itti 2010) (see lit review). Might be that this was chosen purely for computational convenience, but even so, worth understanding what it implies.\nDepending on which direction of KL we choose to use we have two ways of expressing the surprisal:\n\n\\begin{aligned}\n\\surp{\\uu}\n    &= \\overbrace{\\E_{\\qZu}{\\log\\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{-\\operatorname{ELBO}_\\qZu(\\uu)}\n    - \\KL{\\qZu}{\\pZu}\\\\\n\\surp{\\uu}\n    &= \\underbrace{\\E_{\\pZu}{\\log \\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}_{\\R+\\Dq}\n    + \\KL{\\pZu}{\\qZu}\n\\end{aligned}\n\nor, put another way, with R, D, and the ELBO we can express the sum of the KL and reverse-KL \\DIV{J},pq\\coloneqq \\KL pq  + \\KL qp = \\E_q{(\\dpdq-1)\\log\\dpdq} = \\DIV{\\lambda t.(t-1)\\log t}\\|pq (this symmetric f-divergence is actually the one originally proposed by K&L, and earlier defined by Jeffreys):\n\n0\\le\n{\\DIV{J},{\\pZu}{\\qZu}} =\n\\overbrace{\\E_{\\qZu}{\\log\\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{-\\operatorname{ELBO}_\\qZu(\\uu)}\n- \\overbrace{\\E_{\\pZu}{\\log \\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{\\R+\\Dq}\n\nThis is a bit pointless in the abstract, since,\\Dq can be positive or negative, hence no bounds are implied. Yet, when we just consider the case where \\qZu=p_Z, then D = 0, and we have\n\n0\\le\n\\DIV{J},{\\pZu}{p_Z} =\n\\overbrace{\\E_{p_Z}{\\log\\frac{1}{p(\\uu\\mid z)}}}^{-\\operatorname{ELBO}_{p_Z}(\\uu)=\\operatorname{R}_{p_Z}(\\uu)}\n- \\overbrace{\\E_{\\pZu}{\\log \\frac{1}{p(\\uu\\mid z)}}}^{\\R}\n\nwhere all the terms are nonnegative, so then we can say the magnitude of the ELBO (or, free energy, I guess) is in fact a upper bound on the magnitude of surprisal, which is an upper bound on R.\n\n0\\le\\R\\le\\surp{\\uu}\\le-\\operatorname{ELBO}_{p_Z}(\\uu)=\\operatorname{R}_{p_Z}(\\uu)\n\nIs it useful to say that surprisal is bounded between R below and prior-reconstruction error/free energy above?\n\n\n\n\n\nIncremental version\nAbove we’re assuming all the probabilities depend on a (notationally suppressed) ‘context’ random var. Now let’s write out the same derivation but with the observation being explicitly the ith item in a sequence \\uu_1, \\uu_2, \\ldots. So the ‘context’ is \\mathbf\\uu_{&lt;i}, and \\mathbf\\uu_{\\le i} the context with the current observation.\n\nprior p_Z above becomes \\priorZ\nposterior \\pZu above becomes \\posteriorZ\nproposal \\qZu above becomes \\proposalZ\n\nDecomposing the KL into two pieces (Leaving R+D as a single term), we can write the KL as:2\n2 where the first step is since\n\n\\begin{aligned}\n&\\log \\frac{\\posteriorZ(z)}{\\priorZ(z)}\\\\\n&= \\log \\frac{p({z\\mid \\mathbf\\uu_{&lt; i},\\uu_i})}{p({z\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\log \\frac{p({z,\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}{p({z\\mid \\mathbf\\uu_{&lt; i}})p({\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\log \\frac{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}{p({\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\surp{\\uu_i}+\\log{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\n\\end{aligned}\n\n\nand the second is since\n\n\\begin{aligned}\n&=  \\log\\frac1{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\\frac{\\proposalZ}{\\priorZ}\\\\\n&= \\log\\frac{q({z; \\mathbf\\uu_{\\le i}})}{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})p({z\\mid \\mathbf\\uu_{&lt; i}})}\n\\end{aligned}\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n&= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\proposalZ}}\n= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\priorZ}\\frac{\\priorZ}{\\proposalZ}}\\\\\n&= \\surp{\\uu_i}-\\E_{\\posteriorZ}{\\log\\frac1{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\\frac{\\priorZ}{\\proposalZ}}\\\\\n&= \\surp{\\uu_i}-\\E_{\\posteriorZ}{\\log\\frac{q({z; \\mathbf\\uu_{\\le i}})}{p({z,\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}}\n\\end{aligned}\n\nOr, breaking the KL into three pieces\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n    &= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\proposalZ}}\n    = \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\priorZ}\\frac{\\priorZ}{\\proposalZ}}\\\\\n    &= \\KL{\\posteriorZ}{\\priorZ}\n        - \\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}\\\\\n    &= \\underbrace{\\log \\frac1{p(\\uu_i\\mid\\mathbf\\uu_{&lt; i})}}_{\\surp{\\uu}}\n        - \\underbrace{\\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}}_{\\Ri}\n        - \\underbrace{\\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}}_{\\Dqi}\n\\end{aligned}\n\nWe can also write out the surprisal term as the joint marginalized over the prior meanings:\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n    &= \\underbrace{\n        \\log \\frac1{\\E_{\\priorZ} {p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}}\n    }_{\\surp{\\uu}}\n        - \\underbrace{\n            \\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}\n        }_{\\Ri}\n        - \\underbrace{\n            \\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}\n        }_{\\Dqi}\n\\end{aligned}\n\nLet’s look at the first two terms above (the q=prior situation), and let’s break down the posterior into prior and likelihood (Bayes), recalling that the negative log marginal likelihood is the surprisal:\n\n\\posteriorz =\n\\frac{\\priorz p(\\uu_i\\mid z,\\mathbf\\uu_{&lt; i})}\n{p(\\uu_i\\mid\\mathbf\\uu_{&lt;i})}\n= e^{\\surp{\\uu_i}}\n      \\priorz p(\\uu_i\\mid z,\\mathbf\\uu_{&lt; i})\n\nSo,\n\n\\begin{align}\n\\KL{\\posteriorZ}{\\priorZ}\n&= \\overbrace{\n\\log \\frac1{p(\\uu_i\\mid \\mathbf\\uu_{&lt;i})}\n}^{\\surp{\\uu_i}}\n- \\overbrace{\n\\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}\n}^{\\Ri}\n\\\\\n&=  \\log \\frac1{\\E_{\\priorZ} {p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}}\\\\\n&\\quad - e^{\\surp{\\uu_i}}\\E_{\\priorZ}p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}\n\\end{align}\n\nThis is complicated looking, but one thing it means is if the prior is a one-hot/Dirac delta entirely concentrated on some value {\\cPRIO z'}, then the KL is zero.3\n3 Note in this degenerate case surprisal = negative log likelihood of \\cPRIO z'.\n\\KL{\\posteriorZ}{{\\cPRIO \\delta_{z'}}} =\n\\log \\frac1{ p(\\uu_i\\mid z',\\mathbf\\uu_{&lt; i})}\n- \\log\\frac1{p(\\uu_i\\mid z',\\mathbf\\uu_{&lt; i})} = 0\n\n\nWhat does it mean?\n\\KL{\\posteriorZ}{\\proposalZ} is the magnitude of the belief update from the proposal \\proposalZ to the posterior \\posteriorZ. Importance sampling is ideally exponential in this quantity (if such an algorithm exists). This quantity can be broken down into three pieces as KL = S – R – D.\n\nS (nonnegative) is the surprisal of \\uu_i, how unexpected it is (under the true prior)\nR (nonnegative) measures the nondeterminism in converting from meanings in the posterior to \\uu_i. If, under the posterior, \\uu_i is a certainty, then R is zero.\nD (positive or negative) measures how helpful it is to use the proposal \\proposalZ rather than the prior \\priorZ for this particular \\uu_i and context. Positive if helpful, zero if not at all helpful, negative if misleading.\n\n\n\nRelationship to lossy-context surprisal\nBy Jensen’s inequality (note -\\log(\\cdot) is concave up):\n\n\\surp{\\uu}\n%= -\\log p(\\uu_i\\mid\\mathbf\\uu_{&lt; i})\n= -\\log \\E_{\\priorZ}{p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}\n\\le \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}\\\\\n%= \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z})}\\quad\\text{if }U_i\\indep U_{\\le i} \\mid {\\cPRIO Z}\\\\\n\nIf we interpret the latent variable \\cPRIO z \\sim \\priorZ as a lossy/noised version of the context, and make an independence assumption, U_i\\indep U_{\\le i} \\mid {\\cPRIO Z}, then surprisal is upperbounded by lossy-context surprisal \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z})}, as defined in Futrell, Gibson, and Levy (2020).\n\n\n\n\n\n\nPossible independence assumptions\n\n\n\n\n\n\nFor the S term: we might want to add an assumption that U_i\\indep U_{\\le i} \\mid Z_{i-1} (motivated by a model wherein \\cPOST z \\sim \\posteriorZ contains all the useful information from \\uu_{\\le i})\nThis assumption would yield that p(\\uu_i\\mid\\mathbf\\uu_{&lt; i}) = \\E_{\\priorZ} p(\\uu_i\\mid {\\cPRIO z}), so:\n\n\\surp{\\uu_i} = \\log \\frac1{\\E_{\\priorZ}{p(\\uu_i\\mid {\\cPRIO z})}}\n\nFor the R term: If we assume that U_i\\indep U_{\\le i} \\mid Z_i (this might be somewhat less obviously reasonable to assume), then we get something similar for R:\n\nR_{\\posteriorZ}(\\uu) = \\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z})}}\n\n\n\n\n\n\n\n\nKL theory vs (LC-)surprisal theory\nWe have the decomposition\n\n\\KL\\posteriorZ\\proposalZ\n= \\surp{\\uu} - \\left(\\Ri + \\Dqi\\right)\n\n\nSurprisal theory models difficulty as \\approx\\mathrm{S}. Generalizing, we can describe surprisal theory as difficulty \\approx f(\\mathrm{S}), for monotonic increasing f (not necessarily linear).\nWe would like to propose KL-theory which models difficulty as \\approx f(\\mathrm{D_{KL}}) instead (motivated by algorithmic complexity of sampling being \\approx e^{\\mathrm{D_{KL}}}).\n\nQ: When do these make different predictions?\nA: When \\surp{\\uu} is high, but [\\Ri+\\Dqi] is similarly high. Then surprisal theory predicts \\uu is difficult, and KL theory predicts it is not.\nThe possible cases leading to high surprisal but low KL:\n\nAssume D is negligible. We get low KL when \\uu remains unpredictable on average even when given the latent z_i (which encodes information about y_i). Intuitively: when the latent variable forgets/misrepresents the identity of y_i. For example, a perceived production error/typo?\nAssume R is negligible. We get low KL when \\proposalZ is much better than \\priorZ. That is, when your smart proposal gives a large reduction in excess surprise.\nWhen both of the above happen simultaneously."
  },
  {
    "objectID": "notebooks/difficulty-KL-summary.html#notes",
    "href": "notebooks/difficulty-KL-summary.html#notes",
    "title": "Bayesian incremental parsing",
    "section": "Notes",
    "text": "Notes\n\nThe number of samples from p_Z for an IS estimate of \\pZu is e^{\\KL{\\pZu}{p_Z}}.\nThe number of samples from \\qZu for an IS estimate of \\pZu is e^{\\KL{\\pZu}{\\qZu}}.\n\nIf the observable U is assumed to be a deterministic function of the latent Z (as in Levy (2008), where latent state consists partially of the observable string), then \\R=0, and thus \\KL{\\pZu}{p_Z}=-\\log p(\\uu). Thus, IS (which, with a binary likelihood function becomes simply rejection sampling, see Chen 2005), will require e^{-\\log p(\\uu)}=\\frac1{p(\\uu)} samples.\nThis gives us a clear prediction for runtime being an exponential function of surprisal.\nHowever, there are two issues with this:\n\nIf we want to model the latent states as not containing the observations within them, then U will not be a deterministic function of Z in general (and so \\R may be nonzero?), and thus the runtime will depend on \\R as well as suprisal.\nIndependent of the previous point, if we want to sample from something smarter than simply the prior, then the number of samples needed will be e^{\\KL{\\pZu}{\\qZu}}, and so the runtime will depend on \\Dq (and possibly also \\R)\n\n\nRequirements of a parametric relationship\nWhat properties of \\qZu or \\R would have to hold to have runtime be a particular parametric relationship with surprisal (such as a linear one)?\nIf we don’t assume anything particular about \\qZu, then, the sampling-based update cost is\n\n\\begin{aligned}\n\\operatorname{cost}(\\uu)\n    &= e^{\\KL\\pZu\\qZu}\\\\\n    &= e^{\\surp{\\uu} - \\R - \\Dq}\n\\end{aligned}\n\nIf \\operatorname{cost}(\\uu) = f(\\surp{\\uu}) for some linking function f, then\n\n\\begin{aligned}\nf(\\surp{\\uu})\n    &= e^{\\surp{\\uu} - \\R - \\Dq}\\\\\n\\log f(\\surp{\\uu})\n    &= \\surp{\\uu} - \\R - \\Dq\\\\\n\\end{aligned}\n\nso\n\n\\begin{aligned}\n\\R + \\Dq\n    &= \\surp{\\uu} - \\log f(\\surp{\\uu}) \\\\\n    % &= - \\log\\left(p(\\uu) f( \\surp{\\uu})\\right) \\\\\ne^{\\R + \\Dq}\n    &= \\frac1{p(\\uu) f(\\surp{\\uu})}\n\\end{aligned}\n\nIf we assume we’re sampling from the prior (so \\qZu=p_Z), then \\Dq=0, so\n\n\\R = \\surp{\\uu} - \\log f(\\surp{\\uu})\n\n\n\nParametric relationships. \nThus, if we measure cost as reading time (RT), and fit RT as a function of surprisal, \\operatorname{cost}(\\uu) = \\operatorname{RT}(\\uu) = f_\\mathrm{GAM}(\\surp{\\uu}), we can inspect the implies about the term G\\coloneqq \\R + \\Dq, by simply subtracting the log fit RT value from the surprisal.\n\nG(s) = s - \\log f_\\mathrm{GAM}(s)\n\n\n\nOptimal code-length\nWe’re interested in the question: How many bits of information are required to transform prior p_Z into posterior \\pZu?\nJust using bayes rule, we can break this down into three parts\n\n-\\log \\pzu\n= -\\log p(z) - \\log \\frac{p(\\uu\\mid z)}{p(\\uu)}\n= \\log p(\\uu) -\\log p(z) - \\log p(\\uu\\mid z)\n\nTBC"
  },
  {
    "objectID": "notebooks/difficulty-KL-summary_PUBLIC.html",
    "href": "notebooks/difficulty-KL-summary_PUBLIC.html",
    "title": "Bayesian incremental parsing",
    "section": "",
    "text": "\\global\\def\\I#1{\\operatorname{I}(#1)}\n\\global\\def\\H#1{\\operatorname{H}(#1)}\n\\global\\def\\surp#1{\\operatorname{surp}(#1)}\n\\global\\def\\DIV#1#2#3#4{\\operatorname{D_{#1}}(#3#2#4)}\n\\global\\def\\KL#1#2{\\DIV{KL}\\|{#1}{#2}}\n\\global\\def\\E{\\operatorname*{\\mathbb{E}}}\n\\global\\def\\dee{\\mathop{\\mathrm{d}\\!}}\n\\global\\def\\var#1{\\operatorname{\\mathbb{V}}(#1)}\n\\global\\def\\Var#1#2{\\operatorname{\\mathbb{V}}\\!\\!{}_{#1}(#2)}\n\\global\\def\\indep{\\bot\\!\\!\\!\\bot}\n\\global\\def\\uu{\\breve u}"
  },
  {
    "objectID": "notebooks/difficulty-KL-summary_PUBLIC.html#difficulty-as-kl",
    "href": "notebooks/difficulty-KL-summary_PUBLIC.html#difficulty-as-kl",
    "title": "Bayesian incremental parsing",
    "section": "Difficulty as KL",
    "text": "Difficulty as KL\nSay we want to approximate some target distribution p using samples from some other distribution q.\nIf we use samples from q to approximate p, using importance sampling, the number of samples necessary and sufficient for for an accurate approximation is exponential in the relative entropy:\n\n\\#\\text{samples}_{\\mathrm{IS}(p\\leftarrow q)} \\approx e^{\\KL p q}\n\nFor proof, see Chatterjee and Diaconis (2018), who show that taking e^{\\KL p q} samples from q is a necessary and sufficient condition for the absolute value of the error to be close to zero with high probability.\n\n\n\n\n\n\nConditions for Chatterjee result\n\n\n\nTechnically, this result only obtains when the log density of p wrt q is likely to be concentrated around its expected value.\nThat is, that \\log \\dpdq(Z) (where Z\\sim p) is concentrated around \\E_p{\\log \\dpdq(Z)} = \\KL p q. Or equivalently, that \\dpdq(Z')\\log \\dpdq(Z') (where Z'\\sim q) is concentrated around \\E_q{\\dpdq(Z')\\log \\dpdq(Z')} = \\KL p q. Roughly, this requirement is that the expected variance in importance weights is small.\nMore precisely, their result says that in order to bound the L^1-error of the estimate close to zero with high probability, a sample size of\n\ne^{\\KL p q + \\mathcal{O}(s)} is sufficient\ne^{\\KL p q - \\mathcal{O}(s)} is necessary\n\nwhere s is the typical order of fluctuations in \\log \\dpdq(Z) around its expected value, \\KL p q.\n\n\nSo, for a sampling-based mechanism we can define the update cost as this exponentiatied relative entropy:\n\n\\mathrm{cost} \\coloneqq e^{\\KL p q}\n\nalso note, many empirical studies of human reading time as a function of surprisal log transform the response variable, which in fact implies an exponential relationship like this. this is acknowledged, if only rarely (for example, in Oh, Yue, and Schuler 2024)\n\n\n\n\n\n\nOr… choose your fave divergence:\n\n\n\n\n\nThe number of samples necessary for IS can also be related to other divergences (other than KL). See Agapiou et al. (2017) discuss the \\chi^2-divergence, and also Sanz-Alonso (2018) (who also dicsuss Hellinger and TV).\n\n\\DIV{\\chi^2}\\| p q = \\E_q{((\\dpdq)^2)}-1 = \\E_p{(\\dpdq)}-1\n\\KL p q = \\E_q{(\\dpdq\\log\\dpdq)} =\\E_p{(\\log\\dpdq)}\n\nNote it’s clear by Jensen’s that e^{\\mathrm{KL}}\\le \\mathrm{D}_{\\chi^2} + 1.\nThe result discussed in Agapiou et al. (2017) is:\n\nThey define\n\nunnormalized density g as \\frac1{\\E_q{g}}g(\\cdot)\\coloneqq\\dpdq(\\cdot), and\ndenote with \\rho the second moment of this RN-derivative: \\rho\\coloneqq \\E_q(\\dpdq^2) = \\frac{\\E_q{g^2}}{(\\E_q{g})^2}\n\n\\rho \\ge 1 since (\\E_q{g})^2\\le\\E_q{\\mathbf1^2}\\E_q{g^2}= \\E_q{g^2} by Cauchy-Schwarz.\n\n\nTheir main result is that both bias and MSE of IS are \\approx\\rho/N\nThis gives, for some fixed accuracy, the sufficient sample size in terms of KL and chi-squared divergences as:\n\ngrowing linearly in chi-squared, since \\DIV{\\chi^2}\\| p q = \\E_q{((\\dpdq)^2)}-1 = \\rho-1\nexponentially in KL, since \\KL p q= \\E_q{(\\dpdq\\log\\dpdq)} =\\E_p{(\\log\\dpdq)}\\le\\log\\E_p{(\\dpdq)} = \\log \\E_q{((\\dpdq)^2)}=\\log \\rho, by Jensen’s ineq. so e^{\\KL p q} \\le \\rho\n\n\nThere are many references for the relationships between KL and \\chi^2 and other probability metrics/divergences (see Gibbs and Su 2002; Sanz-Alonso 2018).\n\\implies For us, the point is: we could alternatively say \\mathrm{cost} \\coloneqq \\DIV{\\chi^2}\\| p q, and it would amount to something roughly similar as \\mathrm{cost} \\coloneqq e^{\\KL p q}.\n\n\n\n\nSetup\n\nLet p_{Z,U} be a hypothetical joint distribution for Z a latent random variable, and U an observable random variable. We don’t assume we have any access to this distribution, but we’ll make use of the following derived distributions:\n\nthe marginal p_Z (the prior distribution on Z)\nthe conditional \\pZu, for any fixed outcome \\uu of U (the posterior on Z).\nAssume \\pZu\\ll p_Z, that is \\pZu=0 anywhere p_Z=0.1\n\nLet the proposal \\qZu be some other distribution over Z, which may depend on the outcome \\uu. Again assume \\pZu\\ll\\qZu.\n\n1 A sufficient but not quite necessary condition for IS weights to be well defined. Also a natural property in a Bayesian setting where the posterior is a reweighted version of the prior, so can’t put mass outside the support of the prior.I’m writing a breve on the outcome variable just to denote that it is fixed.\n\n\nDecomposing KL between prior and posterior\nThe relative entropy of prior p_Z with respect to posterior \\pZu can be written as:\n\n\\begin{aligned}\n\\KL{\\pZu}{p_Z}\n    &= \\E_{\\pZu}{ \\log\\frac{\\pzu}{p(z)} }\n     = \\E_{\\pZu}{ \\log\\frac{p(z,\\uu)}{p(z)p(\\uu)} }\\\\\n    &= \\E_{\\pZu}{ \\log\\frac{p(\\uu\\mid z)}{p(\\uu)} }\\\\\n    &= \\log \\frac1{p(\\uu)}\n     + \\E_{\\pZu}{\\log p(\\uu\\mid z)} \\\\\n    &= \\underbrace{-\\log p(\\uu)}_{\\surp{\\uu}}\n     - \\underbrace{\\E_{\\pZu}{-\\log p(\\uu\\mid z)}}_{\\coloneqq\\ \\R}\n\\end{aligned}\n\nSo the relative entropy between prior and posterior consists of\n\nthe surprisal, \\surp{\\uu}\\ge0,\nminus a term which we denote \\R, which I’ll call ‘reconstruction information’. It is the posterior-expected conditional surprisal… the number of bits by which surprisal of the observation exceeds the size of the belief update it causes. We can think of \\R measuring how many bits of surprisal are irrelevant to belief-updating.\n\nNote that 0\\le\\R\\le \\surp{\\uu}\n\n\n\n\nWe could define a more general \\Rs{q} generally as\n\nDefinition 1 \n\\Rs{q} \\coloneqq \\E_{z\\sim q}{\\log \\frac1{p(\\uu|z,\\breve c)}}\n\\tag{1}\nwhere q is any distribution over the latent variable Z (such as a proposal, or variational approximation to the posterior), and \\uu is a fixed observed outcome of U. Both the probability and the proposal may optionally depend on context \\breve c.\n\n\n\\mathrm{R}_{\\qZu}(\\uu) (for \\qZu an approximation to the posterior \\pZu) is sometimes called the (negative) reconstruction error in variational inference literature (Liang et al. 2018; Dehaene et al. 2019) or original autoencoders literature (such as Vincent et al. 2010).\nin a bounded-rational decision-making framework (as in Ortega and Braun 2013; Genewein et al. 2015), the negative of this quantity is the expected utility.\n\nNote \\R=0 is a necessary and sufficient condition for the relative entropy between prior and posterior equalling surprisal:\n\n\\KL{\\pZu}{p_Z}=-\\log p(\\uu) \\quad\\iff\\quad \\R=0\n\nU being a deterministic function of Z is a sufficient condition for this to hold (this is the assumption made in the proofs of the equivalence of KL and surprisal, such as in Levy 2008).\n\n\nDecomposing KL between proposal and posterior\nIf instead of sampling from the prior, we were sampling from some proposal distribution proposal \\qZu, then the we can break down that divergence with respect to posterior \\pZu, to get an additional term:\n\n\\begin{aligned}\n\\KL{\\pZu}{\\qZu}\n    &= \\E_{\\pZu}\\log\\frac{\\pzu}{\\qzu}\\\\\n    &= \\E_{\\pZu}\\log\\frac{\\pzu}{p(z)}\\frac{p(z)}{\\qzu}\\\\\n    &= \\KL{\\pZu}{p_Z}\n        + \\E_{\\pZu}\\log\\frac{p(z)}{\\qzu}\\\\\n    &= {\\surp{\\uu}} - {\\R}\n       - \\underbrace{\\E_{\\pZu}\\log\\frac{\\qzu}{p(z)}}_{\\coloneqq\\ \\Dq}\\\\\n\\end{aligned}\n\nwhere the term \\Dq, quantifies how much better \\qZu is than p_Z for estimating \\pZu. More precisely, as a difference in KLs, it represents the reduction in excess surprise resulting from using \\qZu instead of p_Z, when the actual distribution is \\pZu:\n\n\\Dq = \\KL{\\pZu}{p_Z} - \\KL{\\pZu}{\\qZu}\n\nEquivalently it can be viewed as measuring the reduction in cross-entropy: \n\\Dq = \\E_{\\pZu}\\log\\frac{1}{p(z)}- \\E_{\\pZu}\\log\\frac{1}{\\qzu}=\\H{\\pZu,p_Z}-\\H{\\pZu,\\qZu}\n\nInterpretation:\n\n\\Dq &gt; 0 if \\qZu is better than p_Z for estimating \\pZu and\n\\Dq &lt; 0 if \\qZu is worse than p_Z for estimating \\pZu, and\n\\Dq = 0 if \\qZu = p_Z.\n\nBounds: -\\infty\\le\\Dq\\le\\KL{\\pZu}{p_Z}={\\surp{\\uu}} - {\\R}.\n\n\n\nJust to get a visual picture of the boundaries, here are level surfaces of D_\\mathrm{KL} = S - (R + D)\n\n\n\n\n\n\n\n\nGeneral identity for change in KLs\n\n\n\n\n\nThis could be seen as a result of the general identity\n\n\\KL P Q = \\KL P R - \\E_P\\log\\frac{\\dee Q}{\\dee R}\n\nfor any measures P, Q, R, on the same space with P\\ll R and P\\ll Q.\n\n\n\nThis could alternatively be written as\n\n\\begin{aligned}\n\\KL{\\pZu}{\\qZu}\n    &= \\surp{\\uu}\n    - \\left(\n        \\E_{\\pZu}{\\log\\frac1{p(\\uu\\mid z)}}\n        + \\E_{\\pZu}{\\log\\frac{\\qzu}{p(z)}}\n    \\right)\\\\\n    &= \\surp{\\uu}\n    + \\underbrace{\\E_{\\pZu}{\\log\\frac{p(z,\\uu)}{\\qzu}}}_{-\\R-\\Dq}\n\\end{aligned}\n\n\n\n\n\n\n\nComparison with VAE and ‘Bayesian Surprise’\n\n\n\n\n\nThe expectation here looks a lot like \\operatorname{ELBO}_\\qZu(\\uu)\\coloneqq\\E_{\\qZu}\\left[\\log\\frac{p(z, \\uu)}{\\qzu}\\right] in variational inference (aka negative variational free energy), but they differ in what the expectation is taken with respect to.\nFor comparison, using the same notation, the usual derivation in VAE looks like this. See, e.g., (Kingma 2017; though this math goes back before VAEs at least to Dayan et al. 1995, eq 2.5):\n\n\\begin{aligned}\n\\log p(\\uu)\n    &= \\KL{\\qZu}{\\pZu}\n    + \\overbrace{\\E_{\\qZu}{\\log\\frac{p(z, \\uu)}{\\qzu}}}^{\\operatorname{ELBO}_\\qZu(\\uu)}\\\\\n\\end{aligned}\n\nSo, we get a very similar equation to the above for a KL in the opposite direction:\n\n\\begin{aligned}\n\\KL{\\qZu}{\\pZu}\n    &= -\\surp{\\uu}\n    - \\E_{\\qZu}{\\log\\frac{p(z, \\uu)}{\\qzu}}\n\\end{aligned}\n\nor\n\n\\begin{aligned}\n\\KL{\\qZu}{\\pZu}\n    &= - \\surp{\\uu}  \n    - \\E_{\\qZu}{\\log\\frac{p(\\uu\\mid z)p(z)}{\\qzu}}\\\\\n    &= -\\surp{\\uu}\n    - \\E_{\\qZu}{\\log p(\\uu\\mid z)}\n    + \\KL{\\qZu}{p_Z}\\\\\n    &= - \\surp{\\uu}\n    + \\underbrace{\\E_{\\qZu}{-\\log p(\\uu\\mid z)}}_{\\text{neg. reconstr. error }\\operatorname{R}_{\\qZu}(\\uu)}\n    + \\underbrace{\\KL{\\qZu}{p_Z}}_{\\text{regularizer}}\\\\\n\\end{aligned}\n\nIn this setup, \\qZu is chosen in order to maximize the ELBO. The ELBO consists of two components, the reconstruction error (which is a negative-log-likelihood term, to be maximized) minus the KL between \\qZu and the prior (which can be seen as a regularization term, to be minimized).\nThese equations look very similar, but it is very different from the case we are interested in, where expectations are taken with respect to the true unknown posterior.\n\nWhile this direction of KL (with expectation over \\qZu) may be the “backward” direction from the point of view of the connection with sampling, it might be important to understand whether/how it relates to processing effort in some way, since when \\qZu=p_Z, this is precisely the divergence used as “Bayesian Surprise” (Baldi 2002; Baldi and Itti 2010) (see lit review). Might be that this was chosen purely for computational convenience, but even so, worth understanding what it implies.\nDepending on which direction of KL we choose to use we have two ways of expressing the surprisal:\n\n\\begin{aligned}\n\\surp{\\uu}\n    &= \\overbrace{\\E_{\\qZu}{\\log\\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{-\\operatorname{ELBO}_\\qZu(\\uu)}\n    - \\KL{\\qZu}{\\pZu}\\\\\n\\surp{\\uu}\n    &= \\underbrace{\\E_{\\pZu}{\\log \\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}_{\\R+\\Dq}\n    + \\KL{\\pZu}{\\qZu}\n\\end{aligned}\n\nor, put another way, with R, D, and the ELBO we can express the sum of the KL and reverse-KL \\DIV{J},pq\\coloneqq \\KL pq  + \\KL qp = \\E_q{(\\dpdq-1)\\log\\dpdq} = \\DIV{\\lambda t.(t-1)\\log t}\\|pq (this symmetric f-divergence is actually the one originally proposed by K&L, and earlier defined by Jeffreys):\n\n0\\le\n{\\DIV{J},{\\pZu}{\\qZu}} =\n\\overbrace{\\E_{\\qZu}{\\log\\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{-\\operatorname{ELBO}_\\qZu(\\uu)}\n- \\overbrace{\\E_{\\pZu}{\\log \\frac{\\qzu}{p(\\uu\\mid z)p(z)}}}^{\\R+\\Dq}\n\nThis is a bit pointless in the abstract, since,\\Dq can be positive or negative, hence no bounds are implied. Yet, when we just consider the case where \\qZu=p_Z, then D = 0, and we have\n\n0\\le\n\\DIV{J},{\\pZu}{p_Z} =\n\\overbrace{\\E_{p_Z}{\\log\\frac{1}{p(\\uu\\mid z)}}}^{-\\operatorname{ELBO}_{p_Z}(\\uu)=\\operatorname{R}_{p_Z}(\\uu)}\n- \\overbrace{\\E_{\\pZu}{\\log \\frac{1}{p(\\uu\\mid z)}}}^{\\R}\n\nwhere all the terms are nonnegative, so then we can say the magnitude of the ELBO (or, free energy, I guess) is in fact a upper bound on the magnitude of surprisal, which is an upper bound on R.\n\n0\\le\\R\\le\\surp{\\uu}\\le-\\operatorname{ELBO}_{p_Z}(\\uu)=\\operatorname{R}_{p_Z}(\\uu)\n\nIs it useful to say that surprisal is bounded between R below and prior-reconstruction error/free energy above?\n\n\n\n\n\nIncremental version\nAbove we’re assuming all the probabilities depend on a (notationally suppressed) ‘context’ random var. Now let’s write out the same derivation but with the observation being explicitly the ith item in a sequence \\uu_1, \\uu_2, \\ldots. So the ‘context’ is \\mathbf\\uu_{&lt;i}, and \\mathbf\\uu_{\\le i} the context with the current observation.\n\nprior p_Z above becomes \\priorZ\nposterior \\pZu above becomes \\posteriorZ\nproposal \\qZu above becomes \\proposalZ\n\nDecomposing the KL into two pieces (Leaving R+D as a single term), we can write the KL as:2\n2 where the first step is since\n\n\\begin{aligned}\n&\\log \\frac{\\posteriorZ(z)}{\\priorZ(z)}\\\\\n&= \\log \\frac{p({z\\mid \\mathbf\\uu_{&lt; i},\\uu_i})}{p({z\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\log \\frac{p({z,\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}{p({z\\mid \\mathbf\\uu_{&lt; i}})p({\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\log \\frac{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}{p({\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}\\\\\n&= \\surp{\\uu_i}+\\log{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\n\\end{aligned}\n\n\nand the second is since\n\n\\begin{aligned}\n&=  \\log\\frac1{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\\frac{\\proposalZ}{\\priorZ}\\\\\n&= \\log\\frac{q({z; \\mathbf\\uu_{\\le i}})}{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})p({z\\mid \\mathbf\\uu_{&lt; i}})}\n\\end{aligned}\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n&= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\proposalZ}}\n= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\priorZ}\\frac{\\priorZ}{\\proposalZ}}\\\\\n&= \\surp{\\uu_i}-\\E_{\\posteriorZ}{\\log\\frac1{p({\\uu_i\\mid z, \\mathbf\\uu_{&lt; i}})}\\frac{\\priorZ}{\\proposalZ}}\\\\\n&= \\surp{\\uu_i}-\\E_{\\posteriorZ}{\\log\\frac{q({z; \\mathbf\\uu_{\\le i}})}{p({z,\\uu_i\\mid \\mathbf\\uu_{&lt; i}})}}\n\\end{aligned}\n\nOr, breaking the KL into three pieces\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n    &= \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\proposalZ}}\n    = \\E_{\\posteriorZ}{\\log\\frac{\\posteriorZ}{\\priorZ}\\frac{\\priorZ}{\\proposalZ}}\\\\\n    &= \\KL{\\posteriorZ}{\\priorZ}\n        - \\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}\\\\\n    &= \\underbrace{\\log \\frac1{p(\\uu_i\\mid\\mathbf\\uu_{&lt; i})}}_{\\surp{\\uu}}\n        - \\underbrace{\\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}}_{\\Ri}\n        - \\underbrace{\\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}}_{\\Dqi}\n\\end{aligned}\n\nWe can also write out the surprisal term as the joint marginalized over the prior meanings:\n\n\\begin{aligned}\n\\KL{\\posteriorZ}{\\proposalZ}\n    &= \\underbrace{\n        \\log \\frac1{\\E_{\\priorZ} {p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}}\n    }_{\\surp{\\uu}}\n        - \\underbrace{\n            \\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}\n        }_{\\Ri}\n        - \\underbrace{\n            \\E_{\\posteriorZ}{\\log\\frac{\\proposalZ}{\\priorZ}}\n        }_{\\Dqi}\n\\end{aligned}\n\nLet’s look at the first two terms above (the q=prior situation), and let’s break down the posterior into prior and likelihood (Bayes), recalling that the negative log marginal likelihood is the surprisal:\n\n\\posteriorz =\n\\frac{\\priorz p(\\uu_i\\mid z,\\mathbf\\uu_{&lt; i})}\n{p(\\uu_i\\mid\\mathbf\\uu_{&lt;i})}\n= e^{\\surp{\\uu_i}}\n      \\priorz p(\\uu_i\\mid z,\\mathbf\\uu_{&lt; i})\n\nSo,\n\n\\begin{align}\n\\KL{\\posteriorZ}{\\priorZ}\n&= \\overbrace{\n\\log \\frac1{p(\\uu_i\\mid \\mathbf\\uu_{&lt;i})}\n}^{\\surp{\\uu_i}}\n- \\overbrace{\n\\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}}\n}^{\\Ri}\n\\\\\n&=  \\log \\frac1{\\E_{\\priorZ} {p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}}\\\\\n&\\quad - e^{\\surp{\\uu_i}}\\E_{\\priorZ}p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})\\log\\frac1{p(\\uu_i\\mid {\\cPOST z},\\mathbf\\uu_{&lt; i})}\n\\end{align}\n\nThis is complicated looking, but one thing it means is if the prior is a one-hot/Dirac delta entirely concentrated on some value {\\cPRIO z'}, then the KL is zero.3\n3 Note in this degenerate case surprisal = negative log likelihood of \\cPRIO z'.\n\\KL{\\posteriorZ}{{\\cPRIO \\delta_{z'}}} =\n\\log \\frac1{ p(\\uu_i\\mid z',\\mathbf\\uu_{&lt; i})}\n- \\log\\frac1{p(\\uu_i\\mid z',\\mathbf\\uu_{&lt; i})} = 0\n\n\nWhat does it mean?\n\\KL{\\posteriorZ}{\\proposalZ} is the magnitude of the belief update from the proposal \\proposalZ to the posterior \\posteriorZ. Importance sampling is ideally exponential in this quantity (if such an algorithm exists). This quantity can be broken down into three pieces as KL = S – R – D.\n\nS (nonnegative) is the surprisal of \\uu_i, how unexpected it is (under the true prior)\nR (nonnegative) measures the nondeterminism in converting from meanings in the posterior to \\uu_i. If, under the posterior, \\uu_i is a certainty, then R is zero.\nD (positive or negative) measures how helpful it is to use the proposal \\proposalZ rather than the prior \\priorZ for this particular \\uu_i and context. Positive if helpful, zero if not at all helpful, negative if misleading.\n\n\n\nRelationship to lossy-context surprisal\nBy Jensen’s inequality (note -\\log(\\cdot) is concave up):\n\n\\surp{\\uu}\n%= -\\log p(\\uu_i\\mid\\mathbf\\uu_{&lt; i})\n= -\\log \\E_{\\priorZ}{p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}\n\\le \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z},\\mathbf\\uu_{&lt; i})}\\\\\n%= \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z})}\\quad\\text{if }U_i\\indep U_{\\le i} \\mid {\\cPRIO Z}\\\\\n\nIf we interpret the latent variable \\cPRIO z \\sim \\priorZ as a lossy/noised version of the context, and make an independence assumption, U_i\\indep U_{\\le i} \\mid {\\cPRIO Z}, then surprisal is upperbounded by lossy-context surprisal \\E_{\\priorZ}{ -\\log p(\\uu_i\\mid {\\cPRIO z})}, as defined in Futrell, Gibson, and Levy (2020).\n\n\n\n\n\n\nPossible independence assumptions\n\n\n\n\n\n\nFor the S term: we might want to add an assumption that U_i\\indep U_{\\le i} \\mid Z_{i-1} (motivated by a model wherein \\cPOST z \\sim \\posteriorZ contains all the useful information from \\uu_{\\le i})\nThis assumption would yield that p(\\uu_i\\mid\\mathbf\\uu_{&lt; i}) = \\E_{\\priorZ} p(\\uu_i\\mid {\\cPRIO z}), so:\n\n\\surp{\\uu_i} = \\log \\frac1{\\E_{\\priorZ}{p(\\uu_i\\mid {\\cPRIO z})}}\n\nFor the R term: If we assume that U_i\\indep U_{\\le i} \\mid Z_i (this might be somewhat less obviously reasonable to assume), then we get something similar for R:\n\nR_{\\posteriorZ}(\\uu) = \\E_{\\posteriorZ}{\\log\\frac1{p(\\uu_i\\mid {\\cPOST z})}}\n\n\n\n\n\n\n\n\nKL theory vs (LC-)surprisal theory\nWe have the decomposition\n\n\\KL\\posteriorZ\\proposalZ\n= \\surp{\\uu} - \\left(\\Ri + \\Dqi\\right)\n\n\nSurprisal theory models difficulty as \\approx\\mathrm{S}. Generalizing, we can describe surprisal theory as difficulty \\approx f(\\mathrm{S}), for monotonic increasing f (not necessarily linear).\nWe would like to propose KL-theory which models difficulty as \\approx f(\\mathrm{D_{KL}}) instead (motivated by algorithmic complexity of sampling being \\approx e^{\\mathrm{D_{KL}}}).\n\nQ: When do these make different predictions?\nA: When \\surp{\\uu} is high, but [\\Ri+\\Dqi] is similarly high. Then surprisal theory predicts \\uu is difficult, and KL theory predicts it is not.\nThe possible cases leading to high surprisal but low KL:\n\nAssume D is negligible. We get low KL when \\uu remains unpredictable on average even when given the latent z_i (which encodes information about y_i). Intuitively: when the latent variable forgets/misrepresents the identity of y_i. For example, a perceived production error/typo?\nAssume R is negligible. We get low KL when \\proposalZ is much better than \\priorZ. That is, when your smart proposal gives a large reduction in excess surprise.\nWhen both of the above happen simultaneously."
  },
  {
    "objectID": "notebooks/difficulty-KL-summary_PUBLIC.html#notes",
    "href": "notebooks/difficulty-KL-summary_PUBLIC.html#notes",
    "title": "Bayesian incremental parsing",
    "section": "Notes",
    "text": "Notes\n\nThe number of samples from p_Z for an IS estimate of \\pZu is e^{\\KL{\\pZu}{p_Z}}.\nThe number of samples from \\qZu for an IS estimate of \\pZu is e^{\\KL{\\pZu}{\\qZu}}.\n\nIf the observable U is assumed to be a deterministic function of the latent Z (as in Levy (2008), where latent state consists partially of the observable string), then \\R=0, and thus \\KL{\\pZu}{p_Z}=-\\log p(\\uu). Thus, IS (which, with a binary likelihood function becomes simply rejection sampling, see Chen 2005), will require e^{-\\log p(\\uu)}=\\frac1{p(\\uu)} samples.\nThis gives us a clear prediction for runtime being an exponential function of surprisal.\nHowever, there are two issues with this:\n\nIf we want to model the latent states as not containing the observations within them, then U will not be a deterministic function of Z in general (and so \\R may be nonzero?), and thus the runtime will depend on \\R as well as suprisal.\nIndependent of the previous point, if we want to sample from something smarter than simply the prior, then the number of samples needed will be e^{\\KL{\\pZu}{\\qZu}}, and so the runtime will depend on \\Dq (and possibly also \\R)\n\n\nRequirements of a parametric relationship\nWhat properties of \\qZu or \\R would have to hold to have runtime be a particular parametric relationship with surprisal (such as a linear one)?\nIf we don’t assume anything particular about \\qZu, then, the sampling-based update cost is\n\n\\begin{aligned}\n\\operatorname{cost}(\\uu)\n    &= e^{\\KL\\pZu\\qZu}\\\\\n    &= e^{\\surp{\\uu} - \\R - \\Dq}\n\\end{aligned}\n\nIf \\operatorname{cost}(\\uu) = f(\\surp{\\uu}) for some linking function f, then\n\n\\begin{aligned}\nf(\\surp{\\uu})\n    &= e^{\\surp{\\uu} - \\R - \\Dq}\\\\\n\\log f(\\surp{\\uu})\n    &= \\surp{\\uu} - \\R - \\Dq\\\\\n\\end{aligned}\n\nso\n\n\\begin{aligned}\n\\R + \\Dq\n    &= \\surp{\\uu} - \\log f(\\surp{\\uu}) \\\\\n    % &= - \\log\\left(p(\\uu) f( \\surp{\\uu})\\right) \\\\\ne^{\\R + \\Dq}\n    &= \\frac1{p(\\uu) f(\\surp{\\uu})}\n\\end{aligned}\n\nIf we assume we’re sampling from the prior (so \\qZu=p_Z), then \\Dq=0, so\n\n\\R = \\surp{\\uu} - \\log f(\\surp{\\uu})\n\n\n\nParametric relationships. \nThus, if we measure cost as reading time (RT), and fit RT as a function of surprisal, \\operatorname{cost}(\\uu) = \\operatorname{RT}(\\uu) = f_\\mathrm{GAM}(\\surp{\\uu}), we can inspect the implies about the term G\\coloneqq \\R + \\Dq, by simply subtracting the log fit RT value from the surprisal.\n\nG(s) = s - \\log f_\\mathrm{GAM}(s)\n\n\n\nOptimal code-length\nWe’re interested in the question: How many bits of information are required to transform prior p_Z into posterior \\pZu?\nJust using bayes rule, we can break this down into three parts\n\n-\\log \\pzu\n= -\\log p(z) - \\log \\frac{p(\\uu\\mid z)}{p(\\uu)}\n= \\log p(\\uu) -\\log p(z) - \\log p(\\uu\\mid z)\n\nTBC"
  }
]