<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jacob Louis Hoover">

<title>📓 – KL and runtime</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": true,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <!-- <a class="navbar-brand" href="../index.html"> -->
    <span class="navbar-title">📓</span>
    <!-- </a> -->
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
          <!-- <div id="quarto-search" class="" title="Search"></div> -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#difficulty-as-kl" id="toc-difficulty-as-kl" class="nav-link active" data-scroll-target="#difficulty-as-kl">Difficulty as KL</a>
  <ul class="collapse">
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#decomposing-kl-between-prior-and-posterior" id="toc-decomposing-kl-between-prior-and-posterior" class="nav-link" data-scroll-target="#decomposing-kl-between-prior-and-posterior">Decomposing KL between prior and posterior</a></li>
  <li><a href="#decomposing-kl-between-proposal-and-posterior" id="toc-decomposing-kl-between-proposal-and-posterior" class="nav-link" data-scroll-target="#decomposing-kl-between-proposal-and-posterior">Decomposing KL between <em>proposal</em> and posterior</a></li>
  <li><a href="#incremental-version" id="toc-incremental-version" class="nav-link" data-scroll-target="#incremental-version">Incremental version</a></li>
  <li><a href="#kl-theory-vs-lc-surprisal-theory" id="toc-kl-theory-vs-lc-surprisal-theory" class="nav-link" data-scroll-target="#kl-theory-vs-lc-surprisal-theory">KL theory vs (LC-)surprisal theory</a></li>
  </ul></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes</a>
  <ul class="collapse">
  <li><a href="#requirements-of-a-parametric-relationship" id="toc-requirements-of-a-parametric-relationship" class="nav-link" data-scroll-target="#requirements-of-a-parametric-relationship">Requirements of a parametric relationship</a></li>
  </ul></li>
  <li><a href="#ordered-search-runtime" id="toc-ordered-search-runtime" class="nav-link" data-scroll-target="#ordered-search-runtime">Ordered search runtime</a>
  <ul class="collapse">
  <li><a href="#probability-ordered-search-pareto-weights" id="toc-probability-ordered-search-pareto-weights" class="nav-link" data-scroll-target="#probability-ordered-search-pareto-weights">Probability-ordered search (Pareto weights)</a></li>
  <li><a href="#probability-ordered-search-pareto-odds" id="toc-probability-ordered-search-pareto-odds" class="nav-link" data-scroll-target="#probability-ordered-search-pareto-odds">Probability-ordered search (Pareto odds)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">
<div class="hidden">
<p><span class="math display">
\global\def\I#1{\operatorname{I}(#1)}
\global\def\H#1{\operatorname{H}(#1)}
\global\def\surp#1{\operatorname{surp}(#1)}
\global\def\DIV#1#2#3#4{\operatorname{D_{#1}}(#3#2#4)}
\global\def\KL#1#2{\DIV{KL}\|{#1}{#2}}
\global\def\E{\operatorname*{\mathbb{E}}}
\global\def\dee{\mathop{\mathrm{d}\!}}
\global\def\var#1{\operatorname{\mathbb{V}}(#1)}
\global\def\Var#1#2{\operatorname{\mathbb{V}}\!\!{}_{#1}(#2)}
\global\def\indep{\bot\!\!\!\bot}
\global\def\uu{\breve u}
</span></p>
</div>
<div class="hidden">
<p><span class="math display">
\global\def\fade#1{{\color{gray}{#1}}}
\global\def\uu{\breve u}
\global\def\R{\operatorname{R}_{\pZu}(\uu)}
\global\def\Rs#1{\operatorname{R}_{#1}(\uu)}
\global\def\cPOST{\color{0A00A0}}
\global\def\cPRIO{\color{008008}}
\global\def\cPROP{\color{A00A00}}
\global\def\pZu{{\cPOST p_{Z\mid\uu}}}
\global\def\pzu{{\cPOST p(z\mid \uu)}}
\global\def\qZu{{\cPROP q_{Z; \uu}}}
\global\def\qzu{{\cPROP q(z; \uu)}}
\global\def\Dq{D_{\pZu}^{\qZu \leftarrow  p_Z}(\uu)}
\global\def\surp#1{\operatorname{surp}(#1)}
\global\def\priorZ{{\cPRIO p_{Z\mid \mathbf\uu_{&lt; i}}}}
\global\def\posteriorZ{{\cPOST p_{Z\mid \mathbf\uu_{\le i}}}}
\global\def\proposalZ{{\cPROP q_{Z; \mathbf\uu_{\le i}}}}
\global\def\priorz{{\cPRIO p(z\mid \mathbf\uu_{&lt; i})}}
\global\def\posteriorz{{\cPOST p(z\mid \mathbf\uu_{\le i})}}
\global\def\proposalz{{\cPROP q(z; \mathbf\uu_{\le i})}}
\global\def\Dqi{D_{ {\cPOST p} }^{{\cPROP q} \leftarrow  {\cPRIO p} }(\uu)}
\global\def\Ri{\Rs{{\cPOST p}}}
%
\global\def\posteriorU{{{p_\mathrm{LM}}(\mathbf U_{&gt;i}\mid \mathbf\uu_{\le i})}}
\global\def\proposalU{{{q_\mathrm{LM}}(\mathbf U_{&gt;i}\mid \mathbf\uu_{\le i})}}
\global\def\posterioru{{{p_\mathrm{LM}}(\mathbf u_{&gt;i}\mid \mathbf\uu_{\le i})}}
\global\def\proposalu{{{q_\mathrm{LM}}(\mathbf u_{&gt;i}\mid \mathbf\uu_{\le i})}}
\global\def\prioru{p_\mathrm{LM}(\mathbf u_{&gt;i}\mid \mathbf\uu_{&lt; i})}
%
\global\def\posteriorN{{{p_\mathrm{LM}}(\mathbf U_{i+1}\mid \mathbf\uu_{\le i})}}
\global\def\posteriorn{{{p_\mathrm{LM}}(u_{i+1}\mid \mathbf\uu_{\le i})}}
\global\def\dpdq{\frac{\dee p}{\dee q}}
</span></p>
</div>

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">KL and runtime</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jacob Louis Hoover </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="difficulty-as-kl" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="difficulty-as-kl">Difficulty as KL</h2>
<p>Say we want to approximate some target distribution <span class="math inline">p</span> using samples from some other distribution <span class="math inline">q</span>.</p>
<p>If we use samples from <span class="math inline">q</span> to approximate <span class="math inline">p</span>, using importance sampling, the number of samples necessary and sufficient for for an accurate approximation is exponential in the relative entropy:</p>
<p><span class="math display">
\#\text{samples}_{\mathrm{IS}(p\leftarrow q)} \approx e^{\KL p q}
</span></p>
<p>For proof, see <span class="citation" data-cites="chatterjee.s:2018">Chatterjee and Diaconis (<a href="#ref-chatterjee.s:2018" role="doc-biblioref">2018</a>)</span>, who show that taking <span class="math inline">e^{\KL p q}</span> samples from <span class="math inline">q</span> is a necessary and sufficient condition for the absolute value of the error to be close to zero with high probability.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Conditions for Chatterjee result
</div>
</div>
<div class="callout-body-container callout-body">
<p>Technically, this result only obtains when the log density of <span class="math inline">p</span> wrt <span class="math inline">q</span> is likely to be concentrated around its expected value.</p>
<p>That is, that <span class="math inline">\log \dpdq(Z)</span> (where <span class="math inline">Z\sim p</span>) is concentrated around <span class="math inline">\E_p{\log \dpdq(Z)} = \KL p q</span>. Or equivalently, that <span class="math inline">\dpdq(Z')\log \dpdq(Z')</span> (where <span class="math inline">Z'\sim q</span>) is concentrated around <span class="math inline">\E_q{\dpdq(Z')\log \dpdq(Z')} = \KL p q</span>. Roughly, this requirement is that the expected variance in importance weights is small.</p>
<p>More precisely, their result says that in order to bound the <span class="math inline">L^1</span>-error of the estimate close to zero with high probability, a sample size of</p>
<ul>
<li><span class="math inline">e^{\KL p q + \mathcal{O}(s)}</span> is sufficient</li>
<li><span class="math inline">e^{\KL p q - \mathcal{O}(s)}</span> is necessary</li>
</ul>
<p>where <span class="math inline">s</span> is the typical order of fluctuations in <span class="math inline">\log \dpdq(Z)</span> around its expected value, <span class="math inline">\KL p q</span>.</p>
</div>
</div>
<p>So, for a sampling-based mechanism we can define the update cost as this exponentiatied relative entropy:</p>
<p><span class="math display">
\mathrm{cost} \coloneqq e^{\KL p q}
</span></p>
<p>also note, many empirical studies of human reading time as a function of surprisal log transform the response variable, which in fact implies an exponential relationship like this. this is acknowledged, if only rarely <span class="citation" data-cites="oh.b:2024arxiv">(for example, in <a href="#ref-oh.b:2024arxiv" role="doc-biblioref">Oh, Yue, and Schuler 2024</a>)</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Or… choose your fave divergence:
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<div class="callout-body-container callout-body">
<p>The number of samples necessary for IS can also be related to other divergences (other than KL). See <span class="citation" data-cites="agapiou.s:2017">Agapiou et al. (<a href="#ref-agapiou.s:2017" role="doc-biblioref">2017</a>)</span> discuss the <span class="math inline">\chi^2</span>-divergence, and also <span class="citation" data-cites="sanz-alonso.d:2018">Sanz-Alonso (<a href="#ref-sanz-alonso.d:2018" role="doc-biblioref">2018</a>)</span> (who also dicsuss Hellinger and TV).</p>
<ul>
<li><span class="math inline">\DIV{\chi^2}\| p q = \E_q{((\dpdq)^2)}-1 = \E_p{(\dpdq)}-1</span></li>
<li><span class="math inline">\KL p q = \E_q{(\dpdq\log\dpdq)} =\E_p{(\log\dpdq)}</span></li>
</ul>
<p>Note it’s clear by Jensen’s that <span class="math inline">e^{\mathrm{KL}}\le \mathrm{D}_{\chi^2} + 1</span>.</p>
<p>The result discussed in <span class="citation" data-cites="agapiou.s:2017">Agapiou et al. (<a href="#ref-agapiou.s:2017" role="doc-biblioref">2017</a>)</span> is:</p>
<ul>
<li>They define
<ul>
<li>unnormalized density <span class="math inline">g</span> as <span class="math inline">\frac1{\E_q{g}}g(\cdot)\coloneqq\dpdq(\cdot)</span>, and</li>
<li>denote with <span class="math inline">\rho</span> the second moment of this RN-derivative: <span class="math inline">\rho\coloneqq \E_q(\dpdq^2) = \frac{\E_q{g^2}}{(\E_q{g})^2}</span>
<ul>
<li><span class="math inline">\rho \ge 1</span> since <span class="math inline">(\E_q{g})^2\le\E_q{\mathbf1^2}\E_q{g^2}= \E_q{g^2}</span> by Cauchy-Schwarz.</li>
</ul></li>
</ul></li>
<li>Their main result is that <strong>both bias and MSE of IS are</strong> <span class="math inline">\approx\rho/N</span></li>
<li>This gives, for some fixed accuracy, the sufficient sample size in terms of KL and chi-squared divergences as:
<ul>
<li>growing linearly in chi-squared, since <span class="math inline">\DIV{\chi^2}\| p q = \E_q{((\dpdq)^2)}-1 = \rho-1</span></li>
<li>exponentially in KL, since <span class="math inline">\KL p q= \E_q{(\dpdq\log\dpdq)} =\E_p{(\log\dpdq)}\le\log\E_p{(\dpdq)} = \log \E_q{((\dpdq)^2)}=\log \rho</span>, by Jensen’s ineq. so <span class="math inline">e^{\KL p q} \le \rho</span></li>
</ul></li>
</ul>
<p>There are many references for the relationships between KL and <span class="math inline">\chi^2</span> and other probability metrics/divergences <span class="citation" data-cites="gibbs.a:2002 sanz-alonso.d:2018">(see <a href="#ref-gibbs.a:2002" role="doc-biblioref">Gibbs and Su 2002</a>; <a href="#ref-sanz-alonso.d:2018" role="doc-biblioref">Sanz-Alonso 2018</a>)</span>.</p>
<p><span class="math inline">\implies</span> For us, the point is: we could alternatively say <span class="math inline">\mathrm{cost} \coloneqq \DIV{\chi^2}\| p q</span>, and it would amount to something roughly similar as <span class="math inline">\mathrm{cost} \coloneqq e^{\KL p q}</span>.</p>
</div>
</div>
</div>
<section id="setup" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="setup">Setup</h3>
<ul>
<li><p>Let <span class="math inline">p_{Z,U}</span> be a hypothetical joint distribution for <span class="math inline">Z</span> a latent random variable, and <span class="math inline">U</span> an observable random variable. We don’t assume we have any access to this distribution, but we’ll make use of the following derived distributions:</p>
<ul>
<li>the marginal <span class="math inline">p_Z</span> (the <strong>prior</strong> distribution on <span class="math inline">Z</span>)</li>
<li>the conditional <span class="math inline">\pZu</span>, for any fixed outcome <span class="math inline">\uu</span> of <span class="math inline">U</span> (the <strong>posterior</strong> on <span class="math inline">Z</span>).<br>
Assume <span class="math inline">\pZu\ll p_Z</span>, that is <span class="math inline">\pZu=0</span> anywhere <span class="math inline">p_Z=0</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
</ul></li>
<li><p>Let <strong>the proposal</strong> <span class="math inline">\qZu</span> be some other distribution over <span class="math inline">Z</span>, which may depend on the outcome <span class="math inline">\uu</span>. Again assume <span class="math inline">\pZu\ll\qZu</span>.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;A sufficient but not quite necessary condition for IS weights to be well defined. Also a natural property in a Bayesian setting where the posterior is a reweighted version of the prior, so can’t put mass outside the support of the prior.</p></div></div><p>I’m writing a breve on the outcome variable just to denote that it is fixed.</p>
</section>
<section id="decomposing-kl-between-prior-and-posterior" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="decomposing-kl-between-prior-and-posterior">Decomposing KL between prior and posterior</h3>
<p>The relative entropy of prior <span class="math inline">p_Z</span> with respect to posterior <span class="math inline">\pZu</span> can be written as:</p>
<p><span class="math display">
\begin{aligned}
\KL{\pZu}{p_Z}
    &amp;= \E_{\pZu}{ \log\frac{\pzu}{p(z)} }
     = \E_{\pZu}{ \log\frac{p(z,\uu)}{p(z)p(\uu)} }\\
    &amp;= \E_{\pZu}{ \log\frac{p(\uu\mid z)}{p(\uu)} }\\
    &amp;= \log \frac1{p(\uu)}
     + \E_{\pZu}{\log p(\uu\mid z)} \\
    &amp;= \underbrace{-\log p(\uu)}_{\surp{\uu}}
     - \underbrace{\E_{\pZu}{-\log p(\uu\mid z)}}_{\coloneqq\ \R}
\end{aligned}
</span></p>
<p>So the relative entropy between prior and posterior consists of</p>
<ul>
<li>the <em>surprisal</em>, <span class="math inline">\surp{\uu}\ge0</span>,</li>
<li>minus a term which we denote <span class="math inline">\R</span>, which I’ll call ‘reconstruction information’. It is the posterior-expected conditional surprisal… the number of bits by which surprisal of the observation exceeds the size of the belief update it causes. We can think of <span class="math inline">\R</span> measuring how many bits of surprisal are irrelevant to belief-updating.
<ul>
<li>Note that <span class="math inline">0\le\R\le \surp{\uu}</span></li>
</ul></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>We could define a more general <span class="math inline">\Rs{q}</span> generally as</p>
<div id="def-R" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1</strong></span> <span id="eq-def-R"><span class="math display">
\Rs{q} \coloneqq \E_{z\sim q}{\log \frac1{p(\uu|z,\breve c)}}
\tag{1}</span></span></p>
<p>where <span class="math inline">q</span> is any distribution over the latent variable <span class="math inline">Z</span> (such as a proposal, or variational approximation to the posterior), and <span class="math inline">\uu</span> is a fixed observed outcome of <span class="math inline">U</span>. Both the probability and the proposal may optionally depend on context <span class="math inline">\breve c</span>.</p>
</div>
<ul>
<li><span class="math inline">\mathrm{R}_{\qZu}(\uu)</span> (for <span class="math inline">\qZu</span> an approximation to the posterior <span class="math inline">\pZu</span>) is sometimes called the <em>(negative) reconstruction error</em> in variational inference literature <span class="citation" data-cites="liang.d:2018 dehaene.d:2019">(<a href="#ref-liang.d:2018" role="doc-biblioref">Liang et al. 2018</a>; <a href="#ref-dehaene.d:2019" role="doc-biblioref">Dehaene et al. 2019</a>)</span> or original autoencoders literature <span class="citation" data-cites="vincent.p:2010">(such as <a href="#ref-vincent.p:2010" role="doc-biblioref">Vincent et al. 2010</a>)</span>.</li>
<li>in a bounded-rational decision-making framework <span class="citation" data-cites="ortega.p:2013 genewein.t:2015">(as in <a href="#ref-ortega.p:2013" role="doc-biblioref">Ortega and Braun 2013</a>; <a href="#ref-genewein.t:2015" role="doc-biblioref">Genewein et al. 2015</a>)</span>, the negative of this quantity is the <em>expected utility</em>.</li>
</ul>
</div></div><p>Note <span class="math inline">\R=0</span> is a necessary and sufficient condition for the relative entropy between prior and posterior equalling surprisal:</p>
<p><span class="math display">
\KL{\pZu}{p_Z}=-\log p(\uu) \quad\iff\quad \R=0
</span></p>
<p><span class="math inline">U</span> being a deterministic function of <span class="math inline">Z</span> is a sufficient condition for this to hold <span class="citation" data-cites="levy.r:2008">(this is the assumption made in the proofs of the equivalence of KL and surprisal, such as in <a href="#ref-levy.r:2008" role="doc-biblioref">Levy 2008</a>)</span>.</p>
</section>
<section id="decomposing-kl-between-proposal-and-posterior" class="level3">
<h3 class="anchored" data-anchor-id="decomposing-kl-between-proposal-and-posterior">Decomposing KL between <em>proposal</em> and posterior</h3>
<p>If instead of sampling from the prior, we were sampling from some proposal distribution <em>proposal</em> <span class="math inline">\qZu</span>, then the we can break down that divergence with respect to posterior <span class="math inline">\pZu</span>, to get an additional term:</p>
<p><span class="math display">
\begin{aligned}
\KL{\pZu}{\qZu}
    &amp;= \E_{\pZu}\log\frac{\pzu}{\qzu}\\
    &amp;= \E_{\pZu}\log\frac{\pzu}{p(z)}\frac{p(z)}{\qzu}\\
    &amp;= \KL{\pZu}{p_Z}
        + \E_{\pZu}\log\frac{p(z)}{\qzu}\\
    &amp;= {\surp{\uu}} - {\R}
       - \underbrace{\E_{\pZu}\log\frac{\qzu}{p(z)}}_{\coloneqq\ \Dq}\\
\end{aligned}
</span></p>
<p>where the term <span class="math inline">\Dq</span>, quantifies how much better <span class="math inline">\qZu</span> is than <span class="math inline">p_Z</span> for estimating <span class="math inline">\pZu</span>. More precisely, as a difference in KLs, it represents the <em>reduction</em> in excess surprise resulting from using <span class="math inline">\qZu</span> instead of <span class="math inline">p_Z</span>, when the actual distribution is <span class="math inline">\pZu</span>:</p>
<p><span class="math display">
\Dq = \KL{\pZu}{p_Z} - \KL{\pZu}{\qZu}
</span></p>
<p>Equivalently it can be viewed as measuring the reduction in <em>cross-entropy</em>: <span class="math display">
\Dq = \E_{\pZu}\log\frac{1}{p(z)}- \E_{\pZu}\log\frac{1}{\qzu}=\H{\pZu,p_Z}-\H{\pZu,\qZu}
</span></p>
<p><strong>Interpretation</strong>:</p>
<ul>
<li><span class="math inline">\Dq &gt; 0</span> if <span class="math inline">\qZu</span> is better than <span class="math inline">p_Z</span> for estimating <span class="math inline">\pZu</span> and</li>
<li><span class="math inline">\Dq &lt; 0</span> if <span class="math inline">\qZu</span> is worse than <span class="math inline">p_Z</span> for estimating <span class="math inline">\pZu</span>, and</li>
<li><span class="math inline">\Dq = 0</span> if <span class="math inline">\qZu = p_Z</span>.</li>
</ul>
<p><strong>Bounds</strong>: <span class="math inline">-\infty\le\Dq\le\KL{\pZu}{p_Z}={\surp{\uu}} - {\R}</span>.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="img/KL.png" class="img-fluid figure-img" style="width:40.0%"></p>
<figcaption>Just to get a visual picture of the boundaries, here are level surfaces of <a href="https://www.geogebra.org/3d/asbesfpq"><span class="math inline">D_\mathrm{KL} = S - (R + D)</span></a></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
General identity for change in KLs
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This could be seen as a result of the general identity</p>
<p><span class="math display">
\KL P Q = \KL P R - \E_P\log\frac{\dee Q}{\dee R}
</span></p>
<p>for any measures <span class="math inline">P, Q, R</span>, on the same space with <span class="math inline">P\ll R</span> and <span class="math inline">P\ll Q</span>.</p>
</div>
</div>
</div>
<p>This could alternatively be written as</p>
<p><span class="math display">
\begin{aligned}
\KL{\pZu}{\qZu}
    &amp;= \surp{\uu}
    - \left(
        \E_{\pZu}{\log\frac1{p(\uu\mid z)}}
        + \E_{\pZu}{\log\frac{\qzu}{p(z)}}
    \right)\\
    &amp;= \surp{\uu}
    + \underbrace{\E_{\pZu}{\log\frac{p(z,\uu)}{\qzu}}}_{-\R-\Dq}
\end{aligned}
</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Comparison with VAE and ‘Bayesian Surprise’
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The expectation here looks a lot like <span class="math inline">\operatorname{ELBO}_\qZu(\uu)\coloneqq\E_{\qZu}\left[\log\frac{p(z, \uu)}{\qzu}\right]</span> in variational inference (aka negative variational free energy), but they differ in what the expectation is taken with respect to.</p>
<p>For comparison, using the same notation, the usual derivation in VAE looks like this. See, e.g., <span class="citation" data-cites="kingma.d:2017 dayan.p:1995">(<a href="#ref-kingma.d:2017" role="doc-biblioref">Kingma 2017</a>; though this math goes back before VAEs at least to <a href="#ref-dayan.p:1995" role="doc-biblioref">Dayan et al. 1995</a>, eq 2.5)</span>:</p>
<p><span class="math display">
\begin{aligned}
\log p(\uu)
    &amp;= \KL{\qZu}{\pZu}
    + \overbrace{\E_{\qZu}{\log\frac{p(z, \uu)}{\qzu}}}^{\operatorname{ELBO}_\qZu(\uu)}\\
\end{aligned}
</span></p>
<p>So, we get a very similar equation to the above for a KL in the opposite direction:</p>
<p><span class="math display">
\begin{aligned}
\KL{\qZu}{\pZu}
    &amp;= -\surp{\uu}
    - \E_{\qZu}{\log\frac{p(z, \uu)}{\qzu}}
\end{aligned}
</span></p>
<p>or</p>
<p><span class="math display">
\begin{aligned}
\KL{\qZu}{\pZu}
    &amp;= - \surp{\uu}  
    - \E_{\qZu}{\log\frac{p(\uu\mid z)p(z)}{\qzu}}\\
    &amp;= -\surp{\uu}
    - \E_{\qZu}{\log p(\uu\mid z)}
    + \KL{\qZu}{p_Z}\\
    &amp;= - \surp{\uu}
    + \underbrace{\E_{\qZu}{-\log p(\uu\mid z)}}_{\text{neg. reconstr. error }\operatorname{R}_{\qZu}(\uu)}
    + \underbrace{\KL{\qZu}{p_Z}}_{\text{regularizer}}\\
\end{aligned}
</span></p>
<p>In this setup, <span class="math inline">\qZu</span> is chosen in order to maximize the ELBO. The ELBO consists of two components, the reconstruction error (which is a negative-log-likelihood term, to be maximized) minus the KL between <span class="math inline">\qZu</span> and the prior (which can be seen as a regularization term, to be minimized).</p>
<p>These equations look very similar, but it is very different from the case we are interested in, where expectations are taken with respect to the true unknown posterior.</p>
<hr>
<p>While this direction of KL (with expectation over <span class="math inline">\qZu</span>) may be the “backward” direction from the point of view of the connection with sampling, it might be important to understand whether/how it relates to processing effort in some way, since when <span class="math inline">\qZu=p_Z</span>, this is precisely the divergence used as “Bayesian Surprise” <span class="citation" data-cites="baldi.p:2002 baldi.p:2010">(<a href="#ref-baldi.p:2002" role="doc-biblioref">Baldi 2002</a>; <a href="#ref-baldi.p:2010" role="doc-biblioref">Baldi and Itti 2010</a>)</span> (see lit review). Might be that this was chosen purely for computational convenience, but even so, worth understanding what it implies.</p>
<p>Depending on which direction of KL we choose to use we have two ways of expressing the surprisal:</p>
<p><span class="math display">
\begin{aligned}
\surp{\uu}
    &amp;= \overbrace{\E_{\qZu}{\log\frac{\qzu}{p(\uu\mid z)p(z)}}}^{-\operatorname{ELBO}_\qZu(\uu)}
    - \KL{\qZu}{\pZu}\\
\surp{\uu}
    &amp;= \underbrace{\E_{\pZu}{\log \frac{\qzu}{p(\uu\mid z)p(z)}}}_{\R+\Dq}
    + \KL{\pZu}{\qZu}
\end{aligned}
</span></p>
<p>or, put another way, with R, D, and the ELBO we can express the sum of the KL and reverse-KL <span class="math inline">\DIV{J},pq\coloneqq \KL pq  + \KL qp = \E_q{(\dpdq-1)\log\dpdq} = \DIV{\lambda t.(t-1)\log t}\|pq</span> (this symmetric <span class="math inline">f</span>-divergence is actually the one originally proposed by K&amp;L, and earlier defined by Jeffreys):</p>
<p><span class="math display">
0\le
{\DIV{J},{\pZu}{\qZu}} =
\overbrace{\E_{\qZu}{\log\frac{\qzu}{p(\uu\mid z)p(z)}}}^{-\operatorname{ELBO}_\qZu(\uu)}
- \overbrace{\E_{\pZu}{\log \frac{\qzu}{p(\uu\mid z)p(z)}}}^{\R+\Dq}
</span></p>
<p>This is a bit pointless in the abstract, since,<span class="math inline">\Dq</span> can be positive or negative, hence no bounds are implied. Yet, when we just consider the case where <span class="math inline">\qZu=p_Z</span>, then <span class="math inline">D = 0</span>, and we have</p>
<p><span class="math display">
0\le
\DIV{J},{\pZu}{p_Z} =
\overbrace{\E_{p_Z}{\log\frac{1}{p(\uu\mid z)}}}^{-\operatorname{ELBO}_{p_Z}(\uu)=\operatorname{R}_{p_Z}(\uu)}
- \overbrace{\E_{\pZu}{\log \frac{1}{p(\uu\mid z)}}}^{\R}
</span></p>
<p>where all the terms are nonnegative, so then we can say the magnitude of the ELBO (or, free energy, I guess) is in fact a upper bound on the magnitude of surprisal, which is an upper bound on R.</p>
<p><span class="math display">
0\le\R\le\surp{\uu}\le-\operatorname{ELBO}_{p_Z}(\uu)=\operatorname{R}_{p_Z}(\uu)
</span></p>
<p>Is it useful to say that surprisal is bounded between R below and prior-reconstruction error/free energy above?</p>
</div>
</div>
</div>
</section>
<section id="incremental-version" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="incremental-version">Incremental version</h3>
<p>Above we’re assuming all the probabilities depend on a (notationally suppressed) ‘context’ random var. Now let’s write out the same derivation but with the observation being explicitly the <span class="math inline">i</span>th item in a sequence <span class="math inline">\uu_1, \uu_2, \ldots</span>. So the ‘context’ is <span class="math inline">\mathbf\uu_{&lt;i}</span>, and <span class="math inline">\mathbf\uu_{\le i}</span> the context with the current observation.</p>
<ul>
<li>prior <span class="math inline">p_Z</span> above becomes <span class="math inline">\priorZ</span></li>
<li>posterior <span class="math inline">\pZu</span> above becomes <span class="math inline">\posteriorZ</span></li>
<li>proposal <span class="math inline">\qZu</span> above becomes <span class="math inline">\proposalZ</span></li>
</ul>
<p>Decomposing the KL into two pieces (Leaving R+D as a single term), we can write the KL as:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;where the first step is since</p>
<p><span class="math display">
\begin{aligned}
&amp;\log \frac{\posteriorZ(z)}{\priorZ(z)}\\
&amp;= \log \frac{p({z\mid \mathbf\uu_{&lt; i},\uu_i})}{p({z\mid \mathbf\uu_{&lt; i}})}\\
&amp;= \log \frac{p({z,\uu_i\mid \mathbf\uu_{&lt; i}})}{p({z\mid \mathbf\uu_{&lt; i}})p({\uu_i\mid \mathbf\uu_{&lt; i}})}\\
&amp;= \log \frac{p({\uu_i\mid z, \mathbf\uu_{&lt; i}})}{p({\uu_i\mid \mathbf\uu_{&lt; i}})}\\
&amp;= \surp{\uu_i}+\log{p({\uu_i\mid z, \mathbf\uu_{&lt; i}})}
\end{aligned}
</span></p>
<!-- (by $\frac{P(A|BC)}{P(A| C)}  =\frac{P(AB|C)}{P(B|C)P(A|C)}  =\frac{P(B|AC)}{P(B| C)}$) -->
<p>and the second is since<br>
<span class="math display">
\begin{aligned}
&amp;=  \log\frac1{p({\uu_i\mid z, \mathbf\uu_{&lt; i}})}\frac{\proposalZ}{\priorZ}\\
&amp;= \log\frac{q({z; \mathbf\uu_{\le i}})}{p({\uu_i\mid z, \mathbf\uu_{&lt; i}})p({z\mid \mathbf\uu_{&lt; i}})}
\end{aligned}
</span></p></div></div><p><span class="math display">
\begin{aligned}
\KL{\posteriorZ}{\proposalZ}
&amp;= \E_{\posteriorZ}{\log\frac{\posteriorZ}{\proposalZ}}
= \E_{\posteriorZ}{\log\frac{\posteriorZ}{\priorZ}\frac{\priorZ}{\proposalZ}}\\
&amp;= \surp{\uu_i}-\E_{\posteriorZ}{\log\frac1{p({\uu_i\mid z, \mathbf\uu_{&lt; i}})}\frac{\priorZ}{\proposalZ}}\\
&amp;= \surp{\uu_i}-\E_{\posteriorZ}{\log\frac{q({z; \mathbf\uu_{\le i}})}{p({z,\uu_i\mid \mathbf\uu_{&lt; i}})}}
\end{aligned}
</span></p>
<p>Or, breaking the KL into three pieces</p>
<p><span class="math display">
\begin{aligned}
\KL{\posteriorZ}{\proposalZ}
    &amp;= \E_{\posteriorZ}{\log\frac{\posteriorZ}{\proposalZ}}
    = \E_{\posteriorZ}{\log\frac{\posteriorZ}{\priorZ}\frac{\priorZ}{\proposalZ}}\\
    &amp;= \KL{\posteriorZ}{\priorZ}
        - \E_{\posteriorZ}{\log\frac{\proposalZ}{\priorZ}}\\
    &amp;= \underbrace{\log \frac1{p(\uu_i\mid\mathbf\uu_{&lt; i})}}_{\surp{\uu}}
        - \underbrace{\E_{\posteriorZ}{\log\frac1{p(\uu_i\mid {\cPOST z},\mathbf\uu_{&lt; i})}}}_{\Ri}
        - \underbrace{\E_{\posteriorZ}{\log\frac{\proposalZ}{\priorZ}}}_{\Dqi}
\end{aligned}
</span></p>
<p>We can also write out the surprisal term as the joint marginalized over the prior meanings:</p>
<p><span class="math display">
\begin{aligned}
\KL{\posteriorZ}{\proposalZ}
    &amp;= \underbrace{
        \log \frac1{\E_{\priorZ} {p(\uu_i\mid {\cPRIO z},\mathbf\uu_{&lt; i})}}
    }_{\surp{\uu}}
        - \underbrace{
            \E_{\posteriorZ}{\log\frac1{p(\uu_i\mid {\cPOST z},\mathbf\uu_{&lt; i})}}
        }_{\Ri}
        - \underbrace{
            \E_{\posteriorZ}{\log\frac{\proposalZ}{\priorZ}}
        }_{\Dqi}
\end{aligned}
</span></p>
<p>Let’s look at the first two terms above (the q=prior situation), and let’s break down the posterior into prior and likelihood (Bayes), recalling that the negative log marginal likelihood is the surprisal:<br>
<span class="math display">
\posteriorz =
\frac{\priorz p(\uu_i\mid z,\mathbf\uu_{&lt; i})}
{p(\uu_i\mid\mathbf\uu_{&lt;i})}
= e^{\surp{\uu_i}}
      \priorz p(\uu_i\mid z,\mathbf\uu_{&lt; i})
</span></p>
<p>So,</p>
<p><span class="math display">
\begin{align}
\KL{\posteriorZ}{\priorZ}
&amp;= \overbrace{
\log \frac1{p(\uu_i\mid \mathbf\uu_{&lt;i})}
}^{\surp{\uu_i}}
- \overbrace{
\E_{\posteriorZ}{\log\frac1{p(\uu_i\mid {\cPOST z},\mathbf\uu_{&lt; i})}}
}^{\Ri}
\\
&amp;=  \log \frac1{\E_{\priorZ} {p(\uu_i\mid {\cPRIO z},\mathbf\uu_{&lt; i})}}\\
&amp;\quad - e^{\surp{\uu_i}}\E_{\priorZ}p(\uu_i\mid {\cPRIO z},\mathbf\uu_{&lt; i})\log\frac1{p(\uu_i\mid {\cPOST z},\mathbf\uu_{&lt; i})}
\end{align}
</span></p>
<p>This is complicated looking, but one thing it means is if the prior is a one-hot/Dirac delta entirely concentrated on some value <span class="math inline">{\cPRIO z'}</span>, then the KL is zero.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Note in this degenerate case surprisal = negative log likelihood of <span class="math inline">\cPRIO z'</span>.</p></div></div><p><span class="math display">
\KL{\posteriorZ}{{\cPRIO \delta_{z'}}} =
\log \frac1{ p(\uu_i\mid z',\mathbf\uu_{&lt; i})}
- \log\frac1{p(\uu_i\mid z',\mathbf\uu_{&lt; i})} = 0
</span></p>
<section id="what-does-it-mean" class="level4">
<h4 class="anchored" data-anchor-id="what-does-it-mean">What does it mean?</h4>
<p><span class="math inline">\KL{\posteriorZ}{\proposalZ}</span> is the <strong>magnitude of the belief update</strong> from the proposal <span class="math inline">\proposalZ</span> to the posterior <span class="math inline">\posteriorZ</span>. Importance sampling is ideally exponential in this quantity (if such an algorithm exists). This quantity can be broken down into three pieces as <strong>KL = S – R – D</strong>.</p>
<ul>
<li><strong>S</strong> (nonnegative) is the surprisal of <span class="math inline">\uu_i</span>, how unexpected it is (under the true prior)</li>
<li><strong>R</strong> (nonnegative) measures the nondeterminism in converting from meanings in the posterior to <span class="math inline">\uu_i</span>. If, under the posterior, <span class="math inline">\uu_i</span> is a certainty, then R is zero.</li>
<li><strong>D</strong> (positive or negative) measures how helpful it is to use the proposal <span class="math inline">\proposalZ</span> rather than the prior <span class="math inline">\priorZ</span> for this particular <span class="math inline">\uu_i</span> and context. Positive if helpful, zero if not at all helpful, negative if misleading.</li>
</ul>
</section>
<section id="relationship-to-lossy-context-surprisal" class="level4">
<h4 class="anchored" data-anchor-id="relationship-to-lossy-context-surprisal">Relationship to lossy-context surprisal</h4>
<p>By Jensen’s inequality (note <span class="math inline">-\log(\cdot)</span> is concave up):</p>
<p><span class="math display">
\surp{\uu}
%= -\log p(\uu_i\mid\mathbf\uu_{&lt; i})
= -\log \E_{\priorZ}{p(\uu_i\mid {\cPRIO z},\mathbf\uu_{&lt; i})}
\le \E_{\priorZ}{ -\log p(\uu_i\mid {\cPRIO z},\mathbf\uu_{&lt; i})}\\
%= \E_{\priorZ}{ -\log p(\uu_i\mid {\cPRIO z})}\quad\text{if }U_i\indep U_{\le i} \mid {\cPRIO Z}\\
</span></p>
<p>If we interpret the latent variable <span class="math inline">\cPRIO z \sim \priorZ</span> as a lossy/noised version of the context, and make an independence assumption, <span class="math inline">U_i\indep U_{\le i} \mid {\cPRIO Z}</span>, then surprisal is upperbounded by lossy-context surprisal <span class="math inline">\E_{\priorZ}{ -\log p(\uu_i\mid {\cPRIO z})}</span>, as defined in <span class="citation" data-cites="futrell.r:2020">Futrell, Gibson, and Levy (<a href="#ref-futrell.r:2020" role="doc-biblioref">2020</a>)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Possible independence assumptions
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li><p>For the S term: we might want to add an assumption that <span class="math inline">U_i\indep U_{\le i} \mid Z_{i-1}</span> (motivated by a model wherein <span class="math inline">\cPOST z \sim \posteriorZ</span> contains all the useful information from <span class="math inline">\uu_{\le i}</span>)</p>
<p>This assumption would yield that <span class="math inline">p(\uu_i\mid\mathbf\uu_{&lt; i}) = \E_{\priorZ} p(\uu_i\mid {\cPRIO z})</span>, so:</p>
<p><span class="math display">
\surp{\uu_i} = \log \frac1{\E_{\priorZ}{p(\uu_i\mid {\cPRIO z})}}
</span></p></li>
<li><p>For the R term: If we assume that <span class="math inline">U_i\indep U_{\le i} \mid Z_i</span> (this might be somewhat less obviously reasonable to assume), then we get something similar for R:</p>
<p><span class="math display">
R_{\posteriorZ}(\uu) = \E_{\posteriorZ}{\log\frac1{p(\uu_i\mid {\cPOST z})}}
</span></p></li>
</ul>
</div>
</div>
</div>
</section>
</section>
<section id="kl-theory-vs-lc-surprisal-theory" class="level3">
<h3 class="anchored" data-anchor-id="kl-theory-vs-lc-surprisal-theory">KL theory vs (LC-)surprisal theory</h3>
<p>We have the decomposition</p>
<p><span class="math display">
\KL\posteriorZ\proposalZ
= \surp{\uu} - \left(\Ri + \Dqi\right)
</span></p>
<ol type="1">
<li><p><strong>Surprisal theory</strong> models difficulty as <span class="math inline">\approx\mathrm{S}</span>. Generalizing, we can describe surprisal theory as difficulty <span class="math inline">\approx f(\mathrm{S})</span>, for monotonic increasing <span class="math inline">f</span> (not necessarily linear).</p></li>
<li><p>We would like to propose <strong>KL-theory</strong> which models difficulty as <span class="math inline">\approx f(\mathrm{D_{KL}})</span> instead (motivated by algorithmic complexity of sampling being <span class="math inline">\approx e^{\mathrm{D_{KL}}}</span>).</p></li>
</ol>
<p><strong>Q:</strong> When do these make different predictions?</p>
<p><strong>A:</strong> When <span class="math inline">\surp{\uu}</span> is high, but <span class="math inline">[\Ri+\Dqi]</span> is similarly high. Then surprisal theory predicts <span class="math inline">\uu</span> is difficult, and KL theory predicts it is not.</p>
<p>The possible cases leading to high surprisal but low KL:</p>
<ul>
<li><p>Assume D is negligible. We get low KL when <span class="math inline">\uu</span> <em>remains</em> unpredictable on average even when given the latent <span class="math inline">z_i</span> (which encodes information about <span class="math inline">y_i</span>). Intuitively: when the latent variable forgets/misrepresents the identity of <span class="math inline">y_i</span>. For example, a perceived production error/typo?</p></li>
<li><p>Assume R is negligible. We get low KL when <span class="math inline">\proposalZ</span> is much better than <span class="math inline">\priorZ</span>. That is, when your smart proposal gives a large reduction in excess surprise.</p></li>
<li><p>When both of the above happen simultaneously.</p></li>
</ul>
</section>
</section>
<section id="notes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="notes">Notes</h2>
<ol type="1">
<li><p>The number of samples from <span class="math inline">p_Z</span> for an IS estimate of <span class="math inline">\pZu</span> is <span class="math inline">e^{\KL{\pZu}{p_Z}}</span>.</p></li>
<li><p>The number of samples from <span class="math inline">\qZu</span> for an IS estimate of <span class="math inline">\pZu</span> is <span class="math inline">e^{\KL{\pZu}{\qZu}}</span>.</p></li>
</ol>
<p>If the observable <span class="math inline">U</span> is assumed to be a deterministic function of the latent <span class="math inline">Z</span> (as in <span class="citation" data-cites="levy.r:2008">Levy (<a href="#ref-levy.r:2008" role="doc-biblioref">2008</a>)</span>, where latent state consists partially of the observable string), then <span class="math inline">\R=0</span>, and thus <span class="math inline">\KL{\pZu}{p_Z}=-\log p(\uu)</span>. Thus, IS <span class="citation" data-cites="chen.y:2005">(which, with a binary likelihood function becomes simply rejection sampling, see <a href="#ref-chen.y:2005" role="doc-biblioref">Chen 2005</a>)</span>, will require <span class="math inline">e^{-\log p(\uu)}=\frac1{p(\uu)}</span> samples.</p>
<p>This gives us a clear prediction for runtime being an exponential function of surprisal.</p>
<p>However, there are two issues with this:</p>
<ul>
<li>If we want to model the latent states as not containing the observations within them, then <span class="math inline">U</span> will not be a deterministic function of <span class="math inline">Z</span> in general (and so <span class="math inline">\R</span> may be nonzero?), and thus the runtime will depend on <span class="math inline">\R</span> as well as suprisal.</li>
<li>Independent of the previous point, if we want to sample from something smarter than simply the prior, then the number of samples needed will be <span class="math inline">e^{\KL{\pZu}{\qZu}}</span>, and so the runtime will depend on <span class="math inline">\Dq</span> (and possibly also <span class="math inline">\R</span>)</li>
</ul>
<section id="requirements-of-a-parametric-relationship" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="requirements-of-a-parametric-relationship">Requirements of a parametric relationship</h3>
<p>What properties of <span class="math inline">\qZu</span> or <span class="math inline">\R</span> would have to hold to have runtime be a particular parametric relationship with surprisal (such as a linear one)?</p>
<p>If we don’t assume anything particular about <span class="math inline">\qZu</span>, then, the sampling-based update cost is</p>
<p><span class="math display">
\begin{aligned}
\operatorname{cost}(\uu)
    &amp;= e^{\KL\pZu\qZu}\\
    &amp;= e^{\surp{\uu} - \R - \Dq}
\end{aligned}
</span></p>
<p>If <span class="math inline">\operatorname{cost}(\uu) = f(\surp{\uu})</span> for some linking function <span class="math inline">f</span>, then</p>
<p><span class="math display">
\begin{aligned}
f(\surp{\uu})
    &amp;= e^{\surp{\uu} - \R - \Dq}\\
\log f(\surp{\uu})
    &amp;= \surp{\uu} - \R - \Dq\\
\end{aligned}
</span></p>
<p>so</p>
<p><span class="math display">
\begin{aligned}
\R + \Dq
    &amp;= \surp{\uu} - \log f(\surp{\uu}) \\
    % &amp;= - \log\left(p(\uu) f( \surp{\uu})\right) \\
e^{\R + \Dq}
    &amp;= \frac1{p(\uu) f(\surp{\uu})}
\end{aligned}
</span></p>
<p>If we assume we’re sampling from the prior (so <span class="math inline">\qZu=p_Z</span>), then <span class="math inline">\Dq=0</span>, so</p>
<p><span class="math display">
\R = \surp{\uu} - \log f(\surp{\uu})
</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Parametric relationships<a href="https://www.desmos.com/calculator/yyuosjboz6">.</a> <iframe src="https://www.desmos.com/calculator/yyuosjboz6?embed" width="200" height="300" style="border: 1px solid #ccc" frameborder="0"></iframe></p>
</div></div><p>Thus, if we measure cost as reading time (RT), and fit RT as a function of surprisal, <span class="math inline">\operatorname{cost}(\uu) = \operatorname{RT}(\uu) = f_\mathrm{GAM}(\surp{\uu})</span>, we can inspect the implies about the term <span class="math inline">G\coloneqq \R + \Dq</span>, by simply subtracting the log fit RT value from the surprisal.</p>
<p><span class="math display">
G(s) = s - \log f_\mathrm{GAM}(s)
</span></p>
</section>
</section>
<section id="ordered-search-runtime" class="level1 page-columns page-full">
<h1>Ordered search runtime</h1>
<p>For deterministic ranked search, if items are sorted in order of decreasing probability, runtime is simply the number of items with higher probability (assuming no two items have the same probability, in which case the sorting is not well defined). To say anything about the expected runtime requires making some assumption about how item probabilities are distributed. I’ll assume probabilities are distributed proportional to a Pareto distribution, and derive that the runtime is exponential in surprisal. A similar derivation results from assuming the item <em>odds</em> are Pareto-distributed <span class="citation" data-cites="anderson.j:1998atomic">(which is the assumption made in <a href="#ref-anderson.j:1998atomic" role="doc-biblioref">Anderson and Lebiere 1998</a>)</span>. Here are both those derivations.</p>
<section id="probability-ordered-search-pareto-weights" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="probability-ordered-search-pareto-weights">Probability-ordered search (Pareto weights)</h3>
<p><em>Runtime = number of items with higher probability.</em></p>
<p>Assume: weights <span class="math inline">\mathsf{w}\sim</span> Pareto distribution</p>
<p>By weights here I just mean unnormalized probabilities. So the probability of an item with weight <span class="math inline">w</span> is <span class="math inline">\frac{w}{Z}</span>, for global normalizing constant <span class="math inline">Z</span>.</p>
<section id="search-runtime-for-pareto-distributed-weights" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="search-runtime-for-pareto-distributed-weights">Search runtime for Pareto-distributed weights</h4>
<p>If item weights have density <span class="math inline">\operatorname{pdf}(\mathsf{w})= \fade{a}\mathsf{w}^{-(\alpha + 1)}</span> then item <em>surprisal</em>, <span class="math inline">\mathsf{s} (= -\log\frac{\mathsf{w}}{\fade{Z}})</span>, has density:<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;The density of <span class="math inline">X \sim \operatorname{Pareto}(\alpha,\theta)</span> is <span class="math inline">f(x) = \alpha\theta^\alpha x^{-(\alpha+1)}</span> (for <span class="math inline">x \ge \theta</span>), with parameters <span class="math inline">\alpha&gt;0,\theta&gt;0</span>. Abbreviate with constant <span class="math inline">a = \alpha\theta^\alpha</span>.</p></div><div class="">
<div id="5a91a612" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Plots</span>, <span class="bu">Distributions</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">gr</span>(size<span class="op">=</span>(<span class="fl">200</span>,<span class="fl">200</span>))</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>α,θ <span class="op">=</span> <span class="fl">1</span>, <span class="fl">1</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">x-&gt;pdf</span>(<span class="fu">Pareto</span>(α,θ),x), </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>  xlim<span class="op">=</span>(θ,<span class="fl">5</span>), label<span class="op">=</span><span class="st">"pdf of Pareto(</span><span class="sc">$</span><span class="st">α,</span><span class="sc">$</span><span class="st">θ)")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="2">
<!--?xml version="1.0" encoding="utf-8"?-->
<svg xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" width="200" height="200" viewbox="0 0 800 800">
<defs>
  <clippath id="clip530">
    <rect x="0" y="0" width="800" height="800"></rect>
  </clippath>
</defs>
<path clip-path="url(#clip530)" d="M0 800 L800 800 L800 0 L0 0  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path>
<defs>
  <clippath id="clip531">
    <rect x="160" y="79" width="561" height="561"></rect>
  </clippath>
</defs>
<path clip-path="url(#clip530)" d="M140.598 677.168 L752.756 677.168 L752.756 47.2441 L140.598 47.2441  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path>
<defs>
  <clippath id="clip532">
    <rect x="140" y="47" width="613" height="631"></rect>
  </clippath>
</defs>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="140.598,677.168 140.598,47.2441 "></polyline>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="293.638,677.168 293.638,47.2441 "></polyline>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="446.677,677.168 446.677,47.2441 "></polyline>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="599.716,677.168 599.716,47.2441 "></polyline>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="752.756,677.168 752.756,47.2441 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,677.168 752.756,677.168 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,677.168 140.598,658.27 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="293.638,677.168 293.638,658.27 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="446.677,677.168 446.677,658.27 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="599.716,677.168 599.716,658.27 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="752.756,677.168 752.756,658.27 "></polyline>
<path clip-path="url(#clip530)" d="M130.98 722.833 L138.619 722.833 L138.619 696.467 L130.309 698.134 L130.309 693.875 L138.573 692.208 L143.249 692.208 L143.249 722.833 L150.888 722.833 L150.888 726.768 L130.98 726.768 L130.98 722.833 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M288.29 722.833 L304.61 722.833 L304.61 726.768 L282.665 726.768 L282.665 722.833 Q285.328 720.078 289.911 715.448 Q294.517 710.796 295.698 709.453 Q297.943 706.93 298.823 705.194 Q299.726 703.435 299.726 701.745 Q299.726 698.99 297.781 697.254 Q295.86 695.518 292.758 695.518 Q290.559 695.518 288.105 696.282 Q285.675 697.046 282.897 698.597 L282.897 693.875 Q285.721 692.74 288.175 692.162 Q290.628 691.583 292.665 691.583 Q298.036 691.583 301.23 694.268 Q304.425 696.953 304.425 701.444 Q304.425 703.574 303.614 705.495 Q302.827 707.393 300.721 709.986 Q300.142 710.657 297.04 713.874 Q293.939 717.069 288.29 722.833 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M450.925 708.134 Q454.281 708.851 456.156 711.12 Q458.054 713.388 458.054 716.722 Q458.054 721.837 454.536 724.638 Q451.017 727.439 444.536 727.439 Q442.36 727.439 440.045 726.999 Q437.754 726.583 435.3 725.726 L435.3 721.212 Q437.244 722.347 439.559 722.925 Q441.874 723.504 444.397 723.504 Q448.795 723.504 451.087 721.768 Q453.402 720.032 453.402 716.722 Q453.402 713.666 451.249 711.953 Q449.119 710.217 445.3 710.217 L441.272 710.217 L441.272 706.374 L445.485 706.374 Q448.934 706.374 450.763 705.009 Q452.591 703.62 452.591 701.027 Q452.591 698.365 450.693 696.953 Q448.818 695.518 445.3 695.518 Q443.378 695.518 441.179 695.935 Q438.98 696.351 436.341 697.231 L436.341 693.064 Q439.004 692.324 441.318 691.953 Q443.656 691.583 445.716 691.583 Q451.04 691.583 454.142 694.013 Q457.244 696.421 457.244 700.541 Q457.244 703.412 455.601 705.402 Q453.957 707.37 450.925 708.134 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M602.726 696.282 L590.92 714.731 L602.726 714.731 L602.726 696.282 M601.499 692.208 L607.378 692.208 L607.378 714.731 L612.309 714.731 L612.309 718.62 L607.378 718.62 L607.378 726.768 L602.726 726.768 L602.726 718.62 L587.124 718.62 L587.124 714.106 L601.499 692.208 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M743.034 692.208 L761.39 692.208 L761.39 696.143 L747.316 696.143 L747.316 704.615 Q748.335 704.268 749.353 704.106 Q750.372 703.921 751.39 703.921 Q757.177 703.921 760.557 707.092 Q763.936 710.263 763.936 715.68 Q763.936 721.259 760.464 724.36 Q756.992 727.439 750.673 727.439 Q748.497 727.439 746.228 727.069 Q743.983 726.698 741.575 725.958 L741.575 721.259 Q743.659 722.393 745.881 722.948 Q748.103 723.504 750.58 723.504 Q754.585 723.504 756.923 721.398 Q759.261 719.291 759.261 715.68 Q759.261 712.069 756.923 709.962 Q754.585 707.856 750.58 707.856 Q748.705 707.856 746.83 708.273 Q744.978 708.689 743.034 709.569 L743.034 692.208 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="140.598,560.295 752.756,560.295 "></polyline>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="140.598,436.489 752.756,436.489 "></polyline>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="140.598,312.684 752.756,312.684 "></polyline>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="140.598,188.878 752.756,188.878 "></polyline>
<polyline clip-path="url(#clip532)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none" points="140.598,65.0721 752.756,65.0721 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,677.168 140.598,47.2441 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,560.295 159.496,560.295 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,436.489 159.496,436.489 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,312.684 159.496,312.684 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,188.878 159.496,188.878 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,65.0721 159.496,65.0721 "></polyline>
<path clip-path="url(#clip530)" d="M73.0198 546.094 Q69.4087 546.094 67.58 549.659 Q65.7745 553.2 65.7745 560.33 Q65.7745 567.436 67.58 571.001 Q69.4087 574.543 73.0198 574.543 Q76.6541 574.543 78.4596 571.001 Q80.2883 567.436 80.2883 560.33 Q80.2883 553.2 78.4596 549.659 Q76.6541 546.094 73.0198 546.094 M73.0198 542.39 Q78.83 542.39 81.8855 546.997 Q84.9642 551.58 84.9642 560.33 Q84.9642 569.057 81.8855 573.663 Q78.83 578.247 73.0198 578.247 Q67.2097 578.247 64.131 573.663 Q61.0754 569.057 61.0754 560.33 Q61.0754 551.58 64.131 546.997 Q67.2097 542.39 73.0198 542.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M93.1818 571.696 L98.066 571.696 L98.066 577.575 L93.1818 577.575 L93.1818 571.696 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M112.279 573.64 L128.598 573.64 L128.598 577.575 L106.654 577.575 L106.654 573.64 Q109.316 570.885 113.899 566.256 Q118.506 561.603 119.686 560.261 Q121.932 557.737 122.811 556.001 Q123.714 554.242 123.714 552.552 Q123.714 549.798 121.77 548.061 Q119.848 546.325 116.746 546.325 Q114.547 546.325 112.094 547.089 Q109.663 547.853 106.885 549.404 L106.885 544.682 Q109.709 543.548 112.163 542.969 Q114.617 542.39 116.654 542.39 Q122.024 542.39 125.219 545.075 Q128.413 547.761 128.413 552.251 Q128.413 554.381 127.603 556.302 Q126.816 558.2 124.709 560.793 Q124.131 561.464 121.029 564.682 Q117.927 567.876 112.279 573.64 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M70.9365 422.288 Q67.3254 422.288 65.4967 425.853 Q63.6912 429.395 63.6912 436.524 Q63.6912 443.631 65.4967 447.195 Q67.3254 450.737 70.9365 450.737 Q74.5707 450.737 76.3763 447.195 Q78.205 443.631 78.205 436.524 Q78.205 429.395 76.3763 425.853 Q74.5707 422.288 70.9365 422.288 M70.9365 418.584 Q76.7467 418.584 79.8022 423.191 Q82.8809 427.774 82.8809 436.524 Q82.8809 445.251 79.8022 449.857 Q76.7467 454.441 70.9365 454.441 Q65.1264 454.441 62.0477 449.857 Q58.9921 445.251 58.9921 436.524 Q58.9921 427.774 62.0477 423.191 Q65.1264 418.584 70.9365 418.584 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M91.0984 447.89 L95.9827 447.89 L95.9827 453.769 L91.0984 453.769 L91.0984 447.89 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M119.015 423.284 L107.209 441.732 L119.015 441.732 L119.015 423.284 M117.788 419.209 L123.668 419.209 L123.668 441.732 L128.598 441.732 L128.598 445.621 L123.668 445.621 L123.668 453.769 L119.015 453.769 L119.015 445.621 L103.413 445.621 L103.413 441.107 L117.788 419.209 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M71.2606 298.482 Q67.6495 298.482 65.8208 302.047 Q64.0152 305.589 64.0152 312.718 Q64.0152 319.825 65.8208 323.39 Q67.6495 326.931 71.2606 326.931 Q74.8948 326.931 76.7004 323.39 Q78.5291 319.825 78.5291 312.718 Q78.5291 305.589 76.7004 302.047 Q74.8948 298.482 71.2606 298.482 M71.2606 294.779 Q77.0707 294.779 80.1263 299.385 Q83.205 303.968 83.205 312.718 Q83.205 321.445 80.1263 326.052 Q77.0707 330.635 71.2606 330.635 Q65.4504 330.635 62.3717 326.052 Q59.3162 321.445 59.3162 312.718 Q59.3162 303.968 62.3717 299.385 Q65.4504 294.779 71.2606 294.779 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M91.4225 324.084 L96.3067 324.084 L96.3067 329.964 L91.4225 329.964 L91.4225 324.084 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M117.071 310.82 Q113.922 310.82 112.071 312.973 Q110.242 315.126 110.242 318.876 Q110.242 322.603 112.071 324.779 Q113.922 326.931 117.071 326.931 Q120.219 326.931 122.047 324.779 Q123.899 322.603 123.899 318.876 Q123.899 315.126 122.047 312.973 Q120.219 310.82 117.071 310.82 M126.353 296.168 L126.353 300.427 Q124.594 299.593 122.788 299.154 Q121.006 298.714 119.246 298.714 Q114.617 298.714 112.163 301.839 Q109.733 304.964 109.385 311.283 Q110.751 309.269 112.811 308.205 Q114.871 307.117 117.348 307.117 Q122.557 307.117 125.566 310.288 Q128.598 313.436 128.598 318.876 Q128.598 324.2 125.45 327.417 Q122.302 330.635 117.071 330.635 Q111.075 330.635 107.904 326.052 Q104.733 321.445 104.733 312.718 Q104.733 304.524 108.621 299.663 Q112.51 294.779 119.061 294.779 Q120.82 294.779 122.603 295.126 Q124.408 295.473 126.353 296.168 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M71.5152 174.677 Q67.9041 174.677 66.0754 178.241 Q64.2699 181.783 64.2699 188.913 Q64.2699 196.019 66.0754 199.584 Q67.9041 203.126 71.5152 203.126 Q75.1494 203.126 76.955 199.584 Q78.7837 196.019 78.7837 188.913 Q78.7837 181.783 76.955 178.241 Q75.1494 174.677 71.5152 174.677 M71.5152 170.973 Q77.3254 170.973 80.3809 175.579 Q83.4596 180.163 83.4596 188.913 Q83.4596 197.639 80.3809 202.246 Q77.3254 206.829 71.5152 206.829 Q65.7051 206.829 62.6264 202.246 Q59.5708 197.639 59.5708 188.913 Q59.5708 180.163 62.6264 175.579 Q65.7051 170.973 71.5152 170.973 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M91.6771 200.278 L96.5614 200.278 L96.5614 206.158 L91.6771 206.158 L91.6771 200.278 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M116.746 189.746 Q113.413 189.746 111.492 191.528 Q109.594 193.311 109.594 196.436 Q109.594 199.561 111.492 201.343 Q113.413 203.126 116.746 203.126 Q120.08 203.126 122.001 201.343 Q123.922 199.538 123.922 196.436 Q123.922 193.311 122.001 191.528 Q120.103 189.746 116.746 189.746 M112.071 187.755 Q109.061 187.014 107.371 184.954 Q105.705 182.894 105.705 179.931 Q105.705 175.788 108.645 173.38 Q111.608 170.973 116.746 170.973 Q121.908 170.973 124.848 173.38 Q127.788 175.788 127.788 179.931 Q127.788 182.894 126.098 184.954 Q124.432 187.014 121.445 187.755 Q124.825 188.542 126.7 190.834 Q128.598 193.126 128.598 196.436 Q128.598 201.459 125.52 204.144 Q122.464 206.829 116.746 206.829 Q111.029 206.829 107.95 204.144 Q104.895 201.459 104.895 196.436 Q104.895 193.126 106.793 190.834 Q108.691 188.542 112.071 187.755 M110.358 180.371 Q110.358 183.056 112.024 184.561 Q113.714 186.065 116.746 186.065 Q119.756 186.065 121.445 184.561 Q123.158 183.056 123.158 180.371 Q123.158 177.686 121.445 176.181 Q119.756 174.677 116.746 174.677 Q113.714 174.677 112.024 176.181 Q110.358 177.686 110.358 180.371 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M62.2328 78.417 L69.8717 78.417 L69.8717 52.0514 L61.5616 53.718 L61.5616 49.4588 L69.8254 47.7921 L74.5013 47.7921 L74.5013 78.417 L82.1402 78.417 L82.1402 82.3521 L62.2328 82.3521 L62.2328 78.417 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M91.5845 76.4725 L96.4688 76.4725 L96.4688 82.3521 L91.5845 82.3521 L91.5845 76.4725 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M116.654 50.8708 Q113.043 50.8708 111.214 54.4356 Q109.409 57.9773 109.409 65.1068 Q109.409 72.2133 111.214 75.7781 Q113.043 79.3197 116.654 79.3197 Q120.288 79.3197 122.094 75.7781 Q123.922 72.2133 123.922 65.1068 Q123.922 57.9773 122.094 54.4356 Q120.288 50.8708 116.654 50.8708 M116.654 47.1671 Q122.464 47.1671 125.52 51.7736 Q128.598 56.3569 128.598 65.1068 Q128.598 73.8337 125.52 78.4401 Q122.464 83.0234 116.654 83.0234 Q110.844 83.0234 107.765 78.4401 Q104.709 73.8337 104.709 65.1068 Q104.709 56.3569 107.765 51.7736 Q110.844 47.1671 116.654 47.1671 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><polyline clip-path="url(#clip532)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="140.598,65.0721 141.595,73.0545 142.591,80.8835 143.587,88.563 144.584,96.0967 146.887,112.974 149.19,129.135 151.493,144.62 153.797,159.465 156.1,173.706 158.403,187.375 160.706,200.502 163.009,213.116 165.313,225.242 167.616,236.906 169.919,248.131 172.222,258.938 176.829,279.382 181.435,298.385 184.204,309.169 186.972,319.506 189.74,329.422 192.508,338.938 198.045,356.858 203.581,373.417 208.263,386.456 212.944,398.692 217.625,410.188 222.306,421.003 227.391,432.041 232.476,442.399 237.561,452.132 242.646,461.288 247.334,469.257 252.022,476.806 256.709,483.964 261.397,490.758 271.449,504.207 281.501,516.3 293.147,528.846 304.793,540.036 315.143,548.996 325.492,557.144 334.568,563.698 343.644,569.758 353.539,575.857 363.434,581.481 373.387,586.708 383.341,591.545 405.351,601.04 426.294,608.78 448.069,615.735 466.399,620.869 488.777,626.389 509.278,630.836 528.212,634.501 547.391,637.841 568.885,641.199 588.363,643.936 610.663,646.759 631.312,649.116 651.668,651.228 671.433,653.101 691.694,654.859 713.893,656.619 746.14,658.906 752.756,659.34 "></polyline>
<path clip-path="url(#clip530)" d="M263.707 171.922 L732.351 171.922 L732.351 68.2416 L263.707 68.2416  Z" fill="#ffffff" fill-rule="evenodd" fill-opacity="1"></path>
<polyline clip-path="url(#clip530)" style="stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="263.707,171.922 732.351,171.922 732.351,68.2416 263.707,68.2416 263.707,171.922 "></polyline>
<polyline clip-path="url(#clip530)" style="stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none" points="270.508,120.082 311.319,120.082 "></polyline>
<path clip-path="url(#clip530)" d="M322.403 133.473 L322.403 147.223 L318.121 147.223 L318.121 111.436 L322.403 111.436 L322.403 115.371 Q323.745 113.056 325.783 111.945 Q327.843 110.811 330.69 110.811 Q335.412 110.811 338.352 114.561 Q341.315 118.311 341.315 124.422 Q341.315 130.533 338.352 134.283 Q335.412 138.033 330.69 138.033 Q327.843 138.033 325.783 136.922 Q323.745 135.787 322.403 133.473 M336.894 124.422 Q336.894 119.723 334.949 117.061 Q333.028 114.376 329.648 114.376 Q326.269 114.376 324.324 117.061 Q322.403 119.723 322.403 124.422 Q322.403 129.121 324.324 131.806 Q326.269 134.468 329.648 134.468 Q333.028 134.468 334.949 131.806 Q336.894 129.121 336.894 124.422 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M365.435 115.371 L365.435 101.343 L369.694 101.343 L369.694 137.362 L365.435 137.362 L365.435 133.473 Q364.093 135.787 362.032 136.922 Q359.995 138.033 357.125 138.033 Q352.426 138.033 349.463 134.283 Q346.523 130.533 346.523 124.422 Q346.523 118.311 349.463 114.561 Q352.426 110.811 357.125 110.811 Q359.995 110.811 362.032 111.945 Q364.093 113.056 365.435 115.371 M350.921 124.422 Q350.921 129.121 352.843 131.806 Q354.787 134.468 358.167 134.468 Q361.546 134.468 363.491 131.806 Q365.435 129.121 365.435 124.422 Q365.435 119.723 363.491 117.061 Q361.546 114.376 358.167 114.376 Q354.787 114.376 352.843 117.061 Q350.921 119.723 350.921 124.422 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M391.592 101.343 L391.592 104.885 L387.518 104.885 Q385.227 104.885 384.324 105.811 Q383.444 106.737 383.444 109.144 L383.444 111.436 L390.458 111.436 L390.458 114.746 L383.444 114.746 L383.444 137.362 L379.162 137.362 L379.162 114.746 L375.088 114.746 L375.088 111.436 L379.162 111.436 L379.162 109.63 Q379.162 105.302 381.176 103.334 Q383.19 101.343 387.565 101.343 L391.592 101.343 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M420.273 114.422 Q416.847 114.422 414.856 117.107 Q412.865 119.769 412.865 124.422 Q412.865 129.075 414.833 131.76 Q416.824 134.422 420.273 134.422 Q423.676 134.422 425.666 131.737 Q427.657 129.051 427.657 124.422 Q427.657 119.815 425.666 117.13 Q423.676 114.422 420.273 114.422 M420.273 110.811 Q425.828 110.811 429 114.422 Q432.171 118.033 432.171 124.422 Q432.171 130.788 429 134.422 Q425.828 138.033 420.273 138.033 Q414.694 138.033 411.523 134.422 Q408.375 130.788 408.375 124.422 Q408.375 118.033 411.523 114.422 Q414.694 110.811 420.273 110.811 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M452.356 101.343 L452.356 104.885 L448.282 104.885 Q445.99 104.885 445.087 105.811 Q444.208 106.737 444.208 109.144 L444.208 111.436 L451.222 111.436 L451.222 114.746 L444.208 114.746 L444.208 137.362 L439.925 137.362 L439.925 114.746 L435.851 114.746 L435.851 111.436 L439.925 111.436 L439.925 109.63 Q439.925 105.302 441.939 103.334 Q443.953 101.343 448.328 101.343 L452.356 101.343 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M475.851 106.644 L475.851 119.63 L481.731 119.63 Q484.995 119.63 486.777 117.94 Q488.559 116.251 488.559 113.126 Q488.559 110.024 486.777 108.334 Q484.995 106.644 481.731 106.644 L475.851 106.644 M471.175 102.802 L481.731 102.802 Q487.541 102.802 490.504 105.44 Q493.49 108.056 493.49 113.126 Q493.49 118.241 490.504 120.857 Q487.541 123.473 481.731 123.473 L475.851 123.473 L475.851 137.362 L471.175 137.362 L471.175 102.802 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M509.231 124.329 Q504.069 124.329 502.078 125.51 Q500.087 126.69 500.087 129.538 Q500.087 131.806 501.569 133.149 Q503.073 134.468 505.643 134.468 Q509.184 134.468 511.314 131.968 Q513.467 129.445 513.467 125.278 L513.467 124.329 L509.231 124.329 M517.726 122.57 L517.726 137.362 L513.467 137.362 L513.467 133.426 Q512.008 135.787 509.832 136.922 Q507.657 138.033 504.508 138.033 Q500.527 138.033 498.166 135.811 Q495.828 133.565 495.828 129.815 Q495.828 125.44 498.745 123.218 Q501.684 120.996 507.495 120.996 L513.467 120.996 L513.467 120.579 Q513.467 117.639 511.522 116.042 Q509.601 114.422 506.106 114.422 Q503.883 114.422 501.777 114.954 Q499.67 115.487 497.726 116.551 L497.726 112.616 Q500.064 111.714 502.263 111.274 Q504.462 110.811 506.545 110.811 Q512.17 110.811 514.948 113.727 Q517.726 116.644 517.726 122.57 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M541.522 115.417 Q540.805 115.001 539.948 114.815 Q539.115 114.607 538.096 114.607 Q534.485 114.607 532.541 116.968 Q530.619 119.306 530.619 123.704 L530.619 137.362 L526.337 137.362 L526.337 111.436 L530.619 111.436 L530.619 115.464 Q531.962 113.102 534.115 111.968 Q536.268 110.811 539.346 110.811 Q539.786 110.811 540.318 110.88 Q540.851 110.927 541.499 111.042 L541.522 115.417 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M567.124 123.334 L567.124 125.417 L547.541 125.417 Q547.818 129.815 550.179 132.13 Q552.564 134.422 556.8 134.422 Q559.253 134.422 561.545 133.82 Q563.86 133.218 566.128 132.014 L566.128 136.042 Q563.837 137.014 561.429 137.524 Q559.022 138.033 556.545 138.033 Q550.342 138.033 546.707 134.422 Q543.096 130.811 543.096 124.653 Q543.096 118.288 546.522 114.561 Q549.971 110.811 555.804 110.811 Q561.036 110.811 564.068 114.19 Q567.124 117.547 567.124 123.334 M562.865 122.084 Q562.818 118.589 560.897 116.505 Q558.999 114.422 555.851 114.422 Q552.286 114.422 550.133 116.436 Q548.004 118.45 547.679 122.107 L562.865 122.084 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M578.327 104.075 L578.327 111.436 L587.101 111.436 L587.101 114.746 L578.327 114.746 L578.327 128.82 Q578.327 131.991 579.184 132.894 Q580.064 133.797 582.726 133.797 L587.101 133.797 L587.101 137.362 L582.726 137.362 Q577.795 137.362 575.92 135.533 Q574.045 133.681 574.045 128.82 L574.045 114.746 L570.92 114.746 L570.92 111.436 L574.045 111.436 L574.045 104.075 L578.327 104.075 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M602.749 114.422 Q599.323 114.422 597.332 117.107 Q595.341 119.769 595.341 124.422 Q595.341 129.075 597.309 131.76 Q599.3 134.422 602.749 134.422 Q606.151 134.422 608.142 131.737 Q610.133 129.051 610.133 124.422 Q610.133 119.815 608.142 117.13 Q606.151 114.422 602.749 114.422 M602.749 110.811 Q608.304 110.811 611.475 114.422 Q614.647 118.033 614.647 124.422 Q614.647 130.788 611.475 134.422 Q608.304 138.033 602.749 138.033 Q597.17 138.033 593.999 134.422 Q590.851 130.788 590.851 124.422 Q590.851 118.033 593.999 114.422 Q597.17 110.811 602.749 110.811 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M631.938 101.39 Q628.836 106.714 627.332 111.922 Q625.827 117.13 625.827 122.477 Q625.827 127.825 627.332 133.079 Q628.86 138.311 631.938 143.612 L628.235 143.612 Q624.762 138.172 623.026 132.917 Q621.313 127.663 621.313 122.477 Q621.313 117.315 623.026 112.084 Q624.739 106.852 628.235 101.39 L631.938 101.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M641.614 133.426 L649.253 133.426 L649.253 107.061 L640.943 108.727 L640.943 104.468 L649.207 102.802 L653.883 102.802 L653.883 133.426 L661.521 133.426 L661.521 137.362 L641.614 137.362 L641.614 133.426 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M671.452 131.482 L676.336 131.482 L676.336 135.463 L672.54 142.871 L669.554 142.871 L671.452 135.463 L671.452 131.482 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M686.845 133.426 L694.484 133.426 L694.484 107.061 L686.174 108.727 L686.174 104.468 L694.438 102.802 L699.114 102.802 L699.114 133.426 L706.753 133.426 L706.753 137.362 L686.845 137.362 L686.845 133.426 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path><path clip-path="url(#clip530)" d="M714.924 101.39 L718.628 101.39 Q722.1 106.852 723.813 112.084 Q725.549 117.315 725.549 122.477 Q725.549 127.663 723.813 132.917 Q722.1 138.172 718.628 143.612 L714.924 143.612 Q718.003 138.311 719.507 133.079 Q721.035 127.825 721.035 122.477 Q721.035 117.13 719.507 111.922 Q718.003 106.714 714.924 101.39 Z" fill="#000000" fill-rule="nonzero" fill-opacity="1"></path></svg>
</div>
</div>
</div></div>
<p><span class="math display">
\begin{equation}
\operatorname{pdf}(\mathsf{s}) = \fade{\frac{a}{Z^\alpha}}e^{\fade{\alpha} \;\!\mathsf{s}}
\end{equation}
</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Derivation (transformation)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Surprisal is a (monotonic smooth) deterministic function of item weight, so, starting with the pdf for weights <span class="math inline">f(\mathsf{w})</span>, we can derive the expression for the pdf of surprisal <span class="math inline">h(\mathsf{s})</span> as follows.</p>
<p>With <span class="math inline">\mathsf{w}=\fade{Z}e^{-\mathsf{s}}</span>, we have</p>
<ul>
<li><span class="math inline">f(\mathsf{w}(\mathsf{s})) = \fade{a}[\fade{Z}e^{-\mathsf{s}}]^{-(\fade{\alpha} + 1)} = \frac{\fade{a}}{\fade{Z}^{\fade{\alpha}+1}}e^{(\fade{\alpha}+1)\mathsf{s}}</span>, and</li>
<li><span class="math inline">\frac{\dee{\mathsf{w}(\mathsf{s})}}{\dee{\mathsf{s}}} = -\fade{Z}e^{-\mathsf{s}}</span></li>
</ul>
<p>So by the transformed pdf is</p>
<p><span class="math display">
\begin{aligned}
h(\mathsf{s})
&amp;= f(\mathsf{w}(\mathsf{s}))
  \cdot
  \left|\frac{\dee{}}{\dee{\mathsf{s}}}\mathsf{w}(\mathsf{s})\right|\\
&amp;= \frac{\fade{a}}{\fade{Z}^{\fade{\alpha}+1}}e^{(\fade{\alpha}+1)\mathsf{s}}
  \cdot
  \fade{Z}e^{-\mathsf{s}}
= \frac{\fade{a}}{\fade{Z}^{\fade{\alpha}}}e^{\fade{\alpha}\mathsf{s}}
\end{aligned}
</span></p>
</div>
</div>
</div>
<p>For target item <span class="math inline">i</span>, the proportion of items with surprisal lower than <span class="math inline">\surp{i}</span> is</p>
<p><span class="math display">
\begin{equation}
\Pr(\mathsf{s} &lt; \surp{i}) = \int_0^{\surp{i}} \operatorname{pdf}(\mathsf{s}) \dee{\mathsf{s}}
= \fade{\frac{a}{\alpha Z^\alpha}}\fade{(}e^{\alpha \surp{i}} -1\fade{)}
\end{equation}
</span></p>
<p>so, collecting contants, the total search runtime to find <span class="math inline">i</span> is <span class="math display">
\begin{equation}
\operatorname{Time}(i) = \fade{K}\fade{(}e^{\alpha\;\!\surp{i}} - 1\fade{)}
\end{equation}
</span></p>
<p><span class="math inline">\implies</span> search runtime increases exponentially with surprisal.</p>
</section>
</section>
<section id="probability-ordered-search-pareto-odds" class="level3">
<h3 class="anchored" data-anchor-id="probability-ordered-search-pareto-odds">Probability-ordered search (Pareto odds)</h3>
<p>Very similar to above, <span class="citation" data-cites="anderson.j:1998atomic">Anderson and Lebiere (<a href="#ref-anderson.j:1998atomic" role="doc-biblioref">1998</a>)</span> start by assuming <em>odds</em> (rather than unnormalized probabilities) are Pareto distributed, and give a similar derivation,</p>
<blockquote class="blockquote">
<p>In ACT-R, assume: need odds <span class="math inline">\sim</span> Pareto.</p>
<p><span class="math inline">\implies</span> seach runtime increases exponentially with (negative) log-odds.</p>
</blockquote>
<p>which can be restated in terms of suprisal as below.</p>
<section id="search-runtime" class="level4">
<h4 class="anchored" data-anchor-id="search-runtime">Search runtime</h4>
<p>If odds have density <span class="math inline">\operatorname{pdf}(\mathsf{o})= \fade{a}\mathsf{o}^{-(\alpha + 1)}</span> then log odds, <span class="math inline">\mathsf{r} (= \log \mathsf{o})</span>, have density</p>
<p><span class="math display">
\begin{equation}
\operatorname{pdf}(\mathsf{r})
= \fade{a}e^{-\fade{\alpha}\;\!\mathsf{r}}
\end{equation}
</span></p>
<p>by transformation, similar to above.</p>
<p>Then as above, for target item <span class="math inline">i</span> one can derive relationship between log-odds and search runtime as the proportion of items with <em>higher</em> log odds:</p>
<p><span class="math display">
\begin{equation}
\Pr(\mathsf{r} &gt; \mathrm{logodds}(i)) = \int_{\mathrm{logodds}(i)}^\infty \operatorname{pdf}(\mathsf{r}) \dee{\mathsf{r}}
= \fade{\frac a\alpha}e^{-\fade{\alpha}\mathrm{logodds}(i)}
\end{equation}
</span></p>
<p>letting constant <span class="math inline">\fade{K}=\fade{a/\alpha}</span>, this is the usual form of what is called the ‘latency formula’ in ACT-R, usually stated in terms of log odds, but which we can translate into a function of surprisal:</p>
<p><span class="math display">
\begin{equation}
% \tag{\fade{\text{ACT-R latency}}}
\qquad \operatorname{Time}(i) = \fade{K} e^{-\fade{\alpha}\;\!\mathrm{logodds}(i)} = \fade{K} (e^{\surp{i}}-1)^\alpha
\end{equation}
</span></p>
<p>by the identity <span class="math inline">\mathrm{logodds}(\cdot) = -\log(e^{\surp{\cdot}}-1)</span>.</p>
<p><span class="math inline">\implies</span> search runtime increases exponentially with surprisal.</p>
<p>Worth noting: In the ACT-R literature, it is often assumed that the parameter <span class="math inline">\alpha = 1</span>, so this simplifies to <span class="math inline">\operatorname{Time}(i) \propto e^{\surp{i}} - 1</span>.</p>



</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-agapiou.s:2017" class="csl-entry" role="listitem">
Agapiou, S., O. Papaspiliopoulos, D. Sanz-Alonso, and A. M. Stuart. 2017. <span>“Importance Sampling: Intrinsic Dimension and Computational Cost.”</span> <em>Statistical Science</em> 32 (3). <a href="https://doi.org/10.1214/17-STS611">https://doi.org/10.1214/17-STS611</a>.
</div>
<div id="ref-anderson.j:1998atomic" class="csl-entry" role="listitem">
Anderson, John R., and Christian Lebiere. 1998. <em>The Atomic Components of Thought</em>. New York: Psychology Press. <a href="https://doi.org/10.4324/9781315805696">https://doi.org/10.4324/9781315805696</a>.
</div>
<div id="ref-baldi.p:2002" class="csl-entry" role="listitem">
Baldi, Pierre. 2002. <span>“A Computational Theory of Surprise.”</span> In <em>Information, <span>Coding</span> and <span>Mathematics</span>: <span>Proceedings</span> of <span>Workshop</span> Honoring <span>Prof</span>. <span>Bob McEliece</span> on His 60th Birthday</em>, edited by Mario Blaum, Patrick G. Farrell, and Henk C. A. van Tilborg, 1–25. The <span>Springer International Series</span> in <span>Engineering</span> and <span>Computer Science</span>. Boston, MA: Springer US. <a href="https://doi.org/10.1007/978-1-4757-3585-7_1">https://doi.org/10.1007/978-1-4757-3585-7_1</a>.
</div>
<div id="ref-baldi.p:2010" class="csl-entry" role="listitem">
Baldi, Pierre, and Laurent Itti. 2010. <span>“Of Bits and Wows: <span>A Bayesian</span> Theory of Surprise with Applications to Attention.”</span> <em>Neural Networks</em> 23 (5): 649–66. <a href="https://doi.org/10.1016/j.neunet.2009.12.007">https://doi.org/10.1016/j.neunet.2009.12.007</a>.
</div>
<div id="ref-chatterjee.s:2018" class="csl-entry" role="listitem">
Chatterjee, Sourav, and Persi Diaconis. 2018. <span>“The Sample Size Required in Importance Sampling.”</span> <em>The Annals of Applied Probability</em> 28 (2). <a href="https://doi.org/10.1214/17-aap1326">https://doi.org/10.1214/17-aap1326</a>.
</div>
<div id="ref-chen.y:2005" class="csl-entry" role="listitem">
Chen, Yuguo. 2005. <span>“Another Look at Rejection Sampling Through Importance Sampling.”</span> <em>Statistics &amp; Probability Letters</em> 72 (4): 277–83. <a href="https://doi.org/10.1016/j.spl.2005.01.002">https://doi.org/10.1016/j.spl.2005.01.002</a>.
</div>
<div id="ref-dayan.p:1995" class="csl-entry" role="listitem">
Dayan, Peter, Geoffrey E. Hinton, Radford M. Neal, and Richard S. Zemel. 1995. <span>“The <span>Helmholtz</span> Machine.”</span> <em>Neural Computation</em> 7 (5): 889–904. <a href="https://doi.org/10.1162/neco.1995.7.5.889">https://doi.org/10.1162/neco.1995.7.5.889</a>.
</div>
<div id="ref-dehaene.d:2019" class="csl-entry" role="listitem">
Dehaene, David, Oriel Frigo, Sébastien Combrexelle, and Pierre Eline. 2019. <span>“Iterative Energy-Based Projection on a Normal Data Manifold for Anomaly Localization.”</span> In. <a href="https://openreview.net/forum?id=HJx81ySKwr">https://openreview.net/forum?id=HJx81ySKwr</a>.
</div>
<div id="ref-futrell.r:2020" class="csl-entry" role="listitem">
Futrell, Richard, Edward Gibson, and Roger Levy. 2020. <span>“Lossy-Context Surprisal: <span>An</span> Information-Theoretic Model of Memory Effects in Sentence Processing.”</span> <em>Cognitive Science</em> 44 (3): e12814. <a href="https://doi.org/10.1111/cogs.12814">https://doi.org/10.1111/cogs.12814</a>.
</div>
<div id="ref-genewein.t:2015" class="csl-entry" role="listitem">
Genewein, Tim, Felix Leibfried, Jordi Grau-Moya, and Daniel Alexander Braun. 2015. <span>“Bounded Rationality, Abstraction, and Hierarchical Decision-Making: An Information-Theoretic Optimality Principle.”</span> <em>Frontiers in Robotics and AI</em> 2. <a href="https://www.frontiersin.org/article/10.3389/frobt.2015.00027">https://www.frontiersin.org/article/10.3389/frobt.2015.00027</a>.
</div>
<div id="ref-gibbs.a:2002" class="csl-entry" role="listitem">
Gibbs, Alison L., and Francis Edward Su. 2002. <span>“On Choosing and Bounding Probability Metrics.”</span> <em>International Statistical Review</em> 70 (3): 419–35. <a href="https://doi.org/10.1111/j.1751-5823.2002.tb00178.x">https://doi.org/10.1111/j.1751-5823.2002.tb00178.x</a>.
</div>
<div id="ref-kingma.d:2017" class="csl-entry" role="listitem">
Kingma, Diederik P. 2017. <span>“Variational Inference &amp; Deep Learning: <span>A</span> New Synthesis.”</span> PhD thesis, University of Amsterdam. <a href="https://pure.uva.nl/ws/files/17891313/Thesis.pdf">https://pure.uva.nl/ws/files/17891313/Thesis.pdf</a>.
</div>
<div id="ref-levy.r:2008" class="csl-entry" role="listitem">
Levy, Roger. 2008. <span>“Expectation-Based Syntactic Comprehension.”</span> <em>Cognition</em> 106 (3): 1126–77. <a href="https://doi.org/10.1016/j.cognition.2007.05.006">https://doi.org/10.1016/j.cognition.2007.05.006</a>.
</div>
<div id="ref-liang.d:2018" class="csl-entry" role="listitem">
Liang, Dawen, Rahul G. Krishnan, Matthew D. Hoffman, and Tony Jebara. 2018. <span>“Variational Autoencoders for Collaborative Filtering.”</span> In <em>Proceedings of the 2018 <span>World Wide Web Conference</span></em>, 689–98. <span>WWW</span> ’18. Republic; Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee. <a href="https://doi.org/10.1145/3178876.3186150">https://doi.org/10.1145/3178876.3186150</a>.
</div>
<div id="ref-oh.b:2024arxiv" class="csl-entry" role="listitem">
Oh, Byung-Doh, Shisen Yue, and William Schuler. 2024. <span>“Frequency Explains the Inverse Correlation of Large Language Models’ Size, Training Data Amount, and Surprisal’s Fit to Reading Times.”</span> February 3, 2024. <a href="https://doi.org/10.48550/arXiv.2402.02255">https://doi.org/10.48550/arXiv.2402.02255</a>.
</div>
<div id="ref-ortega.p:2013" class="csl-entry" role="listitem">
Ortega, Pedro A., and Daniel A. Braun. 2013. <span>“Thermodynamics as a Theory of Decision-Making with Information-Processing Costs.”</span> <em>Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</em> 469 (2153): 20120683. <a href="https://doi.org/10.1098/rspa.2012.0683">https://doi.org/10.1098/rspa.2012.0683</a>.
</div>
<div id="ref-sanz-alonso.d:2018" class="csl-entry" role="listitem">
Sanz-Alonso, Daniel. 2018. <span>“Importance Sampling and Necessary Sample Size: An Information Theory Approach.”</span> <em>SIAM/ASA Journal on Uncertainty Quantification</em> 6 (2): 867–79. <a href="https://doi.org/10.1137/16M1093549">https://doi.org/10.1137/16M1093549</a>.
</div>
<div id="ref-vincent.p:2010" class="csl-entry" role="listitem">
Vincent, Pascal, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. <span>“Stacked Denoising Autoencoders: <span>Learning</span> Useful Representations in a Deep Network with a Local Denoising Criterion.”</span> <em>Journal of Machine Learning Research</em> 11 (December): 3371–3408.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>