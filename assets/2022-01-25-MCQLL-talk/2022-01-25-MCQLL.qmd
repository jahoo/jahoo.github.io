---
title: Processing time vs. surprisal
subtitle: with better language models, the relationship is superlinear
date: 25 Jan 2022
author: Jacob Louis Hoover
institute: MCQLL
bibliography: /Users/j/all.bib
csl: csl/computational-and-applied-mathematics.csl
html-math-method: katex
# number-sections: true
format:
  revealjs:
    theme: black
    scrollable: true
    width: 1300
    height: 1100
    slide-number: h.v
    controls: true
    controls-layout: bottom-right
    navigation-mode: vertical
    hash-type: number
---

# 1: Background

Human processing time and predictability

##

### Processing time is related to incremental predictability.

::: incremental

- General relationship: less predictable words take more time to process [e.g. @ehrlich.s:1981; @balota.d:1985].
We can write:
  $$\text{cost}(w) = f( p(w | \text{context}))$$
- What is linking function, $f$? Influential "surprisal theory" [@hale.j:2001; @levy.r:2005; @levy.r:2008; @smith.n:2013] proposes cost is **linear in surprisal**:
$$\text{cost}(w) \propto  -\log p(w | \text{context}) \triangleq \mathrm{surp}(w)$$

:::

## Surprisal theory

Empirical studies

:::: {.columns align=bottom .onlytextwidth}
::: {.column}
response:
**human processing cost**:

- gaze duration
- self-paced reading time
<!-- - also N400 -->

:::
::: {.column}
predictor:
**incremental surprisal**:

- from language models
<!-- - also "cloze" task estimates -->

:::
::::

. . .

Using 3-gram surprisal estimates [@smith.n:2013]:

![](overview-figs/smith.n.2013-figb2.png){.fragment height=350}

. . .

Looks linear.

---

@smith.n:2013 used (then state-of-the-art) 3-gram language models. 

- careful study; found linear relationship

![@smith.n:2013 [Figure 1]](img_external/smith.j.2013-fig1.jpg)

. . .

Following literature has introduced newer language models (larger n-grams, LSTM, Transformer-based) 

- a number of studies assume linear relationship [e.g. @aurnhammer.c:2019cogsci], and use Linear Mixed Effects (LME) models
- find evidence of a (near-)linear relationship [@goodkind.a:2018; @boyce.v:2020amlap; @wilcox.e:2020]
  - also use LME for analysis 
- the better the language models, the better the fit (**predictive power**)


# However

Reasons to question the conclusion that processing linear in surprisal

- theoretical
- empirical

## Theoretical reasons {auto-animate=true}


Surprisal theory is motivated by a _computational_ level argument.

. . .

:::: {style="padding: 1rem; border: thick solid; border-radius: 12px;"}
"Highly-incremental processing" argument outline:

::: incremental
- suppose there is function $f:$ predictability $\to$ processing cost
- some item's probability $p = p_1 p_2 \cdots p_n$
- item's cost $f(p) = f( p_1 p_2 \cdots p_n ) = f( p_1 ) + f( p_2 ) + \ldots f( p_n )$
- $\implies f$ is logarithmic. That is, cost is linear in surprisal.
:::
::::

. . .

<br />

But, how is this accomplished?  No associated _algorithmic_ level theory.  

. . .

- What processing algorithm has runtime that is linear in surprisal?

- ... first question: what algorithms can depend on probability?

. . .

**Sampling-based algorithms**

::: notes
Note: normally we look at algorithm complexity in some parameter, space or time.
Here parameter is probability.  

Not many algorithms have been studied from this perspective.  Algorithms that do exact inference don't depend on probability at all.  
:::

## {auto-animate=true}

### Processing as sampling

**Sampling-based algorithms**

Idea:

::: incremental
- Distribution $$p(\cdot\mid w_{1:n})$$ of current hypotheses about complete structure conditioned on the partial input ($n$ observed words).
- Processing model comprises of algorithm for sampling from this distribution
- Runtime of this sampler (**how long it takes to successfully sample**) would be a natural predictability-based model of processing difficulty
:::

. . .

Sampling-based processing $\implies$ cost exponential in surprisal

::: notes
Intuitively, expect that it will be harder to sample when conditioning on unlikely/surprising observation
:::

## {auto-animate=true}

### Processing as sampling

Sampling-based processing $\implies$ cost exponential in surprisal

. . .

:::: {style="padding: 1rem; border: thick solid; border-radius: 12px;"}
::: incremental
1.
  **Incremental surprisal is a KL** 

  Relative entropy of distribution before vs after seeing the next word precisely the incremental surprisal [@levy.r:2008, section 2.1]
  $$\mathrm{KL}( p(\cdot\mid w_{1:n+1})  \parallel  p(\cdot\mid w_{1:n})) = -\log p(w_{n+1} \mid w_{1:n})$$


2.
  **Expected sampling runtime is exponential in KL** 

  runtime for a sampling conditional distribution exponential in KL
  $$\text{expected sampling runtime}
    \approx e^{[\mathrm{KL}( p(\cdot\mid w_{1:n+1})
    \parallel  p(\cdot\mid w_{1:n})]}$$

  - straightforward proof for rejection sampling [e.g. @freer.f:2010].
  - importance sampling [@chatterjee.s:2017].
:::
::::

::: notes

- Note a: these distributions are both over structures for the complete sentence.
- Note b: general is this result?

:::

. . .

Exponential is definitely superlinear.

## {auto-animate=true}

### Processing as sampling

Surprisal theory predicts cost is **linear** in surprisal.

Sampling-based processing $\implies$ cost **exponential** in surprisal

. . .

- We don't know of an algorithm where runtime scales linearly in surprisal.

- Reason to ask: _Is the empirical relationship between surprisal and processing cost superlinear?_

## Empirical reasons

Recent studies give reasons to question strictly linear linking function.

- @schijndel.m:2018 examine garden path effects, and find
  - a linear linking function can't predict the magnitude of human increase in processing.

- In context of motivating uniform density hypothesis, @meister.c:2021 examine whether nonlinear linking function may give better fit than linear.
  - Using surprisal estimates from state-of-the-art Transformer-based language models, suggest superlinear shaped linking function.

## Motivates a new study

Given reasons to reconsider linear linking function:

**Theoretical**: Lack of algorithmic theory of processing which is capable predicting of linear linking function

  - sampling algorithms are superlinear

**Empirical**: Recent evidence suggesting superlinear linking function.


<br/>
**What is needed:**

a study 

- directly asking whether linking function is nonlinear
- using modern state-of-the-art language models


# 2: Methods

We want to revisit question of linking function.

- specifically: can we actually assume function is linear?

What tools do we have?

## GAMs!

We fit penalized regression splines using _generalized additive models_ (GAMs).

. . .

Motivation:

- 1. they are the tool for the job
  - don't want to assume any particular parametric form (linear)
  - want an interpretable regression model, like LME

. . .

- 2. previous literature also use these models 
  - @goodkind.a:2018; @boyce.v:2020amlap; @wilcox.e:2020; all following @smith.n:2013
  - often use GAMs to qualitatively confirm relationship.

. . .

Our study is different

- use GAMs to explicitly compare linear and nonlinear fit, rather than as tool to verify qualitatively that relationship is 'near-linear' [@wilcox.e:2020]

- choices in specifying GAMs

## GAMs? {auto-animate=true}

Well-studied, flexible, interpretable tool for nonparametric regression.

. . .

Middle ground: Linear models --- **GAMs** --- black-box ML


![](motivatingGams/gamsRmd-fitting-data.png)


$$\underbrace{\text{penalize underfit}}_\text{maximize lik/minimize err} \quad\text{and}\quad\underbrace{\text{penalize overfit}}_\text{minimize wiggliness}
$$

## GAMs? {auto-animate=true}

$$\underbrace{\text{penalize underfit}}_\text{maximize lik/minimize err} \quad\text{and}\quad\underbrace{\text{penalize overfit}}_\text{minimize wiggliness}
$$

$$
\hat \theta = \arg\min_\theta \{
\overbrace{||\mathbf{y} - \mathbf{f}_\theta||^2} + \overbrace{\lambda\times\text{wiggliness}(f_\theta)}
\}$$

$$\mathbf{f}_\theta = (f_\theta(\mathbf{x_1}),f_\theta(\mathbf{x_2} \ldots , f_\theta(\mathbf{x_N})^T$$

. . .

Wigglinness penalty, for instance,
$\text{wiggliness}(f_\theta)= 
\int \left(
\textstyle\frac{\mathrm{d}^n}{{\mathrm{d}}x^n}f_\theta(x)
\right)^2 \mathrm{d}x$

Penalty _null space_ (functions not penalized):
$n=2 \implies$ linear functions

. . .

Fitting a GAM:

1. for fixed $\lambda$, fit model to maximize likelihood/minimize error 
2. fit $\lambda$ by e.g. cross validation


## GAMs? {auto-animate=true}

Fitting a GAM:

1. for fixed $\lambda$, fit model to maximize likelihood/minimize error 
2. fit $\lambda$ by e.g. cross validation

![](motivatingGams/gamsRmd-fitting-smoothing.png)


## Bases 

How to fit? Represent $f_\theta(x) = \sum_{i} \theta_i b_i(x)$

::: {.panel-tabset .fragment fragment-index=1}

### Cubic polynomial splines

Basis for piecewise cubic functions, between "knots".

![](motivatingGams/gamsRmd-fitting-cr_k10.png)
![](motivatingGams/gamsRmd-fitting-cr_k10.gif){.fragment width=100% fragment-index=2}
![](motivatingGams/gamsRmd-fitting-cr_k20.png)
![](motivatingGams/gamsRmd-fitting-cr_k20.gif){.fragment width=100% fragment-index=2}

<!-- ### Cubic B-splines

Basis for piecewise polynomial functions, between "knots".

![](motivatingGams/gamsRmd-fitting-bs.png)
![](motivatingGams/gamsRmd-fitting-bs.gif){.fragment width=100% fragment-index=2}

![](motivatingGams/gamsRmd-Bsplines.png){.fragment width=100% fragment-index=2}
 -->
### Thin plate splines

General solution to minimize
$||\mathbf{y} - \mathbf{f}_\theta||^2 + \lambda\times J_{md}(f_\theta)$

Does not require knots. Thin plate regressions splines (TPRS) is low-rank approximation. 

![](motivatingGams/gamsRmd-fitting-tp_k6.png)
![](motivatingGams/gamsRmd-fitting-tp_k6.gif){.fragment width=100% fragment-index=2}
![](motivatingGams/gamsRmd-fitting-tp_k20.png)
![](motivatingGams/gamsRmd-fitting-tp_k20.gif){.fragment width=100% fragment-index=2}

:::


## GAM theory

A GAM is just GLM but replace _linear predictor_ with _smooth function_.

- smooth is a linear combination of basis functions.
- fit by penalized estimation, minimize loss = error + penalty


::: {.panel-tabset}

### LM

$$\mu = \beta_0 + \beta_1 x\\
y \sim \mathcal{N}(\mu,\sigma^2)$$


### LMM 

$$\mu = \beta_0 + \beta_1 x + \gamma z\\
y \sim \mathcal{N}(\mu,\sigma^2)$$

$$\gamma \sim \mathcal{N}(\mu,\sigma^2_z)$$

### GLM 

$$g(\mu) = \beta_0 + \beta_1 x\\
y \sim \mathrm{EF}(\mu,\phi)$$

### GAM

$$g(\mu) = f(x)\\
y \sim \mathrm{EF}(\mu,\phi)$$

$$f(x) = \beta_0 + \beta_1 x + \textstyle\sum_j \theta_jb_j(x)$$

:::

. . .

- nonlinear generalization of linear _mixed-effects_ models



# 3: Our study

## Data {auto-animate=true}

### Language models

Surprisal estimates from 

- Huggingface pretrained GPT2-type models (4 different models), and stat-of-the-art OpenAI GPT3 (3 sizes).
- also 5-Gram, LSTM and TransformerXL.

### Corpus

Self-paced reading data from Natural Stories corpus [@futrell.r:2018].

![](pngs/surprisal_densities.png)

## {auto-animate=true}

### Surprisals

![](pngs/surprisal_means.png){.absolute top=10% left=0 width=60%} 

![](pngs/lms-table.png){.absolute top=30% right=0 width=40% }

![](pngs/surprisal_densities.png){.absolute bottom=12% left=0 width=60%}

<!-- 
## Specifying our GAMs {auto-animate=true}

:::: {.columns align=top .onlytextwidth}
::: {.column width="50%"}
**Smooth** effect of surprisal:
```{.R .column code-line-numbers="1|1-2|1-3|1-6" data-id="formula1"}
RT ~ s(surp, bs='tp', k=6) +
     s(subj, surp, bs='fs', m=1) +
     te(freq, len) +
     s(prev_surp, bs='tp') +
     s(subj, prev_surp, bs='fs', m=1) +
     te(prev_freq, prev_len)
```
Main effect:

- TPRS basis: `bs='tp'`, rank `k=6`

By-subject smooths:

- nonlinear factor-smooth interaction `bs='fs'`
- penalty order `m=1`, so null space is just constant functions, penalizes divergence from main smooth

:::
::: {.column width="50%" .fragment}
**Linear** effect of surprisal:
```{.R data-id="formula2" code-line-numbers="1|1-3|1-4|1-7" data-id="formula2"}
RT ~ surp +
     s(subj, bs='re') +
     s(surp, subj, bs='re') +
     te(freq, len) +
     prev_surp +
     s(prev_surp, subj, bs='re') +
     te(prev_freq, prev_len)
```
Main effect

- linear

By-subject linear random effects:

- `s(x,y, bs='re')` $\equiv$ `x:y-1`

:::
:::: -->


::: notes
Reading time is predicted as a nonlin global effect of surprisal,
controlling for subject-related variation, and interactions between log
frequency and orthographic length, all for the current word as well as the
previous.

Rather than indep by-subj smooths (S&L), we use "factor-smooth interaction": controls for potentially different nonlinear effects of each participant, sharing same smoothing parameter.
:::

## Specifying our GAMs {auto-animate=true .smaller fig-cap-location=top}



::::: {.columns align=top .onlytextwidth}
:::: {.column width="50%"}
**Smooth** effect of surprisal:
```{.R data-id="formula1"}
RT ~ s(surp, bs='tp', k=6) +
     s(subj, surp, bs='fs', m=1) +
     te(freq, len) +
     s(prev_surp, bs='tp') +
     s(subj, prev_surp, bs='fs', m=1) +
     te(prev_freq, prev_len)
```

::: incremental
- For nonlinear main effect: **use thin plate regression splines basis (TPRS)**
  - better allow us to examine differences in high surprisal area (less data)
  - not use high number of basis functions: **limiting the max wiggliness**\*

    simple question: _given a few degrees of freedom, will the GAM use to bend curve upward?_

- For by-subject effects: different curve per subject
  - **factor-smooth** interaction basis
  - **penalty order $m=1$**, so null space is just constant functions, penalizes divergence from main smooth

- Likewise effect of previous word
:::


::: fragment
<hr />


![\*Cf. @wilcox.e:2020 [Figure 1, Natural Stories dataset, largest pretrained models] who use cubic regression basis and a larger value of $k=20$.](img_external/wilcox.e.2020-fig1.png){fig-cap-location=top}
:::

::::
:::: {.column width="50%"}
**Linear** effect of surprisal:
```{.R data-id="formula2"}
RT ~ surp + s(subj, bs='re') +
     s(surp, subj, bs='re') +
     te(freq, len) +
     prev_surp +
     s(prev_surp, subj, bs='re') +
     te(prev_freq, prev_len)
```

::: incremental

-  Main effect: linear

-  By-subject linear random effects: a random slope and intercept per subject (in `mgcv`, a random *linear* effect can be obtained with smooth basis `s(x,y, bs='re')` $\equiv$ `x:y-1`)

:::

::::
:::::

# 4: Results

## GAM fits {.smaller}


::: {.panel-tabset}

### fits

![**Smooth** effect of surprisal: solid lines.  **Linear** effect of surprisal: dashed lines](./pngs/fsm1_k6_wlinear1.png)

### fits diff.

![](./pngs/fsm1_k6_difflinear1.png)

### smooth deriv.

![](./pngs/fsm1_k6.derivatives.png)

### lo/hi slope

As a rougher way to look at this: get overall slope of the smooth, in

- high surprisal area (higher half of surprisal values)

- low surprisal area (lower half)

![](./pngs/fsm1_k6_halflinear_slopes.png)

### lo/hi slope diff

As a rougher way to look at this: get overall slope of the smooth, in

- high surprisal area (higher half of surprisal values)

- low surprisal area (lower half)

get difference (high slope - low slope).

![](./pngs/fsm1_k6_halflinear_diffs.png){.absolute height=60%}

### EDF

![](./pngs/fsm1_k6_edfs.png){.absolute height=60%}

:::

## Difference in fit vs LM quality


::::: {.columns align=bottom .onlytextwidth}
:::: {.column}
**Psyhometric predictive power**
Previous literature notes: 

- better LM $\implies$ better fit 

We find possible evidence

- better LMs are more superlinear.

- effect of context not very clear.

::::
:::: {.column .fragment}
:::{.panel-tabset}
### v. mean delta loglik
![](./pngs/linsmooth_delta_loglik1_v_surprisal_means.png)

### by model
![](./pngs/linsmooth_delta_loglik1.png)
:::
::::
:::::


# 5: Conclusion

## Superlinear linking function

Like previous literature

- Same dataset often used (TODO: more datasets, different psychometrics)
- Use GAMs to examine linking function

Different from previous studies


:::{.fragment .fade-in-then-semi-out}
- we used (even) better language models
:::

. . .

- more important: we compare linear and nonlinear fits
- we used GAMS to probe a specific question
  - use thin plate regression splines basis
  - model nonlinear by-subject effects
  - limited max degrees of freedom in main effect

. . .

Find evidence linearity hypothesis is _not_ justified: linking function is superlinear.  Especially for best LMs.

## Questions

####  Why only now?

. . .

Earlier work was good.

- Have to be using the latest, best, language models, and have to be looking for this.
- We have new tools, and we may have to revise our conclusions

![@smith.n:2013 [Figure 1]](img_external/smith.j.2013-fig1.jpg)

## Questions

#### What does it mean?

. . .

Have a model of human processing?  $\implies$ should scale superlinearly in surprisal.

. . .

Evidence for sampling-based model?

- Well, maybe. Shape doesn't look exponential (unless with even better LMs we see this nonlinearity even more)

. . .

Why do older language models look more linear?

. . .

- Worse language models will overestimate surprisals.  Is there structure to this overestimation? What kind of overestimation leads to linking function looking linear?

![](motivatingGams/gamsRmd-superlinear.gif){.fragment}


::: notes
These curves did not look curvy when using ngram models.  What could be going on here is systematic overestimation of surprisal by worse models.  And the degree of overestimation is possibly _structured_.  How these models were wrong is an important question.

Imagine we select words that are low residual only.  Looking at those words, could we color text by how big the residual is?  What would we see?  Can we use this to assess whether there is pattern in the overestimation of surprisal in worse models?
:::

# Thanks!

## References

::: {#refs style="font-size: 0.85em; column-gap: 5rem; column-count: 2;"}
:::